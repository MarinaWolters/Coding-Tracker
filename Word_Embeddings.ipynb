{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaWolters/Coding-Tracker/blob/master/Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo of Word Embeddings for CIS 5300\n"
      ],
      "metadata": {
        "id": "LB_X42Ki4rx7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ3jYOl78D5n"
      },
      "source": [
        "First, we're going to install a software package called Magnitude that allows for the fast efficient manipuation of word vectors.  If you'd like to learn more about it, you can read our [EMNLP 2018 paper about Magnitude](http://www.cis.upenn.edu/~ccb/publications/magnitude-fast-efficient-vector-embeddings-in-python.pdf), or you can read the [Magnitude developer documentation on Github](](https://github.com/plasticityai/magnitude)).<p>Then, we'll download a set of pre-trained word vectors that are stored in the Magnitude file format.  This file is several gigabytes large, so it will take a few minutes to download. *(To execute the code, all you need to do is press the play button below).*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZFoSx86IxEh",
        "outputId": "64375961-b44f-4da6-97ec-263e739295cc"
      },
      "source": [
        "# Install TensorFlow + Keras\n",
        "!pip install -q tensorflow keras\n",
        "\n",
        "# Install Magnitude on Google Colab\n",
        "! echo \"Installing Magnitude.... (please wait, can take a while)\"\n",
        "!pip install -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt\n",
        "!(curl https://raw.githubusercontent.com/plasticityai/magnitude/master/install-colab.sh | /bin/bash 1>/dev/null 2>/dev/null)\n",
        "!pip install spacy==3.1.2 1>/dev/null 2>/dev/null\n",
        "try:\n",
        "  from pymagnitude import *\n",
        "except Exception:\n",
        "  pass\n",
        "from pymagnitude import *\n",
        "! echo \"Done installing Magnitude.\"\n",
        "\n",
        "# Download GloVe vectors\n",
        "!curl -s http://magnitude.plasticity.ai/glove/medium/glove.6B.50d.magnitude --output vectors.magnitude\n",
        "# Uncomment to use word2vec instead: !curl -s http://magnitude.plasticity.ai/word2vec+subword/GoogleNews-vectors-negative300.magnitude --output vectors.magnitude\n",
        "# Uncomment to use fastText instead: !curl -s http://magnitude.plasticity.ai/fasttext+subword/wiki-news-300d-1M.magnitude --output vectors.magnitude"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing Magnitude.... (please wait, can take a while)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: xxhash>=1.0.1 in /usr/local/lib/python3.9/dist-packages/xxhash-3.2.0-py3.9-linux-x86_64.egg (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 2)) (3.2.0)\n",
            "Requirement already satisfied: fasteners>=0.14.1 in /usr/local/lib/python3.9/dist-packages/fasteners-0.18-py3.9.egg (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 3)) (0.18)\n",
            "Requirement already satisfied: annoy>=1.11.4 in /usr/local/lib/python3.9/dist-packages/annoy-1.17.1-py3.9-linux-x86_64.egg (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 4)) (1.17.1)\n",
            "Requirement already satisfied: lz4>=1.0.0 in /usr/local/lib/python3.9/dist-packages/lz4-4.3.2-py3.9-linux-x86_64.egg (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 5)) (4.3.2)\n",
            "Requirement already satisfied: h5py>=2.8.0 in /usr/local/lib/python3.9/dist-packages (from -r https://raw.githubusercontent.com/plasticityai/magnitude/master/requirements.txt (line 6)) (3.8.0)\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   137  100   137    0     0    678      0 --:--:-- --:--:-- --:--:--   678\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done installing Magnitude.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09xkNeOIOtve"
      },
      "source": [
        "Download a word embeddings file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJD8Dzv58pts",
        "outputId": "05177882-cb82-4aad-8d88-5a56e29507c9"
      },
      "source": [
        "#!wget http://magnitude.plasticity.ai/glove/heavy/glove.6B.300d.magnitude\n",
        "!wget http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-21 15:24:11--  http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude\n",
            "Resolving magnitude.plasticity.ai (magnitude.plasticity.ai)... 54.231.203.133, 52.217.114.133, 52.216.41.157, ...\n",
            "Connecting to magnitude.plasticity.ai (magnitude.plasticity.ai)|54.231.203.133|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5295644672 (4.9G) [binary/octet-stream]\n",
            "Saving to: ‘GoogleNews-vectors-negative300.magnitude.1’\n",
            "\n",
            "GoogleNews-vectors- 100%[===================>]   4.93G  36.9MB/s    in 2m 7s   \n",
            "\n",
            "2023-03-21 15:26:19 (39.6 MB/s) - ‘GoogleNews-vectors-negative300.magnitude.1’ saved [5295644672/5295644672]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCxPU9wO8_VD"
      },
      "source": [
        "After the files have downloaded, we can start running Python and Magnitude!  We will load a set of vectors from the file that we just downloaded.<p>One the vectors are loaded, we can see how many vectors we've loaded in with this command.  This means that we have vectors representing this many words.  This is the size of our **vocabulary.**  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n50qMALy-cCu",
        "outputId": "5cebcb2b-002c-4258-d459-cc50b43d58f6"
      },
      "source": [
        "from pymagnitude import *\n",
        "#vectors = Magnitude(\"glove.6B.300d.magnitude\")\n",
        "vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")\n",
        "\n",
        "print(\"The number of words with vector representations in this file is %s.\" % len(vectors))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of words with vector representations in this file is 3000000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YmTdD4U9PPp"
      },
      "source": [
        "We can see what the *dimensionality* of each vector is.  The dimensionality is just the length of the vector.  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a4zsGLG-lwQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646ac53c-5b3d-4738-e4f4-538d3cd1cfd2"
      },
      "source": [
        "vectors.dim"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"cat\" in vectors"
      ],
      "metadata": {
        "id": "fABwyyW1xvY7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11a85554-64d5-4fa3-b185-55bf0848e6f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utt57Xrumdp6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c62f1af0-517f-4943-e506-e55e005495b3"
      },
      "source": [
        "vectors.most_similar(\"excellent\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrific', 0.7409729),\n",
              " ('superb', 0.7062715),\n",
              " ('exceptional', 0.68147063),\n",
              " ('fantastic', 0.6802847),\n",
              " ('good', 0.644293),\n",
              " ('great', 0.6124601),\n",
              " ('Excellent', 0.6091997),\n",
              " ('impeccable', 0.5980968),\n",
              " ('exemplary', 0.5959651),\n",
              " ('marvelous', 0.58292854)]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74zamPJh9c9A"
      },
      "source": [
        "We can print out what a vector look likes.  It should have a bunch of real-valued numbers (positive or negative).  The number of values that we will see is *vectors.dim*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjYcuJsS9z79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34145127-c9f4-4931-9ab9-c7d9557962a0"
      },
      "source": [
        "vectors.query(\"cat\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0040587,  0.0671903, -0.0938735,  0.0713696,  0.0388996,\n",
              "        0.0273262,  0.0163957, -0.0031345,  0.0726555, -0.0414715,\n",
              "        0.0265225, -0.1928908, -0.0014668, -0.0977313, -0.00432  ,\n",
              "       -0.0274869,  0.0166368,  0.0498301, -0.1478829, -0.0044606,\n",
              "        0.0707266, -0.0485442,  0.0739415, -0.04115  , -0.0319877,\n",
              "        0.0819786, -0.0951595,  0.1202353,  0.1356665, -0.0282906,\n",
              "       -0.0258795, -0.0649399, -0.0298981, -0.0466153, -0.0337559,\n",
              "        0.0430789, -0.0011403,  0.0237899,  0.0145472,  0.1138056,\n",
              "        0.0245936, -0.0369707,  0.0221824,  0.0369707,  0.0065101,\n",
              "       -0.0406678,  0.0691192, -0.0237899, -0.0091623,  0.0182443,\n",
              "       -0.1099478,  0.0281299,  0.1131626,  0.0459723,  0.016235 ,\n",
              "       -0.0443649,  0.0536879, -0.1228071,  0.1305228,  0.0352026,\n",
              "        0.072977 ,  0.0700837, -0.0295766,  0.0681547,  0.0294158,\n",
              "       -0.0271655,  0.0196106,  0.0335951, -0.0633325, -0.0298981,\n",
              "        0.1620283,  0.0130201, -0.0233076, -0.000658 , -0.0758704,\n",
              "        0.084229 ,  0.0295766, -0.0350418,  0.0003466, -0.0193695,\n",
              "        0.0167976, -0.0096044,  0.063654 , -0.0466153, -0.1099478,\n",
              "        0.026844 , -0.0906587,  0.0331129, -0.0343989, -0.0406678,\n",
              "       -0.0462938,  0.0114931, -0.0387389, -0.0591532, -0.0707266,\n",
              "       -0.0784423,  0.0278084, -0.0247543, -0.0855149, -0.0700837,\n",
              "       -0.07362  , -0.0319877,  0.0380959,  0.0507946,  0.0242721,\n",
              "       -0.0906587,  0.0469368, -0.0066306,  0.0329522, -0.0626895,\n",
              "       -0.0308625,  0.0466153,  0.0562598,  0.1028751, -0.0549739,\n",
              "       -0.0286121, -0.0164761, -0.0819786, -0.0684762, -0.0310233,\n",
              "       -0.0403463, -0.031184 , -0.1285939, -0.021861 , -0.104161 ,\n",
              "        0.036167 , -0.0005149,  0.0143061,  0.0514375, -0.062368 ,\n",
              "       -0.0114931,  0.0111716, -0.0475797,  0.0053045, -0.0466153,\n",
              "       -0.0078764,  0.0049428,  0.0249151,  0.0353633,  0.041793 ,\n",
              "        0.0352026, -0.003898 , -0.0790852,  0.0096044,  0.0533665,\n",
              "        0.0655829, -0.0925876,  0.0549739, -0.0382567, -0.084229 ,\n",
              "        0.1273079, -0.021861 , -0.1517408, -0.0203339, -0.0475797,\n",
              "       -0.0382567,  0.0187265,  0.0118146, -0.0352026,  0.0620465,\n",
              "       -0.0549739, -0.0059073,  0.036167 , -0.0406678, -0.0536879,\n",
              "       -0.0475797,  0.042436 ,  0.0379352,  0.0450079,  0.0187265,\n",
              "       -0.0266832, -0.0202535, -0.0220217,  0.0919446, -0.0646184,\n",
              "        0.0237899,  0.0405071, -0.0691192, -0.0726555,  0.0704051,\n",
              "       -0.0488657, -0.0195302,  0.0171994,  0.021218 , -0.0086801,\n",
              "        0.0434004,  0.0639754,  0.0893727,  0.0614036,  0.0462938,\n",
              "        0.0215395, -0.0475797,  0.0165565,  0.0290944,  0.0403463,\n",
              "        0.0880868,  0.0176013, -0.1060899, -0.0681547,  0.0597961,\n",
              "        0.0145472, -0.072977 , -0.0453293, -0.0137435, -0.0009444,\n",
              "        0.013422 ,  0.0239506, -0.0273262,  0.0276477, -0.1105907,\n",
              "       -0.1318087,  0.0057867, -0.0614036, -0.0158331, -0.063011 ,\n",
              "        0.0332737,  0.030541 , -0.1009462, -0.0655829, -0.0055456,\n",
              "        0.0401856,  0.0482227, -0.042436 , -0.0774778, -0.0176817,\n",
              "       -0.021861 ,  0.0226647, -0.0119753,  0.0694407, -0.0197713,\n",
              "        0.063654 ,  0.0169583, -0.0175209,  0.009524 , -0.0906587,\n",
              "        0.0278084,  0.1080188,  0.0059877,  0.0049227,  0.0155116,\n",
              "        0.1240931, -0.0720126, -0.0111716,  0.003677 ,  0.1215212,\n",
              "        0.0071128,  0.0114127,  0.0239506,  0.0527235, -0.0543309,\n",
              "       -0.0977313,  0.0495086, -0.0958024,  0.0459723,  0.0014768,\n",
              "        0.0565813, -0.0723341,  0.0307018, -0.0626895,  0.0052643,\n",
              "       -0.0303803,  0.051759 , -0.0466153, -0.0176013,  0.0118949,\n",
              "        0.0777993, -0.0498301, -0.0022705, -0.0893727, -0.0234684,\n",
              "       -0.0543309,  0.0607606, -0.0276477,  0.0610821,  0.0384174,\n",
              "        0.0090819, -0.0156724,  0.0588317,  0.0215395, -0.0116538,\n",
              "        0.0755489,  0.008881 , -0.0321485,  0.0887298,  0.0274869,\n",
              "       -0.0430789, -0.0332737, -0.0024312,  0.0234684,  0.0491872,\n",
              "       -0.0678333,  0.0594747, -0.0520805,  0.0195302,  0.0945165,\n",
              "       -0.0153509,  0.0498301,  0.1620283, -0.0906587,  0.0184854],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMcKgZeX95Bq"
      },
      "source": [
        "That's what a cat looks like according to our model!  It looks just like [this](https://en.wikipedia.org/wiki/Cat_intelligence#/media/File:An_up-close_picture_of_a_curious_male_domestic_shorthair_tabby_cat.jpg), right?  Well, not really, but the cool think about vectors is that they allow us to say how similar two things are.  So we can ask, how similar are *cats* and *dogs*?    The result will be a decimal between 0 and 1.0, with numbers closer to 1 indicating that the words are more similar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giXQok_ABNHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b43cfe0f-b15f-42ed-ee6f-35754dc8153e"
      },
      "source": [
        "vectors.similarity(\"professor\", \"cucumber\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.056618165"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCiZY8Fr_D4V"
      },
      "source": [
        "Wait, isn't that  comparing apples and oranges?  No, but this is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aL0lkOYd_LQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "665ddb9c-8edc-419f-8e7d-b739cf2d5ea8"
      },
      "source": [
        "vectors.similarity(\"apples\", \"oranges\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.69658417"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6vAfygxBaqJ"
      },
      "source": [
        "The Magnitude software allows you to query for the most similar word out of a list of words using the command *most_similar_to_given*, which takes a query word and then a list of other words to compare it to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KCosp00TBdXV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6e4f3c5a-fc4c-4c10-bffe-0b251cc14d68"
      },
      "source": [
        "vectors.most_similar_to_given(\"kittens\", [\"oranges\", \"strawberries\", \"tomatoes\", \"cats\", \"dogs\", \"trees\", ])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cats'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cvl7kxK_R8m"
      },
      "source": [
        "We can also look for the word vectors that are most similar to a query word.  Here are the words that are most similar to *apples*.  Try replacing the word *apples* with whatever word you want, and re-running this cell (by pressing the play button again), and see what the most similar words are to the word that you entered.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polysemous words\n",
        "\n",
        "In early word embeddings poleysemous words were a problem.  Polysemous words are words that have more than 1 meaning.  For example, \"bug\" can have lots of different meanings:\n",
        "* A creepy-crawly thing\n",
        "* Something that makes you ill\n",
        "* An error in your code\n",
        "* A covert listening device\n",
        "* (Verb) be annoying  \n",
        "\n",
        "In word2vec, all of these different meanings get averaged into one vector."
      ],
      "metadata": {
        "id": "McgUo0UsOwAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar(\"bug\", topn = 20)"
      ],
      "metadata": {
        "id": "ZEJc3E3yQOG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c337580-2bee-4dd0-c354-89c31c93b9ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('bugs', 0.7603134),\n",
              " ('worm', 0.6060655),\n",
              " ('Bug', 0.569483),\n",
              " ('virus', 0.55493534),\n",
              " ('Y2K_millennium', 0.53581625),\n",
              " ('ActiveX_vulnerability', 0.53124774),\n",
              " ('http://bugs.gentoo.org', 0.525815),\n",
              " ('vuln', 0.5222951),\n",
              " ('insect', 0.51954466),\n",
              " ('Bagle_virus', 0.5150193),\n",
              " ('worms', 0.5076382),\n",
              " ('brown_recluse_spider', 0.5051397),\n",
              " ('Virut', 0.5046243),\n",
              " ('viruses', 0.50372773),\n",
              " ('flaw', 0.5024464),\n",
              " ('Brown_marmorated_stink', 0.50240886),\n",
              " ('SoBig_virus', 0.5019326),\n",
              " ('Koobface_virus', 0.50151485),\n",
              " ('spoofing_vulnerability', 0.50019515),\n",
              " ('tar_remover', 0.5000378)]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Polysemy is very common.  For instance, \"apple\" could be the fruit or the company.  Since the dominant sense of apple in our corpus is the fruit, then all of the its most similar vectors are fruits and not computers."
      ],
      "metadata": {
        "id": "LNC9aEfjQPr9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRBMyflaBv3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74316b91-7383-4d8c-bfd1-31f72e062d50"
      },
      "source": [
        "vectors.most_similar(\"apple\", topn = 20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('apples', 0.7203598),\n",
              " ('pear', 0.6450697),\n",
              " ('fruit', 0.6410147),\n",
              " ('berry', 0.6302295),\n",
              " ('pears', 0.6133961),\n",
              " ('strawberry', 0.60582614),\n",
              " ('peach', 0.6025872),\n",
              " ('potato', 0.5960934),\n",
              " ('grape', 0.59358644),\n",
              " ('blueberry', 0.5866668),\n",
              " ('cherries', 0.5784383),\n",
              " ('mango', 0.57518566),\n",
              " ('apricot', 0.57277775),\n",
              " ('melon', 0.5719985),\n",
              " ('almond', 0.570483),\n",
              " ('Granny_Smiths', 0.56953335),\n",
              " ('grapes', 0.56922567),\n",
              " ('peaches', 0.5659247),\n",
              " ('pumpkin', 0.5651883),\n",
              " ('apricots', 0.56455696)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can average together vectors to get a different result."
      ],
      "metadata": {
        "id": "eYD5UZR0Qe6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apple = vectors.query(\"apple\")\n",
        "computer = vectors.query(\"computer\")\n",
        "averaged = (apple + computer)/2\n",
        "vectors.most_similar(averaged, topn = 20)"
      ],
      "metadata": {
        "id": "kuZq4QW4M0Bi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b94b2a9-3945-4d05-c6a8-3c16b8972c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('computer', 0.5838944),\n",
              " ('apple', 0.58389425),\n",
              " ('computers', 0.44672984),\n",
              " ('laptop', 0.44189417),\n",
              " ('Apple_IIe', 0.40202093),\n",
              " ('apples', 0.39580956),\n",
              " ('iMac', 0.3923599),\n",
              " ('receive_MacMall_Exclusive', 0.3897292),\n",
              " ('laptop_computer', 0.38527435),\n",
              " ('cartoonish_apple', 0.38067073),\n",
              " ('iBook_laptop', 0.3763073),\n",
              " ('Macbook_laptop', 0.37304696),\n",
              " ('hackintosh', 0.3725756),\n",
              " ('mainframes_minicomputers', 0.37218946),\n",
              " ('Marco_Boglione', 0.3710637),\n",
              " ('iBook', 0.3694635),\n",
              " ('Surface_tabletop', 0.36521816),\n",
              " ('electric_typewriter', 0.36513108),\n",
              " ('logs_keystrokes', 0.36422026),\n",
              " ('view_HealthCast', 0.36404467)]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what's happening under the hood.  I'm showing the first few numbers in the vectors, just to make it easier to see."
      ],
      "metadata": {
        "id": "g_agD_W8QitW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "apple[:3]"
      ],
      "metadata": {
        "id": "-gnUqdjYNpcO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e883839-e3fa-4bce-c5e7-5536e212f564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.0205091, -0.0509621, -0.0038455], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "computer[:3]"
      ],
      "metadata": {
        "id": "Dcaz3jOoNwMH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c3ecc56-eea9-4822-f100-893b47062847"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0408137, -0.076433 ,  0.0467503], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "added = (apple+computer)\n",
        "added[:3]"
      ],
      "metadata": {
        "id": "QFNCPCu3NyUJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdf65a74-8cd6-4973-c8c9-49700045812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0203046 , -0.12739511,  0.0429048 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "averaged = (apple+computer)/2\n",
        "averaged[:3]"
      ],
      "metadata": {
        "id": "lXPaCC8TOHf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5705d8e2-df2a-4b8a-d0a7-49bb68ee9b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.0101523 , -0.06369755,  0.0214524 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that capitalization matters:\n",
        "apple versus Apple"
      ],
      "metadata": {
        "id": "lWrFJzsRkNkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar(\"Apple\", topn = 20)"
      ],
      "metadata": {
        "id": "xrue3UWwkSoa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9fc2e8a-22f9-45ba-8ba5-216a7971d5b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Apple_AAPL', 0.74569863),\n",
              " ('Apple_Nasdaq_AAPL', 0.730041),\n",
              " ('Apple_NASDAQ_AAPL', 0.7175089),\n",
              " ('Apple_Computer', 0.71459734),\n",
              " ('iPhone', 0.6924266),\n",
              " ('Apple_NSDQ_AAPL', 0.68686044),\n",
              " ('Steve_Jobs', 0.6758423),\n",
              " ('iPad', 0.6580769),\n",
              " ('Apple_nasdaq_AAPL', 0.64449704),\n",
              " ('AAPL_PriceWatch_Alert', 0.6439754),\n",
              " ('Apple_iPad', 0.6227746),\n",
              " ('iPhones', 0.6192503),\n",
              " ('Nexus_One', 0.6192277),\n",
              " ('Appleâ_€_™', 0.6176695),\n",
              " ('Apple_AAPL_iPhone', 0.615996),\n",
              " ('Apple_AAPL_Fortune', 0.6144309),\n",
              " ('Apple_Inc_AAPL.O', 0.61417174),\n",
              " ('Mac_cloner_Psystar', 0.6066086),\n",
              " ('Apple_APPL', 0.60539895),\n",
              " ('iPod', 0.6045314)]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar((vectors.query(\"delicious\") + vectors.query(\"food\")), topn = 20)"
      ],
      "metadata": {
        "id": "EPKD-ixXPCyQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf1681c9-d179-4207-ff79-5c03520bd3c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('food', 1.3721681),\n",
              " ('delicious', 1.3721678),\n",
              " ('tasty', 1.2404301),\n",
              " ('scrumptious', 1.2201036),\n",
              " ('yummy', 1.1655107),\n",
              " ('Generous_portions', 1.1525309),\n",
              " ('savory_delights', 1.1455574),\n",
              " ('Rotisserie_chickens', 1.1427587),\n",
              " ('delicious_nutritious', 1.1423676),\n",
              " ('vegan_dishes', 1.1381708),\n",
              " ('crunchy_salty', 1.1376917),\n",
              " ('delectable', 1.1263512),\n",
              " ('tasty_snacks', 1.1258788),\n",
              " ('scrumptious_desserts', 1.1242514),\n",
              " ('foods', 1.1204889),\n",
              " ('gourmet', 1.1174433),\n",
              " ('sinful_desserts', 1.106785),\n",
              " ('nutritious', 1.1001396),\n",
              " ('Rotisserie_chicken', 1.0926499),\n",
              " ('delectable_dessert', 1.0915829)]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In subsequent models like ELMo and BERT, word embeddings were computed for sentences rather than individual words, allowing the surrounding words in the sentence to influence the vector for each word token.  These context-based word embeddings allowed for unique word embeddings for each word instance."
      ],
      "metadata": {
        "id": "eCZVFon-PqRB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsUxDu7d5WNo"
      },
      "source": [
        "# Solving word analogy problems\n",
        "\n",
        "Magnitude also alows us to test the analogy solving capabilities of word vectors.  For analogy problems like \"***man*** is to ***king*** as ***woman*** is to **-----**\" we are doing some vector arithmetic.   We take the vector for *king*, subtract the vector for *king*, and then add the vector for *woman*:<p>+ *king* <p>- *man*<p>+ *woman*<p>The result is a vector.  To figure out what word is closest to it, we find the most similar word vectors to the vector that resulted from our arithmetic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uI2jU1_z5VGF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa44beb-4cc7-4baf-a35c-380d44d61d45"
      },
      "source": [
        "vectors.most_similar(positive = [\"king\", \"woman\"], negative = [\"man\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('queen', 0.71181935),\n",
              " ('monarch', 0.61896753),\n",
              " ('princess', 0.5902431),\n",
              " ('crown_prince', 0.5499462),\n",
              " ('prince', 0.5377322),\n",
              " ('kings', 0.5236845),\n",
              " ('Queen_Consort', 0.5235946),\n",
              " ('queens', 0.5181134),\n",
              " ('sultan', 0.5098594),\n",
              " ('monarchy', 0.50874126)]"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bizns6809DoP"
      },
      "source": [
        "You can try other gender based analogy problems like: <p>***man*** is to ***actor*** as ***woman*** is to what?<p><p>***man*** is to ***congressman*** as ***woman*** is to what?<p>Try out other analogy problems on your own.  Ones related to countries often work.  For instance, <p>***london*** is to ***uk*** as ***paris*** is to what?<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAcXIIL28Ozf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70376d18-1086-4c2f-c1f5-812154e79732"
      },
      "source": [
        "vectors.most_similar(positive = [\"congressman\", \"woman\"], negative = [\"man\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('congresswoman', 0.67109746),\n",
              " ('senator', 0.64611745),\n",
              " ('Congresswoman', 0.63603246),\n",
              " ('Congressman', 0.63283956),\n",
              " ('lawmaker', 0.6258571),\n",
              " ('congressmen', 0.59148425),\n",
              " ('Rep.', 0.5858079),\n",
              " ('congressional', 0.56658834),\n",
              " ('Congressional_District', 0.5613279),\n",
              " ('Congressmember', 0.5602188)]"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias in word vectors\n",
        "\n",
        "Negative society biases appear in word vectors, since they are trained on data that contain those biases.   \n",
        "\n",
        "A classic example of bias in word analogy problems was demonstrated in [this paper](https://arxiv.org/abs/1607.06520).  \n",
        "\n",
        "Out-dated stereotypes of women are revealed in the word2vec embeddings when you use them to solve the analogy problem \"***man*** is to ***computer_programmer*** as ***woman*** is to **-----**\"\n",
        "\n",
        "\n",
        "*(Be sure you are using these vectors `vectors = Magnitude(\"GoogleNews-vectors-negative300.magnitude\")` to see this effect.  It's not in all word embeddings).*\n"
      ],
      "metadata": {
        "id": "U54fLyljGpSa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectors.most_similar(positive = [\"computer_programmer\", \"woman\"], negative = [\"man\"])"
      ],
      "metadata": {
        "id": "E1so3PgNGzzy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3f3f92-68c9-4949-f713-71b602749182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('homemaker', 0.5627119),\n",
              " ('housewife', 0.5105047),\n",
              " ('graphic_designer', 0.50518024),\n",
              " ('schoolteacher', 0.49794948),\n",
              " ('businesswoman', 0.4934892),\n",
              " ('paralegal', 0.49255118),\n",
              " ('registered_nurse', 0.49079752),\n",
              " ('saleswoman', 0.48816282),\n",
              " ('electrical_engineer', 0.47977278),\n",
              " ('mechanical_engineer', 0.47553998)]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    }
  ]
}