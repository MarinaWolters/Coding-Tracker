{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1211a01bbe144b196cffdbcd41c07f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Progress:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a904dd608b9d4a3989ffec81498c91c7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ac5ec853a304f898337199ddc5d5ef8",
            "value": 0
          }
        },
        "a904dd608b9d4a3989ffec81498c91c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "25px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7ac5ec853a304f898337199ddc5d5ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "53b4bf7a801b41ea922c411826d1490b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "Progress:",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b382af71d7544b139085a9ef0080f78e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7f7ce85eed9543bfb3ae4b44c968cf04",
            "value": 0
          }
        },
        "b382af71d7544b139085a9ef0080f78e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": "25px",
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "7f7ce85eed9543bfb3ae4b44c968cf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaWolters/Coding-Tracker/blob/master/CIS5450_Spring23_Homework3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVMCjEV1IEC7"
      },
      "source": [
        "# CIS 545 Homework 3: Spark SQL\n",
        "\n",
        "#### **Worth 100 points, due Monday March 13th, 11:59 PM EST**\n",
        "\n",
        "Welcome to CIS 545 Homework 3! In this homework you will master Spark SQL. By the end, you'll be a star (not that you aren't already one). Over the next few days you will be using an EMR cluster to use Spark to manipulate the datasets about Amazon products and their reviews. \n",
        "\n",
        "## The Necessary Notes and Nags\n",
        "Before we begin here are some important notes to keep in mind,\n",
        "\n",
        "\n",
        "1.   **IMPORTANT!** I said it twice, it's really important. In this homework, we will be using AWS resources. You are given a quota ($100) to use for the entirety of the homework. There is a small chance you will use all this money, however it is important that at the end of every session, you **shut down your EMR cluster**.\n",
        "2.   **Be sure you use Google Colab for this Homework** since we must connect to the EMR cluster and local Jupyter will have issues doing that. Using a Google Colab Notebook with an EMR cluster has two important abnormalities:\n",
        "    * The first line of any cell in which you will use the spark session must be `%%spark`. Notice that all cells below have this.\n",
        "    * You will, unfortunately, not be able to stop a cell while it is running. If you wish to do so, you will need to restart your cluster. See the Setup EMR Document for reference.\n",
        "3.   You are **required** to use Spark SQL queries to handle the data in the assignment. Mastering SQL is more beneficial than being able to use Spark commands (functions) as it will show up in more areas of programming and data science/analytics than just Spark. Use the following [function list](https://spark.apache.org/docs/latest/api/sql/index.html#) to see all the SQL functions avaliable in Spark.\n",
        "4.   Throughout the homework you will be manipulating Spark dataframes (sdfs). We do not specify any ordering on the final output. You are welcome to order your final tables in whatever way you deem fit. We will conduct our own ordering when we grade.\n",
        "5. Based on the challenges you've faced in the previous homework, we are including information on the expected schema of your results.  Apache Spark is very fiddly but we hope this will help.\n",
        "6. There are portions of this homework that are _very_ hard. We urge you start early to come to office hours and get help if you get stuck. But don't worry, I can see the future, and you've all got this.\n",
        "\n",
        "With that said, let's dive in.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XEqGpEGBWs5"
      },
      "source": [
        "## Step 0: Set up EMR\n",
        "\n",
        "Follow the [AWS Academy Getting Started](https://docs.google.com/document/d/1JPitLGaorjTbXjGsaoIHcLTu2cj8rjm5UNr9bSpZ72k/edit?usp=sharing) instructions.\n",
        "\n",
        "Move on to Step 0.1 after you have completed all the steps in the document.\n",
        "\n",
        "![ACME GIANT RUBBER BAND](https://pbs.twimg.com/media/DRqbJh7UMAE2z4o?format=jpg&name=4096x4096)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iBPXxgAdXkv"
      },
      "source": [
        "### Step 0.1: The Superfluous Setup\n",
        "\n",
        "Run the following two cells. These will allow your colab notebook to connect to an use your EMR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvkEbVaaAQ1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b48ef8f-a1a5-4a19-fdd7-f3d2b7e2cd00"
      },
      "source": [
        "#%%capture\n",
        "!apt install libkrb5-dev\n",
        "!pip install sparkmagic\n",
        "!pip install -i https://test.pypi.org/simple/ penn-grader==0.5.0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libkrb5-dev is already the newest version (1.17-6ubuntu4.2).\n",
            "libkrb5-dev set to manually installed.\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-510\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 21 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sparkmagic\n",
            "  Downloading sparkmagic-0.20.4.tar.gz (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 KB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hdijupyterutils>=0.6\n",
            "  Downloading hdijupyterutils-0.20.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting autovizwidget>=0.6\n",
            "  Downloading autovizwidget-0.20.4.tar.gz (9.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=4.0.2 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (7.9.0)\n",
            "Requirement already satisfied: pandas>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (2.25.1)\n",
            "Requirement already satisfied: ipykernel>=4.2.2 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (5.3.4)\n",
            "Requirement already satisfied: ipywidgets>5.0.0 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (7.7.1)\n",
            "Requirement already satisfied: notebook>=4.2 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (5.7.16)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.8/dist-packages (from sparkmagic) (6.0.4)\n",
            "Collecting requests_kerberos>=0.8.0\n",
            "  Downloading requests_kerberos-0.14.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting nest_asyncio==1.5.5\n",
            "  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n",
            "Requirement already satisfied: plotly>=3 in /usr/local/lib/python3.8/dist-packages (from autovizwidget>=0.6->sparkmagic) (5.5.0)\n",
            "Collecting jupyter>=1\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl (2.7 kB)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.2.2->sparkmagic) (5.7.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.2.2->sparkmagic) (6.1.12)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (2.0.10)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (0.7.5)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.2->sparkmagic) (2.6.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.6.1)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>5.0.0->sparkmagic) (3.0.5)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>5.0.0->sparkmagic) (0.2.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (5.6.1)\n",
            "Requirement already satisfied: jinja2<=3.0.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (2.11.3)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (0.16.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (5.7.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (0.13.3)\n",
            "Requirement already satisfied: jupyter-core>=4.4.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (5.2.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (1.8.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.2->sparkmagic) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.1->sparkmagic) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=0.17.1->sparkmagic) (2022.7.1)\n",
            "Collecting pyspnego[kerberos]\n",
            "  Downloading pyspnego-0.7.0-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.1/130.1 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cryptography>=1.3\n",
            "  Downloading cryptography-39.0.1-cp36-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->sparkmagic) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->sparkmagic) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->sparkmagic) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->sparkmagic) (2.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.8/dist-packages (from cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (1.15.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.2->sparkmagic) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2<=3.0.0->notebook>=4.2->sparkmagic) (2.0.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.8/dist-packages (from jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (6.1.0)\n",
            "Collecting qtconsole\n",
            "  Downloading qtconsole-5.4.0-py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.0/121.0 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter-core>=4.4.0->notebook>=4.2->sparkmagic) (3.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (6.0.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (0.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.2->sparkmagic) (0.6.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (2.16.2)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat->notebook>=4.2->sparkmagic) (4.3.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (8.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=3->autovizwidget>=0.6->sparkmagic) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.2->sparkmagic) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.8/dist-packages (from terminado>=0.8.1->notebook>=4.2->sparkmagic) (0.7.0)\n",
            "Collecting gssapi>=1.6.0\n",
            "  Downloading gssapi-1.8.2.tar.gz (94 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.3/94.3 KB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting krb5>=0.3.0\n",
            "  Downloading krb5-0.4.1.tar.gz (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.7/218.7 KB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package krb5 produced metadata for project name unknown. Fix your #egg=krb5 fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/56/f6/64874f52a47e652e37c8648665374e723500a62d82938ea2db842496d2ed/krb5-0.4.1.tar.gz#sha256=606059b9db52086f4e03e15b161d1d172ccc1172880e42fa5744f99a4407cf57 (from https://pypi.org/simple/krb5/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/56/f6/64874f52a47e652e37c8648665374e723500a62d82938ea2db842496d2ed/krb5-0.4.1.tar.gz#sha256=606059b9db52086f4e03e15b161d1d172ccc1172880e42fa5744f99a4407cf57 (from pyspnego\u001b[0m\u001b[33m->requests_kerberos>=0.8.0->sparkmagic) has inconsistent name: filename has 'krb5', but metadata has 'unknown'\u001b[0m\n",
            "  Downloading krb5-0.4.0.tar.gz (218 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.8/218.8 KB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  WARNING: Generating metadata for package krb5 produced metadata for project name unknown. Fix your #egg=krb5 fragments.\u001b[0m\u001b[33m\n",
            "\u001b[0mDiscarding \u001b[4;34mhttps://files.pythonhosted.org/packages/30/b2/fbafed4fe18a47bc3ffdd7a591bc02c4cf9e96d092793831cb98d596fa16/krb5-0.4.0.tar.gz#sha256=6eb0e91e044d37cb34c84b831310f4a48951f2902d4ebb65b97cbac5ff1b2264 (from https://pypi.org/simple/krb5/) (requires-python:>=3.7)\u001b[0m: \u001b[33mRequested unknown from https://files.pythonhosted.org/packages/30/b2/fbafed4fe18a47bc3ffdd7a591bc02c4cf9e96d092793831cb98d596fa16/krb5-0.4.0.tar.gz#sha256=6eb0e91e044d37cb34c84b831310f4a48951f2902d4ebb65b97cbac5ff1b2264 (from pyspnego\u001b[0m\u001b[33m->requests_kerberos>=0.8.0->sparkmagic) has inconsistent name: filename has 'krb5', but metadata has 'unknown'\u001b[0m\n",
            "  Downloading krb5-0.3.0.tar.gz (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi>=1.12->cryptography>=1.3->requests_kerberos>=0.8.0->sparkmagic) (2.21)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (5.10.2)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (22.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook>=4.2->sparkmagic) (0.5.1)\n",
            "Collecting qtpy>=2.0.1\n",
            "  Downloading QtPy-2.3.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->notebook>=4.2->sparkmagic) (3.12.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from qtpy>=2.0.1->qtconsole->jupyter>=1->hdijupyterutils>=0.6->sparkmagic) (23.0)\n",
            "Building wheels for collected packages: sparkmagic, autovizwidget, hdijupyterutils, gssapi, krb5\n",
            "  Building wheel for sparkmagic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sparkmagic: filename=sparkmagic-0.20.4-py3-none-any.whl size=66610 sha256=d5322c82818869b6894a9a52fac02bfc445b8f27891102337873608b826d5a64\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/ed/c4/e3ce0467862d17663ccba2452a3c2346cce1335b759fdc0b76\n",
            "  Building wheel for autovizwidget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autovizwidget: filename=autovizwidget-0.20.4-py3-none-any.whl size=14689 sha256=11e3fdce9fd2dac273a0da27d8e6b419577244b3aad1033374ae3ca7a28df83a\n",
            "  Stored in directory: /root/.cache/pip/wheels/58/30/df/ece52dff09ef6411556c6167cc9153fcca5b533c82bdc3cb47\n",
            "  Building wheel for hdijupyterutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hdijupyterutils: filename=hdijupyterutils-0.20.4-py3-none-any.whl size=7654 sha256=198a21368124f9d7acf06f1371067a51977692d7a24eca3cf369edde81924b55\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/d5/20/db872ecc9208754f6bf33b0b6dacdb861565f23c8c68d79c10\n",
            "  Building wheel for gssapi (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gssapi: filename=gssapi-1.8.2-cp38-cp38-linux_x86_64.whl size=3753778 sha256=b283079cecd51fedc384aed17960825d1d57ca6c48826e35ac4babc0063cb802\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/9c/de/bcf42d46549ed9ebe297440653c62900202f06b90fd40151a3\n",
            "  Building wheel for krb5 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for krb5: filename=krb5-0.3.0-cp38-cp38-linux_x86_64.whl size=4452298 sha256=85664816955c6a00902486ee7af87ab2db0fa0e2aa7238f80940beac8c6bf185\n",
            "  Stored in directory: /root/.cache/pip/wheels/eb/50/4f/fae1eb12a8947d17d87214170dd042960ef9c5bad97b773310\n",
            "Successfully built sparkmagic autovizwidget hdijupyterutils gssapi krb5\n",
            "Installing collected packages: qtpy, nest_asyncio, krb5, jedi, gssapi, cryptography, pyspnego, qtconsole, requests_kerberos, jupyter, hdijupyterutils, autovizwidget, sparkmagic\n",
            "Successfully installed autovizwidget-0.20.4 cryptography-39.0.1 gssapi-1.8.2 hdijupyterutils-0.20.4 jedi-0.18.2 jupyter-1.0.0 krb5-0.3.0 nest_asyncio-1.5.5 pyspnego-0.7.0 qtconsole-5.4.0 qtpy-2.3.0 requests_kerberos-0.14.0 sparkmagic-0.20.4\n",
            "Looking in indexes: https://test.pypi.org/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting penn-grader==0.5.0\n",
            "  Downloading https://test-files.pythonhosted.org/packages/e4/fa/3a09f42fbed05d27fa4e663e3feed831bc88802342e1a862a6cfb09b77fc/penn_grader-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from penn-grader==0.5.0) (0.3.6)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from penn-grader==0.5.0) (6.0)\n",
            "Installing collected packages: penn-grader\n",
            "Successfully installed penn-grader-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WAJmQ8IAbRs"
      },
      "source": [
        "%load_ext sparkmagic.magics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL6n768EPt9E"
      },
      "source": [
        "### Step 0.2: The Sharp Spark\n",
        "\n",
        "Now, connect your notebook to the EMR cluster you created. In the first cell, copy the link to the Master Public DNS specified in the setup document. You will need to add `http://` to the beginning of the address and the auth details to the end.\n",
        "\n",
        "For example, if my DNS (directly from the AWS EMR console) is `ec2-3-15-237-211.us-east-2.compute.amazonaws.com` my address would be,\n",
        "\n",
        "`http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access`\n",
        "\n",
        "Insert this in the `# TODO # below`. For our example, the cell would read,\n",
        "\n",
        "```bash\n",
        "%spark add -s spark_session -l python -u http://ec2-3-15-237-211.us-east-2.compute.amazonaws.com -a cis545-livy -p password1 -t Basic_Access\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G9QbylT-jqX9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "53b4bf7a801b41ea922c411826d1490b",
            "b382af71d7544b139085a9ef0080f78e",
            "7f7ce85eed9543bfb3ae4b44c968cf04"
          ]
        },
        "outputId": "91d53d00-a4dc-4beb-8346-667e357373d2"
      },
      "source": [
        "# TODO: Copy the line above, enter your Master Public DNS with the proper formatting and host, and \n",
        "# update the password\n",
        "%spark add -s spark_session -l python -u http://ec2-44-202-141-137.compute-1.amazonaws.com -a cis545-livy -p testing1 -t Basic_Access\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Spark application\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table>\n",
              "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1676326118059_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-215.ec2.internal:20888/proxy/application_1676326118059_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-4.ec2.internal:8042/node/containerlogs/container_1676326118059_0002_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53b4bf7a801b41ea922c411826d1490b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SparkSession available as 'spark'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "An error was encountered:\n",
            "Session with name 'spark_session' already exists. Please delete the session first if you intend to replace it.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwKAHhQL0lf7"
      },
      "source": [
        "# If you ever need to restart, you may need to...\n",
        "# %spark delete -s my_session\n",
        "#OR just factory reset runtime under the runtime tab\n",
        "# %spark delete -s spark_session"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yzJetbCfgh3"
      },
      "source": [
        "### Step 0.3: Cluster Log\n",
        "\n",
        "In order to keep track of clusters you have created and terminated as well as give us information about time spent on this assignment, please enter each date and time you created a cluster and the date and time you terminated the cluster. This will not impact your score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "powXPhtphhyy"
      },
      "source": [
        "EX: \n",
        "\n",
        "10/12 9:00am - 10/12 12:00pm\n",
        "\n",
        "10/13 7:00pm - 10/13 9:00pm\n",
        "\n",
        "...\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SokSZv4vih0c"
      },
      "source": [
        "TODO: Create cluster log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1IQjUwNObb8"
      },
      "source": [
        "Enter your 8-digit Penn Key as an integer in the cell \n",
        "below. This will be used in the autograder.  **Please also update the cell below, with the same ID!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zM20juwqqQF"
      },
      "source": [
        "from penngrader.grader import *\n",
        "STUDENT_ID = 99999999"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile config.yaml\n",
        "\n",
        "grader_api_url: 'https://wyv616tp17.execute-api.us-east-1.amazonaws.com/default/Grader'\n",
        "grader_api_key: 'Kd32fl3g3p917iM0zwjiO23Bitj4PO9ga4LektOa'"
      ],
      "metadata": {
        "id": "Ep52cRkLoBYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8Oo_5D7qoWp"
      },
      "source": [
        "grader = PennGrader('config.yaml', 'CIS5450O_Sp23_HW3', STUDENT_ID, STUDENT_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfs-EQZUzF4j"
      },
      "source": [
        "Run the above cells to setup the autograder. Make sure to have set your 8 digit Penn ID in the cell above. It will also import all the modules you need for the homework.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Data Wrangling, Cleaning, and Shaping [18 pts]\n",
        "\n",
        "In this homework we will be working with data from Amazon regarding their products and customer reviews. There are three JSON datasets with information about the products and three JSON datasets with information about the customer reviews for these products. The three categories of products are: Gourmet & Grocery Food, Toys & Games, and Home & Kitchen. \n",
        "\n",
        "\n",
        "The data you will use is stored in an S3 bucket, a cloud storage service. You now need to download it onto the nodes of your [EMR cluster](https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-what-is-emr.html). \n",
        "\n",
        "\n",
        "*Citation for data:* \n",
        "\n",
        "- Image-based recommendations on styles and substitutes.\n",
        "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
        "SIGIR, 2015\n",
        "\n",
        "- Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering.\n",
        "R. He, J. McAuley\n",
        "WWW, 2016"
      ],
      "metadata": {
        "id": "4nyAa0wn1XCD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf_ADEXnIK0b"
      },
      "source": [
        "### Step 1.1: The Stupendous Schema\n",
        "\n",
        "When loading data, Spark will try to infer its structure on its own. This process is faulty because Spark will sometimes infer the type incorrectly. Spark's ability to determine types is not reliable, thus you will need to define a schema for [] and .\n",
        "\n",
        "A schema is a description of the structure of data. We have given you an example for the `products` datasets and you will be defining an explicit schema for the `reviews` datasets called ```schema_reviews```. \n",
        "\n",
        "\n",
        "In Spark, schemas are defined using a `StructType` object. This is a collection of data types, termed `StructField`'s, that specify the structure and variable type of each component of the dataset. For example, suppose we have the following simple JSON object,\n",
        "\n",
        "\n",
        "```json\n",
        "{\n",
        " \"student_name\": \"Data Wrangler\",\n",
        " \"GPA\": 1.4,\n",
        " \"courses\": [\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 545\",\n",
        "     \"semester\": \"Fall 2021\"},\n",
        "    {\"department\": \"Computer and Information Science\",\n",
        "     \"course_id\": \"CIS 555\",\n",
        "     \"semester\": \"Fall 2021\"}\n",
        " ],\n",
        " \"grad_year\": 2022\n",
        " }\n",
        "```\n",
        "\n",
        "We would define its schema as follows,\n",
        "\n",
        "```python       \n",
        "schema = StructType([\n",
        "           StructField(\"student_name\", StringType(), nullable=True),\n",
        "           StructField(\"GPA\", FloatType(), nullable=True),\n",
        "           StructField(\"courses\", ArrayType(\n",
        "                StructType([\n",
        "                  StructField(\"department\", StringType(), nullable=True),\n",
        "                  StructField(\"course_id\", StringType(), nullable=True),\n",
        "                  StructField(\"semester\", StringType(), nullable=True)\n",
        "                ])\n",
        "           ), nullable=True),\n",
        "           StructField(\"grad_year\", IntegerType(), nullable=True)\n",
        "         ])\n",
        "```\n",
        "\n",
        "\n",
        "Each `StructField` has the following structure: `(name, type, nullable)`. The `nullable` flag defines that the specified field may be empty. Your first task is to define the `schema` of the five `reviews` .json files. You can take a look at the schema for one of these files for reference. A smaller version of the `food_reviews` JSON dataset can be found here: https://drive.google.com/file/d/1mWYmGIhanT3fwB3cNkMSp67A6kkI_Mu3/view?usp=sharing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "There is also no grading cell for this step.  But your JSON file won't load if it's wrong, so you have a way of testing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pL-Ps4KWIJ9e"
      },
      "source": [
        "%%spark\n",
        "\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "schema_products = StructType([\n",
        "          StructField(\"asin\", StringType(), nullable=True),\n",
        "           StructField(\"title\", StringType(), nullable=True),\n",
        "           StructField(\"price\", FloatType(), nullable=True),\n",
        "           StructField(\"imUrl\", StringType(), nullable=True),\n",
        "           StructField(\"related\", ArrayType(\n",
        "                StructType([\n",
        "                  StructField(\"also_bought\", ArrayType(StringType()), nullable=True),\n",
        "                  StructField(\"also_viewed\", ArrayType(StringType()), nullable=True),\n",
        "                  StructField(\"bought_together\", ArrayType(StringType()), nullable=True)\n",
        "                ])\n",
        "           ), nullable=True),\n",
        "           StructField(\"salesRank\", MapType(StringType(), StringType()), nullable=True),\n",
        "           StructField(\"brand\", StringType(), nullable=True),\n",
        "           StructField(\"categories\", ArrayType(StringType()), nullable=True),\n",
        "])\n",
        "\n",
        "#TODO: Create a schema for the reviews datasets\n",
        "# [{\"name\":\"index\",\"type\":\"integer\"},\n",
        "#       {\"name\":\"reviewerID\",\"type\":\"string\"},\n",
        "#       {\"name\":\"asin\",\"type\":\"string\"},\n",
        "#       {\"name\":\"reviewerName\",\"type\":\"string\"},\n",
        "#       {\"name\":\"helpful\",\"type\":\"string\"},\n",
        "#       {\"name\":\"reviewText\",\"type\":\"string\"},\n",
        "#       {\"name\":\"overall\",\"type\":\"integer\"},\n",
        "#       {\"name\":\"summary\",\"type\":\"string\"},\n",
        "#       {\"name\":\"unixReviewTime\",\"type\":\"integer\"},\n",
        "#       {\"name\":\"reviewTime\",\"type\":\"string\"}\n",
        "#     ]\n",
        "schema_reviews = StructType([\n",
        "        StructField(\"index\", IntegerType(), nullable=True),\n",
        "        StructField(\"reviewerID\", StringType(), nullable=True),\n",
        "        StructField(\"asin\", StringType(), nullable=True),\n",
        "        StructField(\"reviewerName\", StringType(), nullable=True),\n",
        "        StructField(\"helpful\", StringType(), nullable=True),\n",
        "        StructField(\"reviewText\", StringType(), nullable=True),\n",
        "        StructField(\"overall\", IntegerType(), nullable=True),\n",
        "        StructField(\"summary\", StringType(), nullable=True),\n",
        "        StructField(\"unixReviewTime\", IntegerType(), nullable=True),\n",
        "        StructField(\"reviewTime\", StringType(), nullable=True),\n",
        "])\n",
        "\n",
        "          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Su604X9ggc2"
      },
      "source": [
        "### Step 1.2: The Langorous Load [9 pts]\n",
        "\n",
        "In the following cell, load the three `reviews` datasets and the three `products` datsets from your S3 bucket into Spark dataframes (sdf). If you have constructed `schema_reviews` and `schema_products` correctly, `spark.read.json()` will read in the datasets. The names of the datasets are listed below:\n",
        "\n",
        "Reviews:\n",
        "\n",
        "- `food_reviews.json`\n",
        "- `home_kitchen_reviews.json`\n",
        "- `toys_games_reviews.json`\n",
        "\n",
        "Products:\n",
        "\n",
        "- `food_metadata.json`\n",
        "- `home_kitchen_meta.json`\n",
        "- `toys_games_meta.json`\n",
        "\n",
        "\n",
        "\n",
        "***You do not need to edit this cell***. If this doesn't work, go back to the prior cell and update your schema. Note that the cell below will load data even if your schema is incomplete and has left out some columns of the data, so be sure to check that you have included all of the fields from the JSON."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "food_reviews_sdf = spark.read.json(\"s3://penn-cis545-files/food_reviews.json\", schema=schema_reviews, multiLine = False, primitivesAsString = True)\n",
        "home_kitchen_reviews_sdf = spark.read.json(\"s3://penn-cis545-files/home_kitchen_reviews.json\", schema=schema_reviews, multiLine = False, primitivesAsString = True)\n",
        "toys_games_reviews_sdf = spark.read.json(\"s3://penn-cis545-files/toys_games_reviews.json\", schema=schema_reviews, multiLine = False, primitivesAsString = True)\n"
      ],
      "metadata": {
        "id": "Tcpx-SExfyJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "# Let's print out the first few rows to see how the data looks like in tabular form\n",
        "home_kitchen_reviews_sdf.show(5)"
      ],
      "metadata": {
        "id": "eAgWjMU3ikD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create a spark dataframe for food_prod_sdf, home_kitchen_prod_sdf, and toys_games_prod_sdf\n"
      ],
      "metadata": {
        "id": "ff6o83l3f-Af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMVCVotcE1wv"
      },
      "source": [
        "The cell below shows how to run SQL commands on Spark tables. Use this as a template for all your SQL queries in this notebook. \n",
        "\n",
        "***You do not need to edit this cell***."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "EJ0aRJM-NEYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "# Create SQL-accesible table\n",
        "food_reviews_sdf.createOrReplaceTempView(\"food_reviews\")\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "query = '''SELECT * \n",
        "           FROM food_reviews ORDER BY `asin` DESC LIMIT 10'''\n",
        "\n",
        "# Save the output sdf of spark.sql() as answer_review_sdf and convert to Pandas\n",
        "answer_review_sdf = spark.sql(query)\n",
        "answer_review_sdf.show()\n"
      ],
      "metadata": {
        "id": "7CnbKKqtnicU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLF-swRBTuZG"
      },
      "source": [
        "We can then copy the `answer_review_sdf` to Colab to submit to PennGrader..."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o answer_review_sdf"
      ],
      "metadata": {
        "id": "2z-mDBTorcX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_review_sdf"
      ],
      "metadata": {
        "id": "GeB6VJVSsGmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, you will implement the exact same thing yourself for `food_prod_sdf` and submit it to the grader. Select all columns from `food_prod` and order by the `asin` column (descending). Limit your output to only 10 rows and save your output dataframe to Colab as answer_prod_sdf. "
      ],
      "metadata": {
        "id": "-A9nzeVaG4QV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "# Create SQL-accesible table\n",
        "\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "\n",
        "\n",
        "# Save the output sdf of spark.sql() as answer_prod_sdf \n",
        "\n"
      ],
      "metadata": {
        "id": "9ZGUCgYCoVxY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b1211a01bbe144b196cffdbcd41c07f3",
            "a904dd608b9d4a3989ffec81498c91c7",
            "7ac5ec853a304f898337199ddc5d5ef8"
          ]
        },
        "outputId": "76c78301-ce1e-465c-e25a-b66e4bd5f044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1211a01bbe144b196cffdbcd41c07f3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o answer_prod_sdf"
      ],
      "metadata": {
        "id": "-_-HI3SWO2nh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## AUTOGRADER \n",
        "#4 points\n",
        "grader.grade(test_case_id = 'test_answer_prod', answer = answer_prod_sdf)"
      ],
      "metadata": {
        "id": "z5T4kv0jpPLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell, we want to find the ten most expensive food products. Create `food_prices_sdf` to fetch the data from `food_prod_sdf` table, returning rows with schema `(asin,title,price)` and ordered by price (descending). Limit your sdf to 10 rows. Save your final answer to Colab to submit to PennGrader.\n"
      ],
      "metadata": {
        "id": "G81VMofArbSD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark \n",
        "\n",
        "# TODO: create food_prices_sdf\n"
      ],
      "metadata": {
        "id": "FiX4gS6-sgal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o food_prices_sdf"
      ],
      "metadata": {
        "id": "5eXGu4RTRD-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5 points\n",
        "grader.grade(test_case_id = 'test_food_price', answer = food_prices_sdf)"
      ],
      "metadata": {
        "id": "LEDWWnGTs25a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1.3: Further Cleaning [9 pts]\n",
        "\n",
        "Before we move on to the later parts of this homework, we want to do some cleaning of our datasets. Currently, our datasets are divided based on their category (Food is in its own table, Home and Kitchen is its own table, etc.), but we eventually want to combine our dataframes so that they are easier to work with. If we do this, we want to be able to discern what category each product belongs to, but there currently does not exist any column that tells us the category of a product.\n",
        "\n",
        "\n",
        "For `food_prod_sdf`, create a new column called `category` where every value in this column will be 'Food'. Name this dataframe as `food_prod_final_sdf`. Repeat this for `toys_games_prod_sdf` and `home_kitchen_prod_sdf.` In `toys_games_prod_sdf`, every value in the `category` column will be 'Toys & Games'. In `home_kitchen_prod_sdf`, every value in this column will be 'Home & Kitchen'. Name these dataframes as `toys_games_prod_final_sdf` and `home_kitchen_prod_final_sdf`\n"
      ],
      "metadata": {
        "id": "-2vq-3ZfBYZD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create food_prod_final_sdf\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ReMK5P097cZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "# Do not edit\n",
        "answer_food_final_sdf = food_prod_final_sdf.alias('answer_food_final_sdf')"
      ],
      "metadata": {
        "id": "R1xgTOgRkbTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o answer_food_final_sdf"
      ],
      "metadata": {
        "id": "kY3tkb5UBIFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_food_prod_final', answer = answer_food_final_sdf)"
      ],
      "metadata": {
        "id": "hHwISpDBBKDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create home_kitchen_prod_final_sdf\n",
        "\n"
      ],
      "metadata": {
        "id": "VC6ixe149g2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "# Do not edit\n",
        "answer_home_kitchen_sdf = home_kitchen_prod_final_sdf.alias('answer_home_kitchen_sdf')"
      ],
      "metadata": {
        "id": "4w8ReS5Im_Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o answer_home_kitchen_sdf"
      ],
      "metadata": {
        "id": "LigSdgcfBVz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_home_kitchen_final', answer = answer_home_kitchen_sdf)"
      ],
      "metadata": {
        "id": "vsouyFm-BX37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create toys_games_prod_final_sdf\n"
      ],
      "metadata": {
        "id": "gs4yaHLY9s3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "# Do not edit\n",
        "answer_toys_games_sdf = toys_games_prod_final_sdf.alias('answer_toys_games_sdf')"
      ],
      "metadata": {
        "id": "dDQiupo1nKoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o answer_toys_games_sdf"
      ],
      "metadata": {
        "id": "SpyBoucbBaEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 points\n",
        "grader.grade(test_case_id = 'test_toys_games_final', answer = answer_toys_games_sdf)"
      ],
      "metadata": {
        "id": "iZx3RUv-Bb9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This part is given to you. Combine the newly created `food_prod_sdf`, `toys_games_prod_sdf`, and `home_kitchen_prod_sdf` into a single dataframe called `products_sdf`. Combine `food_reviews_sdf`, `toys_games_reviews_sdf`, and `home_kitchen_reviews_sdf` into a single dataframe called `reviews_sdf`.\n",
        "\n",
        "***You do not need to edit the two cells below***."
      ],
      "metadata": {
        "id": "LZ9cw29N-FBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "# Create SQL-accesible table\n",
        "food_prod_final_sdf.createOrReplaceTempView(\"food_prod_final\")\n",
        "home_kitchen_prod_final_sdf.createOrReplaceTempView(\"home_kitchen_prod_final\")\n",
        "toys_games_prod_final_sdf.createOrReplaceTempView(\"toys_games_prod_final\")\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "query = '''(SELECT * FROM food_prod_final)\n",
        "           \n",
        "           UNION All\n",
        "           \n",
        "           (SELECT * FROM home_kitchen_prod_final)\n",
        "           \n",
        "           UNION All\n",
        "           \n",
        "           (SELECT * FROM toys_games_prod_final)'''\n",
        "\n",
        "# Save the output sdf of spark.sql() as products_sdf\n",
        "products_sdf = spark.sql(query)"
      ],
      "metadata": {
        "id": "rgLYCf8v7T8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "# Create SQL-accesible table\n",
        "food_reviews_sdf.createOrReplaceTempView(\"food_reviews_final\")\n",
        "home_kitchen_reviews_sdf.createOrReplaceTempView(\"home_kitchen_reviews_final\")\n",
        "toys_games_reviews_sdf.createOrReplaceTempView(\"toys_games_reviews_final\")\n",
        "\n",
        "# Declare SQL query to be excecuted\n",
        "query = '''(SELECT * FROM food_reviews_final)\n",
        "           \n",
        "           UNION All\n",
        "           \n",
        "           (SELECT * FROM home_kitchen_reviews_final)\n",
        "           \n",
        "           UNION All\n",
        "           \n",
        "           (SELECT * FROM toys_games_reviews_final)'''\n",
        "\n",
        "# Save the output sdf of spark.sql() as reviews_sdf\n",
        "reviews_sdf = spark.sql(query)\n",
        "\n"
      ],
      "metadata": {
        "id": "Etk5px6f7BAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Join `reviews_sdf` and `products_sdf` on `asin` and name the resulting dataframe as `product_reviews_sdf`. Keep only the following columns from reviews_sdf: `reviewerID`, `asin`, `helpful`, `reviewText`, `overall`, `summary`, `reviewTime`, `unixReviewTime`. Keep the following columns from products_sdf: `title`, `category`, `price`."
      ],
      "metadata": {
        "id": "HNbDHz_Swuz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create product_reviews_sdf\n"
      ],
      "metadata": {
        "id": "atRSOMSq-S-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "# Do not edit - run this after creating product_reviews_sdf\n",
        "product_reviews_sdf.createOrReplaceTempView(\"product_reviews\")\n",
        "ans_product_reviews_sdf = spark.sql(\"SELECT * FROM product_reviews ORDER BY asin ASC LIMIT 10\")"
      ],
      "metadata": {
        "id": "d0zNorGvBx6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o ans_product_reviews_sdf"
      ],
      "metadata": {
        "id": "OnTE_pYGXb6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 3 points\n",
        "grader.grade(test_case_id = 'test_product_review', answer = ans_product_reviews_sdf)"
      ],
      "metadata": {
        "id": "nqflA81xVKoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Analysis! [26 pts]"
      ],
      "metadata": {
        "id": "CsLea-1Tyu0v"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svOO4iLPist4"
      },
      "source": [
        "### Step 2.1: Overall Opinion [5 pts]\n",
        "\n",
        "We would like to find the average rating for products based on category. Using `product_reviews_sdf`, create `avg_rating_sdf` with schema `(category,avg_rating)`. Order your resulting dataframe by `avg_rating` (descending). Note that the column `overall` holds ratings for each product.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create avg_rating_sdf\n"
      ],
      "metadata": {
        "id": "JMXkywgqS8Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o avg_rating_sdf"
      ],
      "metadata": {
        "id": "2MN3JYNOThjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 5 points\n",
        "grader.grade(test_case_id = 'test_avg_rating', answer = avg_rating_sdf)"
      ],
      "metadata": {
        "id": "6S6l60w-TjyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.2: The Category Changes [10 pts]\n",
        "\n",
        "We would like to find the percentage change from 2010 to 2011 in both average overall rating and average price for products based on category.\n",
        "\n",
        "*Hint: Use SUBSTR on the reviewTime field to retrieve the correct year.*\n",
        "\n",
        "Your task includes:\n",
        "\n",
        "\n",
        "*   Finding the average overall rating for each category for the years 2010, 2011\n",
        "*   Calculate the percentage change in rating for 2010 to 2011\n",
        "*   Finding the average price for each category for the years 2010, 2011\n",
        "*   Calculate the percentage change in price for 2010 to 2011\n",
        "*   Return an sdf with the category, change_rating, change_price sorted in descending alphabetical order of category name.\n",
        "\n",
        "Hint: Using multiple queries with intermediate dataframes may be helpful\n",
        "```\n",
        "+-------------------+--------------------+-------------+\n",
        "|           category|       change_rating|.change_price|\n",
        "+-------------------+--------------------|.------------|            \n",
        "+-------------------+--------------------+.------------|\n",
        "```"
      ],
      "metadata": {
        "id": "BW5HpURnC6X7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "#TODO: Create pct_change_sdf\n"
      ],
      "metadata": {
        "id": "dT1v_uDsrZNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o pct_change_sdf"
      ],
      "metadata": {
        "id": "HWJCQBy7rcdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10 points\n",
        "grader.grade(test_case_id = 'test_pct_change', answer = pct_change_sdf)"
      ],
      "metadata": {
        "id": "60JVR1mirfRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2.3: The Popular Products [11 pts]\n",
        "\n",
        "When you navigate to the Amazon webpage, you are often recommended some bestsellers. These bestsellers are recommended to you based on several factors, one of them being positive ratings and reviews. Most popular products have tons of reviews and ratings from customers. We're going to look into popular products bought in a few categories to dive deeper into the consumer market culture. "
      ],
      "metadata": {
        "id": "MtjFKMD-C6va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2.3.1: The Terrific Toys [6 pts]\n",
        "\n",
        "From the product_reviews_sdf, find the 5 most commonly reviewed products around Christmas in the Toys and Games category. In this case, a product reviewed in the month of December (12), will count as 'around Christmas'.\n",
        "\n",
        "The resultant dataframe should contain the 5 most popular products and the number of reviews the product sorted in order of most to least popular in this format:\n",
        "\n",
        "\n",
        "```\n",
        "+-------------------+--------------------+\n",
        "|               asin|         num_reviews|\n",
        "+-------------------+--------------------|\n",
        "+-------------------+--------------------+\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dYksV9qTC7ML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create dec_toys_sdf\n"
      ],
      "metadata": {
        "id": "v6-01lW3tGrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o dec_toys_sdf"
      ],
      "metadata": {
        "id": "M142jjdntIMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# 6 points\n",
        "grader.grade(test_case_id = 'test_dec_toys', answer = dec_toys_sdf)"
      ],
      "metadata": {
        "id": "HI-9TlCNtMYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 2.3.2: Healthy Home [5 pts]"
      ],
      "metadata": {
        "id": "88HTvDoHDKQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are interested in finding the products that are most frequently reviewed within the Home and Kitchen category. Find the top three months in which products from the Home and Kitchen category are most frequently reviewed during the years 2010 to 2014 (inclusive). As before, the `SUBSTR` function in SQL will be helpful for you in your query.\n",
        "\n",
        "Return a dataframe with the schema `(month, num_reviews)` where `month` is the months in which products are most frequently reviewed (month values are integers: ex. 11 = November) and `num_reviews` is the number of reviews for that `month` during the years specified."
      ],
      "metadata": {
        "id": "dR0p39j9Ttqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create home_kitch_three_sdf\n"
      ],
      "metadata": {
        "id": "cakzp4DUthj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o home_kitch_three_sdf"
      ],
      "metadata": {
        "id": "MXis6SJ4yKHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5 points\n",
        "grader.grade(test_case_id = 'test_home_kitch_three', answer = home_kitch_three_sdf)"
      ],
      "metadata": {
        "id": "GTYWMVpsyexw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: The Riveting Reviews [30 pts]\n"
      ],
      "metadata": {
        "id": "9G1cFfEmDLit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.1: The Short Summary [5 pts]\n",
        "\n",
        "You don't always have time to read an entire review. When you're about to buy a product you love, you may scroll through some reviews, eyeballing the summaries and how helpful the reviews are. Eyeballing a few reviews that comment on the usability of the product like \"cheap but works\" or \"effective\" or \"waste of money\" can help you make your decision. Your task is to count the number of reviews per category where the summary length is less than 10 characters and it is considered useful.\n",
        "\n",
        "```\n",
        "+-------------------+--------------------+\n",
        "|           category|     helpful_reviews|\n",
        "+-------------------+--------------------|\n",
        "+-------------------+--------------------+\n",
        "```\n",
        "\n",
        "\n",
        "Name this calculated count of reviews as helpful_reviews and rank categories in descending order of helpful_reviews.\n",
        "\n",
        "\n",
        "Useful rating is defined as a list - [no. of helpful votes, total no,of votes. For example, a [2,3] means 2 out of 3 others found this review helpful. So, to be considered useful a review must have 1 or more total votes, and 1 or more helpful votes."
      ],
      "metadata": {
        "id": "qxasT40_DORo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "\n",
        "#TODO: Create summary_sdf\n"
      ],
      "metadata": {
        "id": "PeKtZsamzw6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o summary_sdf"
      ],
      "metadata": {
        "id": "DcwtLJ9azwoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5 points\n",
        "grader.grade(test_case_id = 'test_summary', answer = summary_sdf)"
      ],
      "metadata": {
        "id": "DW-tKZr_zz7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.2: The Review Ratio [10 pts]\n",
        "\n",
        "Find how helpful a review is in terms of a ratio, and return the top 10 most helpful reviews, the corresponding reviewerID and the helpfulness ratio. The helpfulness ratio can be calulated as:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " $\\frac{Number\\ of \\ helpful \\ votes}{Total \\ number \\ of \\ votes}$\n",
        "\n",
        "\n",
        "The following columns are required in your top10_helpful_sdf: reviewerID, reviewText, helpful_ratio. Make sure to sort the dataframe in descending order of helpful_ratio as well as descending order of reviewerID.\n",
        "\n",
        "**Note:** Ensure to only use useful ratings when computing the ratio"
      ],
      "metadata": {
        "id": "lnmqRSskDRvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "#TODO: Create top10_helpful_sdf\n"
      ],
      "metadata": {
        "id": "sstIprXj1Z83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o top10_helpful_sdf"
      ],
      "metadata": {
        "id": "3LlHp2NS1a6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10 points\n",
        "grader.grade(test_case_id = 'test_helpful_ratio', answer = top10_helpful_sdf)"
      ],
      "metadata": {
        "id": "QMPtc6A41fWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3.3: The Sensible Sentiment\n",
        "\n",
        "We would like to determine the sentiment of some of the reviews across the categories. Sentiment Analysis is used with textual data very regularly. A common application includes determing whether the review associated with a product is positive, negative or neutral. We are going to attempt to determine the sentiment associated with the top 10 most helpful reviews from the food category, based on the ratio calculated in the previous section. "
      ],
      "metadata": {
        "id": "rQR-vYBwDYjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3.3.1: The Top Ten [5 pts]\n",
        "\n",
        "Retrieve the asin and reviewText for the top 10 most helpful reviews for the food category, where how helpful a review is determined by the ratio calculated in the previous question. Sort in descending order of helpfulness as well as reviewerID. "
      ],
      "metadata": {
        "id": "omdhaC5FDb88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "#TODO : Create top10_sdf\n"
      ],
      "metadata": {
        "id": "lfry_qaG1ixe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o top10_sdf"
      ],
      "metadata": {
        "id": "E50DWLFx1jxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5 points\n",
        "grader.grade(test_case_id = 'test_top10_helpful', answer = top10_sdf)"
      ],
      "metadata": {
        "id": "ZgDLT4IP1nSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 3.3.2: Positively Positive [10 pts]\n",
        "\n",
        "Recall from your previous assignment, you were asked to return a tokenized list of words (removing stop words and checking for alphabets). \n",
        " \n",
        "Using the pandas dataframe created from the previous section, create a new column called 'tokenized' which contains the tokens from the reviewText. (Hint: Use the apply function to call the custom tokenize function from HW2). \n",
        "\n",
        "Sentiment analysis is a a natural language processing (NLP) technique that attempts to identify the emotional tone behind a body of text. In this case, we will try to determine how posiitve a review is.\n",
        "\n",
        "\n",
        "We will be calculating the sentiment associated with each of these reviews. To read more about sentiment analysis, [click here](https://towardsdatascience.com/sentiment-analysis-concept-analysis-and-applications-6c94d6f58c17)."
      ],
      "metadata": {
        "id": "mSYdQxVlDgoZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "r54o6GQ_3dq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to dataframe & tokenize list \n",
        "#TODO: Add your custom tokenize content function \n",
        "def tokenize_content(content):\n",
        "  pass\n",
        "\n",
        "  \n",
        "#TODO: Apply your function to reviewText\n"
      ],
      "metadata": {
        "id": "jO_nTqwY3mgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Run the cell below to understand how to the inbuilt Sentiment Analyzer works. As you can see below, the compound variable contains the overall sentiment of the sentence \"Wow, NLTK is really powerful!\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a4mfalDpCKNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#EXAMPLE CELL\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "sia.polarity_scores(\"Wow, NLTK is really powerful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJMLaQ8f3oFk",
        "outputId": "7df71abd-1013-40b3-c350-0fc3bc5b50ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'neg': 0.0, 'neu': 0.295, 'pos': 0.705, 'compound': 0.8012}"
            ]
          },
          "metadata": {},
          "execution_count": 329
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to now add a column to your dataframe called sentiment, and calculate the sentiment associated with each review. Sort this dataframe from highest sentiment to lowest.\n",
        "\n",
        "Now, moving back into SQL, we need you to run a query on the product_reviews_sdf to retrieve the titles of 2 products with the highest sentiment. Use the asins from the first two rows of your sorted pandas dataframes, and retrieve the just product titles. Save this result into top2_sentiment_sdf. Please do not hardcode the values of the titles, we expect an SQL approach to obtain the titles. However, you may hardcode ASIN values that you obtain from the top10_tokenize_sdf in your SQL query.\n",
        "\n",
        "You will be passing two results to the autograder : top10_tokenize_sdf (asin, reviewText, tokenized, sentiment columns), top2_sentiment_sdf (only title column)"
      ],
      "metadata": {
        "id": "J0S-6ZB0FCZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "def retrieve_sentiment(content):\n",
        "  pass"
      ],
      "metadata": {
        "id": "NYDHG-1I3qBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Create sentiment column and sort top10_tokenize_sdf by sentiment \n",
        "\n"
      ],
      "metadata": {
        "id": "F2QN2RbE5q8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%spark\n",
        "#TODO: Create top2_sentiment_sdf\n"
      ],
      "metadata": {
        "id": "U_zAPYJw7-5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%spark -o top2_sentiment_sdf"
      ],
      "metadata": {
        "id": "vGgMcQ2U8fNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 10 points\n",
        "grader.grade(test_case_id = 'test_sentiment_positive', answer = (top2_sentiment_sdf,top10_tokenize_sdf))\n",
        "\n"
      ],
      "metadata": {
        "id": "ZPhfsFRc3sHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Candy Land [26 pts]\n",
        "\n",
        "In this section, we will be working with a dataset from a poll where each row holds a person's favorite and second favorite candy/sweet. \n",
        "\n",
        "Let's introduce a fun little concept called the Bacon Number! The Bacon number of an actor or actress is the number of degrees of separation they have from actor Kevin Bacon, as defined by the game known as Six Degrees of Kevin Bacon. For example, Kevin Bacon's Bacon number is 0. If an actor works in a movie with Kevin Bacon, the actor's Bacon number is 1. If an actor works with an actor who worked with Kevin Bacon in a movie, the first actor's Bacon number is 2, and so forth.\n",
        "\n",
        "How do we implement the \"Super Bacon\" for our dataset though? We define this number as follows: if candy A is someone's favorite candy and candy B is that person's second favorite candy while candy B is someone else's favorite candy and candy C is their second favorite candy, then the super bacon of C with respect to A will be 2.\n",
        "\n",
        "Now to calculate this number, we'll use the concepts of graphs and BFS!"
      ],
      "metadata": {
        "id": "rGyXyvhBDr0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "E_i8gCbAEuHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "appName = \"PySpark\"\n",
        "\n",
        "# Create Spark session\n",
        "spark = SparkSession.builder.appName(appName).getOrCreate()"
      ],
      "metadata": {
        "id": "s7AlDICWEt7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.1 “Traversing” a Graph\n",
        "\n",
        "Let's review how BFS works!"
      ],
      "metadata": {
        "id": "zkJ2eyNrDtUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import Image as I\n",
        "\n",
        "bfsgif =\\\n",
        "'https://upload.wikimedia.org/wikipedia/commons/5/5d/Breadth-First-S'+\\\n",
        "'earch-Algorithm.gif'\n",
        "dfsgif=\\\n",
        "'https://upload.wikimedia.org/wikipedia/commons/7/7f/Depth-First-Search.gif'"
      ],
      "metadata": {
        "id": "sH6pKfJJEzpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Step 4.1.1 Intro to Distributed Breadth-First Search\n",
        "\n",
        "\n",
        "To start off, we will be implementing a graph traversal algorithm known as Breadth First Search. It works in a way that's equivalent to how a stain spreads on a white t-shirt. Take a look at the graph below:\n",
        "\n",
        "<p align = \"center\">\n",
        "<img src = \"https://imgur.com/WU3AUwg.png\" width= \"600\" align =\"center\"/>\n",
        "\n",
        "* Consider starting BFS from point A (green). This is considered the starting frontier/singular origin node.\n",
        "* The first round of BFS would involve finding all the nodes directly reachable from A, namely B-F (blue circles). These blue nodes make up the next frontier at depth 1 away from our starting node A.\n",
        "* The second round would then be identifying the red nodes which are the neighbors of the blue nodes. Now, the red nodes all belong to a frontier 2 depth away from A. Note that node A is also a neighbor of a blue node. However, since it has already been visited, it does not get added to this frontier.\n",
        "\n",
        "This process continues until all the nodes in the graph have been visited.\n",
        "If you would like to learn more about BFS, we highly suggest looking [here](https://www.tutorialspoint.com/data_structures_algorithms/breadth_first_traversal.html).\n",
        "\n",
        "\n",
        "We will now be implementing **spark_bfs(G, N, d)**, our spark flavor of BFS that takes a graph **G**, a set of origin nodes **N**, and a max depth **d**.\n",
        "\n",
        "In order to write a successful BFS function, you are going to need to figure out \n",
        "1. how to keep track of nodes that we have visited\n",
        "2. how to properly find all the nodes at the next depth\n",
        "3. how to avoid cycles and ensure that we do not constantly loop through the same edges (take a look at J-K in the graph)"
      ],
      "metadata": {
        "id": "Oo3y1k4EDyod"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### [OPTIONAL/ADDITIONAL] BFS vs. DFS Animation \n",
        "Run the code cells below to understand the difference between depth and breadth first search! (Source: Wikimedia Commons)"
      ],
      "metadata": {
        "id": "NYjIiz_0E7Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not edit\n",
        "print('BFS:')\n",
        "I(url=bfsgif)"
      ],
      "metadata": {
        "id": "1wteWWS7E3KK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not edit\n",
        "print('DFS:')\n",
        "I(url=dfsgif)"
      ],
      "metadata": {
        "id": "2-S7vJ9pE40i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4.1.2 Implement One Traversal [6 pts]\n",
        "\n",
        "To break down this process, let's think about how we would implement a single traversal of the graph. That is given the green node in the graph above, how are we going to get the blue nodes?\n",
        "\n",
        "\n",
        "Consider the simple graph below **which is different from the graph in the image above**:"
      ],
      "metadata": {
        "id": "pyuyXzNjEjEp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not edit\n",
        "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
        "\n",
        "simple = [('A', 'B'),\n",
        "         ('A', 'C'),\n",
        "         ('A', 'D'),\n",
        "         ('C', 'F'),\n",
        "         ('F', 'A'),\n",
        "         ('B', 'G'),\n",
        "         ('G', 'H'),\n",
        "         ('D', 'E')]\n",
        "\n",
        "simple_dict = {'from_node': ['A', 'A', 'A', 'C', 'F', 'B', 'G', 'D'],\n",
        "       'to_node': ['B', 'C', 'D', 'F', 'A', 'G', 'H', 'E']}\n",
        "\n",
        "simple_graph_df = pd.DataFrame.from_dict(simple_dict)\n",
        "simple_graph_sdf = spark.createDataFrame(simple_graph_df)\n",
        "simple_graph_sdf.show()"
      ],
      "metadata": {
        "id": "74yrRK9AE-G5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3da6db-dfe9-4856-c9e6-e7ae398f7f35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+\n",
            "|from_node|to_node|\n",
            "+---------+-------+\n",
            "|        A|      B|\n",
            "|        A|      C|\n",
            "|        A|      D|\n",
            "|        C|      F|\n",
            "|        F|      A|\n",
            "|        B|      G|\n",
            "|        G|      H|\n",
            "|        D|      E|\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, each row of this dataframe represents an edge between two nodes. Here, we are looking at a directed graph, which means that A-->B does not represent the same edge as B-->A.\n",
        "Let's define our starting node as follows:"
      ],
      "metadata": {
        "id": "KTbktZQ9FGox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "smallOrig = [{'node': 'A'}]"
      ],
      "metadata": {
        "id": "POWS1z8hE-ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, bfs with graph G, starting from smallOrig to depth 1, or spark_bfs(G, smallOrig, 1) would output as follows:"
      ],
      "metadata": {
        "id": "xwgYZNZ-FHZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Do not edit\n",
        "simple_1_round_dict = {'node': ['B', 'D', 'C', 'A'],\n",
        "       'distance': [1, 1, 1, 0]}\n",
        "simple_1_round_bfs_df = pd.DataFrame.from_dict(simple_1_round_dict)\n",
        "simple_1_round_bfs_sdf = spark.createDataFrame(simple_1_round_bfs_df)\n",
        "simple_1_round_bfs_sdf.show()"
      ],
      "metadata": {
        "id": "-hBlW2rPFKig",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffa0b543-57b5-4451-f5db-b1638fc36975"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+--------+\n",
            "|node|distance|\n",
            "+----+--------+\n",
            "|   B|       1|\n",
            "|   D|       1|\n",
            "|   C|       1|\n",
            "|   A|       0|\n",
            "+----+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, this dataframe logs each node with its corresponding distance away from A. Moreover, we also know that these nodes are **visited**. \n",
        "\n",
        "Hopefully, you can see how we can use our original graph and this new information to find the nodes at depth two. \n",
        "\n",
        "This is exactly what we will try to accomplish with **spark_bfs_1_round(visited_nodes)** which will ultimately be the inner function of **spark_bfs** that we use to perform exactly one traversal of a graph.\n",
        "\n",
        "**TODO**: Write **spark_bfs_1_round(visted_nodes)** that takes the currently dataframe of visited_nodes, performs one round of BFS, and returns an updated visited nodes dataframe. You should assume that a temporary sdf `G` already exists. Sort your dataframe by node in **ascending** order."
      ],
      "metadata": {
        "id": "15Qk24fGFO_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spark_bfs_1_round(visited_nodes):\n",
        "  \"\"\"\n",
        "  :param visited_nodes: dataframe with columns node and distance\n",
        "  :return: dataframe of updated visuted nodes, with columns node and distance\n",
        "  \"\"\"\n",
        "  # TODO: Complete this function to implement 1 round of BFS\n"
      ],
      "metadata": {
        "id": "OQYlGzw6FSms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run the inner function on simple_1_round_bfs_sdf (i.e. result of 1 round of BFS on the simple graph) and store the results in simple_bfs_result. This is ultimately what the output of BFS to depth 2 should look like."
      ],
      "metadata": {
        "id": "za3-poGjFaRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Run spark_bfs_1_round on simple_1_round_bfs_sdf\n"
      ],
      "metadata": {
        "id": "IUTyEdbpFUVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert this result to Pandas, sort your dataframe by ***node*** ascending, and submit it to the autograder."
      ],
      "metadata": {
        "id": "evUUcJ0yFdhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Convert simple_bfs_result to Pandas sorted by node\n",
        "simple_bfs_test = ..."
      ],
      "metadata": {
        "id": "8wmaNu9YZIlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 points\n",
        "grader.grade(test_case_id = 'checksimpleBFS', answer = simple_bfs_test)"
      ],
      "metadata": {
        "id": "Z0IyMp1dFYEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Step 4.1.3 Full BFS Implementation\n",
        "\n",
        "Now, we will fully implement **spark_bfs**. This function should iteratively call your implemented version of **spark_bfs_1_round** and ultimately return the output of this function at **max_depth**.\n",
        "\n",
        "You are also responsible for initializing the starting dataframe, that is converting the list of origin nodes into a spark dataframe with the nodes logged at distance 0.\n",
        "\n",
        "Consider the following: \n",
        "\n",
        "```python\n",
        "schema = StructType([\n",
        "            StructField(\"node\", StringType(), True)\n",
        "        ])\n",
        "\n",
        "    my_sdf = spark.createDataFrame(origins, schema)\n",
        "```\n",
        "\n",
        "The schema ultimately specifies the structure of the Spark DataFrame with a string `node` column. It then calls **spark.createDataFrame** to map this schema to the **origins** nodes. Also, you are responsible for ensuring that a view of your graph is available within this function. (Note: you will also need to add in a distance column)\n",
        "\n",
        "**TODO:** implement **spark_bfs(G,origins,max_depth)** and run on **candy_graph_sdf** initalized in 4.3. Note: you may want to run tests on the **simple_graph** example as the `candy_graph_sdf` will take quite some time to run.\n",
        "\n",
        "These imports might be useful: \n",
        "`from pyspark.sql.types import StructType, StructField, StringType, IntegerType`"
      ],
      "metadata": {
        "id": "Nk2soBT2EoRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "# TODO: iterative search over undirected graph\n",
        "# Worth 5 points directly, but will be needed later\n",
        "\n",
        "def spark_bfs(G, origins, max_depth):\n",
        "  \"\"\" runs distributed BFS to a specified max depth\n",
        "\n",
        "  :param G: graph dataframe from 4.3\n",
        "  :param origins: list of origin nodes stored as {\"node\": nodeValue}\n",
        "  :param max_depth: integer value of max depth to run BFS to\n",
        "  :return: dataframe with columns node, distance of all visited nodes\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "tIbkTuSTFhR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test that this function works on the simple example."
      ],
      "metadata": {
        "id": "2LRo1B8HFjs1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simple_bfs_iterative_result = spark_bfs(simple_graph_sdf, smallOrig, 3)\n",
        "simple_bfs_iterative_result.show()"
      ],
      "metadata": {
        "id": "rL9XjwyPFhyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.2: Popular Candies [5 pts]"
      ],
      "metadata": {
        "id": "FBIXSJQmFqZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the `candy_graph_extra.csv` dataset."
      ],
      "metadata": {
        "id": "QkpLsfunGKmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! wget -nc https://storage.googleapis.com/penn-cis5450/candy_graph_extra.csv"
      ],
      "metadata": {
        "id": "ahPmj-ZkrJES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to take a look at our candy dataset. Let's load the data to **candy_sdf** and see how it looks."
      ],
      "metadata": {
        "id": "J9tlk9skXegN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "candy_sdf = spark.read.csv(\"/content/candy_graph_extra.csv\", header=True)"
      ],
      "metadata": {
        "id": "F7EsOfD5NMea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candy_sdf.show()"
      ],
      "metadata": {
        "id": "bN-1SaYENlD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at which candies were the most popular. \n",
        "\n",
        "***TODO***: Count the number of times that each candy was chosen as their favorite (each candy appears in the FirstCandy column). Return a dataframe with schema (FirstCandy,Votes) where Votes is the number of times that each candy was chosen as a person's favorite. Order your dataframe by Votes."
      ],
      "metadata": {
        "id": "JPftgXHoXneJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n"
      ],
      "metadata": {
        "id": "idqHrl-BNrH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the dataframe to pandas and pass it to the grader."
      ],
      "metadata": {
        "id": "Qpjvw2j3I9Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_df = ..."
      ],
      "metadata": {
        "id": "-UV5KUtO_RlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'checkCandyCount', answer = count_df)"
      ],
      "metadata": {
        "id": "tF6l4PLF_OKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the Candy dataset, let's convert it to a graph sdf just like the one we had in section 4.1 (P.S. it's not as hard as it sounds).\n",
        "\n",
        "**TODO:** Create **candy_graph_sdf** that has the columns *from_node* and *to_node*. from_node has all the entries from the *FirstCandy* column and to_node has all the entries from the *SecondCandy* column."
      ],
      "metadata": {
        "id": "Y7-sQIg9Ydze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "pY37jvGkOBeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'checkCandyGraph', answer = candy_graph_sdf.toPandas())"
      ],
      "metadata": {
        "id": "_K7U-RF4_eSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4.3: The Super Bacon Search [15 pts]"
      ],
      "metadata": {
        "id": "duQ9JsnL_hqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we found out that the Starburst Sour Chews and Kit Kat Chunky Mug Egg  have been the most popular candies for this group of customers. So let's find out the Super Bacons of candies with respect to Starburst Sour Chews."
      ],
      "metadata": {
        "id": "oQyp_EjeM_16"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we'll create an origin node using Starburst Sour Chews."
      ],
      "metadata": {
        "id": "W4iQD5nJOrJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "orig = [{'node': 'Starburst Sour Chews'}]"
      ],
      "metadata": {
        "id": "Bk8yuvpsOErc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run the **spark_bfs()** function on **candy_graph_sdf** using this origin node and a max depth of 5. Store the result in **bfs_5**."
      ],
      "metadata": {
        "id": "rw_lX0nPPoYT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bfs_5 = spark_bfs(candy_graph_sdf, orig, 5)\n"
      ],
      "metadata": {
        "id": "cXukRdc-Ox0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bfs_5.show()"
      ],
      "metadata": {
        "id": "BJX5QrQxnXeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the result to pandas, store it to answer_df and pass this to the grader."
      ],
      "metadata": {
        "id": "gkmBQnFJQJjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_df = bfs_5.toPandas()"
      ],
      "metadata": {
        "id": "_bFCML1YnYmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grader.grade(test_case_id = 'checkBFS', answer = answer_df)"
      ],
      "metadata": {
        "id": "-1IVMn3Q_rob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voila! We're all done."
      ],
      "metadata": {
        "id": "3dePHd9CQW-C"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHVFHPVIrvu1"
      },
      "source": [
        "# HW Submission\n",
        "\n",
        "Before you submit on Gradescope (you must submit your notebook to receive credit):\n",
        "\n",
        "\n",
        "1.   Restart and Run-All to make sure there's nothing wrong with your notebook\n",
        "2.   **Double check that you have the correct PennID (all numbers) in the autograder**. \n",
        "3. Make sure you've run all the PennGrader cells\n",
        "4. Go to the \"File\" tab at the top left, and click \"Download .ipynb\" and then \"Download .py\".  **Rename** the files to `homework3.ipynb` and `homework3.py` respectively and upload them to Gradescope \n",
        "\n",
        "**Let the course staff know ASAP if you have any issues submitting, but otherwise best of luck!**"
      ]
    }
  ]
}