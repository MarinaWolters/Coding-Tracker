{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaWolters/Coding-Tracker/blob/master/CIS_521_final_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brNDGX-kvzds"
      },
      "source": [
        "# Template for Notes for CIS 5210 Midterm 2\n",
        "\n",
        "The midterm exam for CIS 5210 will be **open notes**. No other matierals are allowed for the exam - you may not use the textbook, the internet, a Python environment, or communicate with anyone during the exam. \n",
        "\n",
        "This python notebook is where you should save all of your notes.  It provides a structured template that allows you to add summaries for each of the textbook sections that are part of the required reading, plus room for lecture notes and definitions.\n",
        "\n",
        "Your summaries must be written in your own words.  Do not copy and paste from the textbook.  Do not share your notes with classmates, or copy any of your classmates' notes.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPtM8g7R0Pds"
      },
      "source": [
        "# Instructions\n",
        "\n",
        "1. Make a copy of this Google Colab file. From the colab menu, pick **File > Save a copy in Drive**.  That will save a copy of this notebook in your Google drive.\n",
        "2. Go through each section of the notebook, and fill in all the summaries for each chapter and its corresponding sections.  Be sure to save your work frequently!\n",
        "3. Read the notes below about how you can format your summaries.\n",
        "4. Submit your version of this notebook to Gradescope before you start the exam.\n",
        "5. If you are taking the exam in person, then you must print a copy of your notes for yourself.   Here's how to print: \n",
        "* Run the last cell of this notebook\n",
        "* Click the \"Mirror cell in tab\" button (this is located in the upper right corner of the cell, immediately to the right of the trash can).  This will open the cell.  \n",
        "* In Chrome, you can then print your notes using the **File > Print** option from the browser's menu.  \n",
        "* In the print options, change the layout from Landscape to Portrait. \n",
        "* Clicking the Save, will save a PDF that you can then print.\n",
        "* If you are having trouble printing directly from Colab, you may copy and paste the last cell into Google Docs and print from there. \n",
        "\n",
        "## Formatting Notes\n",
        "\n",
        "1. The max length for each section's summary is 500 words / 3000 characters.\n",
        "2. You can also write a 2000 word summary of the lectures for each module.\n",
        "3. You may also write definitions of concepts for each module.  The definitions should be written in your own words (not copy and pasted), and they should be no more than 20 words long for each concept. \n",
        "2. You can use Markdown formatting for your summaries. \n",
        "4. You can use Latex formatting for equations. Put two dollar signs before and after your equation like this: \n",
        "$$ \\epsilon = \\frac{1}{mc^2} $$\n",
        "5. When you write python code or psuedocode algorithms, you should surround it with triple backticks like this \n",
        "\\`\\`\\` def function(): ... \\`\\`\\`\n",
        "This will nicely format you code like this.\n",
        "\n",
        "```\n",
        "def function():\n",
        "      print('hello world')\n",
        "```\n",
        "6. Write \\ whenever you want to keep writing on the next line without a newline being inserted."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw5IpDLCZa4W"
      },
      "source": [
        "## Example summary\n",
        "\n",
        "This is a sample summary of [Section 2.1.1](https://web.stanford.edu/~jurafsky/slp3/2.pdf ) of Jurfasky and Martin's Speech and Language Processing's book\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzaavW96tcxM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96a9e0ee-bb8b-4cdb-fda8-3ebe3e0525fb"
      },
      "source": [
        "chapter_2_1_example = \"\"\"\n",
        "Regular expressions are an algebraic notation for characterizing \\\n",
        "a set of strings that are useful for searching for a pattern or through a corpus.\\\n",
        "Only the first matching result will be shown. The search string is case sensitive, \\\n",
        "but it can consist of a single character or a sequence of characters. \\\n",
        "While square braces specify a disjunction of characters, the dash can more easily look \\\n",
        "for characters over a range. The caret specifies the non-existence of a character. \\\n",
        "Additionally, the (?) is used to for flexibility on a single character, meaning \\\n",
        "\"the preceding character or nothing\". More importantly, the Kleene star means zero or \\\n",
        "more occurrences of the immediately previous character or regular expression in square braces.\\\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "length = len(chapter_2_1_example.split(\" \"))\n",
        "print(\"The example summary is {len} words long, so you could write an additional \\\n",
        "{remaining} words.\".format(len=length, remaining=(500-length)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The example summary is 116 words long, so you could write an additional 384 words.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3Uk9gQTevcj"
      },
      "source": [
        "## Example summary length\n",
        "\n",
        "The maximum summary length for each section is 500 words / 3000 characters.  Hopefully you'll find that to be quite generous. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DvZ57A-djo1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037a1548-c49c-49bf-a008-2388c319f2b3"
      },
      "source": [
        "# This is an example of a 500 word placeholder\n",
        "placeholder = \"\"\"ultrices vitae auctor eu augue ut lectus arcu bibendum at varius \\\n",
        " vel pharetra vel turpis nunc eget lorem dolor sed viverra ipsum nunc aliquet \\\n",
        " bibendum enim facilisis gravida neque convallis a cras semper auctor neque vitae \\\n",
        " tempus quam pellentesque nec nam aliquam sem et tortor consequat id porta nibh \\\n",
        " venenatis cras sed felis eget velit aliquet sagittis id consectetur purus ut \\\n",
        " faucibus pulvinar elementum integer enim neque volutpat ac tincidunt vitae semper \\ \n",
        " quis lectus nulla at volutpat diam ut venenatis tellus in metus vulputate eu \\\n",
        " scelerisque felis imperdiet proin fermentum leo vel orci porta non pulvinar neque \\ \n",
        " laoreet suspendisse interdum consectetur libero id faucibus nisl tincidunt eget \\\n",
        " nullam non nisi est sit amet facilisis magna etiam tempor orci eu lobortis \\\n",
        " elementum nibh tellus molestie nunc non blandit massa enim nec dui nunc mattis \\\n",
        " enim ut tellus elementum sagittis vitae et leo duis ut diam quam nulla porttitor \\\n",
        " massa id neque aliquam vestibulum morbi blandit cursus risus at ultrices mi tempus \\\n",
        " imperdiet nulla malesuada pellentesque elit eget gravida cum sociis natoque \\\n",
        " penatibus et magnis dis parturient montes nascetur ridiculus mus mauris vitae \\\n",
        " ultricies leo integer malesuada nunc vel risus commodo viverra maecenas accumsan \\\n",
        " lacus vel facilisis volutpat est velit egestas dui id ornare arcu odio ut sem nulla \\\n",
        " pharetra diam sit amet nisl suscipit adipiscing bibendum est ultricies integer \\\n",
        " quis auctor elit sed vulputate mi sit amet mauris commodo quis imperdiet massa \\\n",
        " tincidunt nunc pulvinar sapien et ligula ullamcorper malesuada proin libero nunc \\\n",
        " consequat interdum varius sit amet mattis vulputate enim nulla aliquet porttitor \\\n",
        " lacus luctus accumsan tortor posuere ac ut consequat semper viverra nam libero \\\n",
        " justo laoreet sit amet cursus sit amet dictum sit amet justo donec enim diam \\\n",
        " vulputate ut pharetra sit amet aliquam id diam maecenas ultricies mi eget mauris \\\n",
        " pharetra et ultrices neque ornare aenean euismod elementum nisi quis eleifend quam \\\n",
        " adipiscing vitae proin sagittis nisl rhoncus mattis rhoncus urna neque viverra justo \\\n",
        " nec ultrices dui sapien eget mi proin sed libero enim sed faucibus turpis in \\\n",
        " eu mi bibendum neque egestas congue quisque egestas diam in arcu cursus euismod \\\n",
        " quis viverra nibh cras pulvinar mattis nunc sed blandit libero volutpat sed cras \\\n",
        " ornare arcu dui vivamus arcu felis bibendum ut tristique et egestas quis ipsum \\\n",
        " suspendisse ultrices gravida dictum fusce ut placerat orci nulla pellentesque \\\n",
        " dignissim enim sit amet venenatis urna cursus eget nunc scelerisque viverra mauris \\\n",
        " in aliquam sem fringilla ut morbi tincidunt augue interdum velit euismod in \\\n",
        " pellentesque massa placerat duis ultricies lacus sed turpis tincidunt id aliquet \\\n",
        " risus feugiat in ante metus dictum at tempor commodo ullamcorper a lacus vestibulum \\\n",
        " sed arcu non odio euismod lacinia at quis risus sed vulputate odio ut enim blandit \\\n",
        " volutpat maecenas volutpat blandit aliquam etiam erat velit scelerisque in \\\n",
        " dictum non consectetur a erat nam at lectus urna\"\"\"\n",
        "\n",
        "print(\"The length in words is\", len(placeholder.split(' ')))\n",
        "print(\"The length in characters is\", len(placeholder))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The length in words is 500\n",
            "The length in characters is 3030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yG3-YncJfTB"
      },
      "source": [
        "# Permissions to use your summaries\n",
        "\n",
        "We'd like to use your summaries to help develop better educational resources like [remember.school](https://remember.school) or a question-answering system about the textbook.  \n",
        "\n",
        "1. Please enter your name.  \n",
        "\n",
        "2. If you grant permission for us to use your summaries under a [Creative Commons License](https://creativecommons.org/licenses/by/4.0/), please set ```permission = True```.  If you do not grant permission, please set it to false. Your answer won't affect your grade.\n",
        "\n",
        "3. If you grant us permission, and you'd like to remain anonymous, set ```anonymous = True```.  We'll remove your name before sharing your summaries with anyone. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RECnJArJew4"
      },
      "source": [
        "name = \"Marina Wolters\"\n",
        "permission = True\n",
        "anonymous = False"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# List of Figures from AIMA\n"
      ],
      "metadata": {
        "id": "oFE-7netSxts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title List of all figures and their URLs\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "\n",
        "AIMA_figures = {\n",
        "\t'Figure 1.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_1.1.jpg',\n",
        "\t'Figure 1.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_1.2.jpg',\n",
        "\t'Figure 1.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_1.3.jpg',\n",
        "\t'Figure 10.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.1.jpg',\n",
        "\t'Figure 10.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.2.jpg',\n",
        "\t'Figure 10.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.3.jpg',\n",
        "\t'Figure 10.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.4.jpg',\n",
        "\t'Figure 10.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.5.jpg',\n",
        "\t'Figure 10.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_10.6.jpg',\n",
        "\t'Figure 11.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.10.jpg',\n",
        "\t'Figure 11.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.12.jpg',\n",
        "\t'Figure 11.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.13.jpg',\n",
        "\t'Figure 11.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.14.jpg',\n",
        "\t'Figure 11.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.15.jpg',\n",
        "\t'Figure 11.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.3.jpg',\n",
        "\t'Figure 11.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.5.jpg',\n",
        "\t'Figure 11.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.6.jpg',\n",
        "\t'Figure 11.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_11.9.jpg',\n",
        "\t'Figure 12.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_12.4.jpg',\n",
        "\t'Figure 12.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_12.5.jpg',\n",
        "\t'Figure 12.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_12.6.jpg',\n",
        "\t'Figure 13.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.1.jpg',\n",
        "\t'Figure 13.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.10.jpg',\n",
        "\t'Figure 13.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.14.jpg',\n",
        "\t'Figure 13.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.15.jpg',\n",
        "\t'Figure 13.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.19.jpg',\n",
        "\t'Figure 13.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.2.jpg',\n",
        "\t'Figure 13.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.21.jpg',\n",
        "\t'Figure 13.22': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.22.jpg',\n",
        "\t'Figure 13.23': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.23.jpg',\n",
        "\t'Figure 13.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.3.jpg',\n",
        "\t'Figure 13.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.4.jpg',\n",
        "\t'Figure 13.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.7.jpg',\n",
        "\t'Figure 13.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.8.jpg',\n",
        "\t'Figure 13.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_13.9.jpg',\n",
        "\t'Figure 14.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.1.jpg',\n",
        "\t'Figure 14.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.10.jpg',\n",
        "\t'Figure 14.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.11.jpg',\n",
        "\t'Figure 14.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.12.jpg',\n",
        "\t'Figure 14.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.13.jpg',\n",
        "\t'Figure 14.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.14.jpg',\n",
        "\t'Figure 14.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.15.jpg',\n",
        "\t'Figure 14.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.16.jpg',\n",
        "\t'Figure 14.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.18.jpg',\n",
        "\t'Figure 14.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.2.jpg',\n",
        "\t'Figure 14.20': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.20.jpg',\n",
        "\t'Figure 14.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.21.jpg',\n",
        "\t'Figure 14.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.3.jpg',\n",
        "\t'Figure 14.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.5.jpg',\n",
        "\t'Figure 14.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.7.jpg',\n",
        "\t'Figure 14.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.8.jpg',\n",
        "\t'Figure 14.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_14.9.jpg',\n",
        "\t'Figure 15.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.1.jpg',\n",
        "\t'Figure 15.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.10.jpg',\n",
        "\t'Figure 15.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.12.jpg',\n",
        "\t'Figure 15.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.13.jpg',\n",
        "\t'Figure 15.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.14.jpg',\n",
        "\t'Figure 15.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.2.jpg',\n",
        "\t'Figure 15.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.3.jpg',\n",
        "\t'Figure 15.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.7.jpg',\n",
        "\t'Figure 15.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_15.8.jpg',\n",
        "\t'Figure 16.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.1.jpg',\n",
        "\t'Figure 16.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.10.jpg',\n",
        "\t'Figure 16.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.11.jpg',\n",
        "\t'Figure 16.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.2.jpg',\n",
        "\t'Figure 16.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.3.jpg',\n",
        "\t'Figure 16.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.4.jpg',\n",
        "\t'Figure 16.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.5.jpg',\n",
        "\t'Figure 16.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.6.jpg',\n",
        "\t'Figure 16.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.7.jpg',\n",
        "\t'Figure 16.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.8.jpg',\n",
        "\t'Figure 17.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.1.jpg',\n",
        "\t'Figure 17.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.10.jpg',\n",
        "\t'Figure 17.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.11.jpg',\n",
        "\t'Figure 17.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.12.jpg',\n",
        "\t'Figure 17.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.13.jpg',\n",
        "\t'Figure 17.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.14.jpg',\n",
        "\t'Figure 17.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.15.jpg',\n",
        "\t'Figure 17.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.17.jpg',\n",
        "\t'Figure 17.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.2.jpg',\n",
        "\t'Figure 17.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.3.jpg',\n",
        "\t'Figure 17.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.4.jpg',\n",
        "\t'Figure 17.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.5.jpg',\n",
        "\t'Figure 17.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.7.jpg',\n",
        "\t'Figure 17.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.8.jpg',\n",
        "\t'Figure 18.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.2.jpg',\n",
        "\t'Figure 18.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.3.jpg',\n",
        "\t'Figure 18.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.4.jpg',\n",
        "\t'Figure 18.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.5.jpg',\n",
        "\t'Figure 18.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.6.jpg',\n",
        "\t'Figure 18.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.7.jpg',\n",
        "\t'Figure 18.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_18.8.jpg',\n",
        "\t'Figure 19.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.1.jpg',\n",
        "\t'Figure 19.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.12.jpg',\n",
        "\t'Figure 19.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.13.jpg',\n",
        "\t'Figure 19.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.14.jpg',\n",
        "\t'Figure 19.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.15.jpg',\n",
        "\t'Figure 19.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.16.jpg',\n",
        "\t'Figure 19.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.17.jpg',\n",
        "\t'Figure 19.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.18.jpg',\n",
        "\t'Figure 19.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.19.jpg',\n",
        "\t'Figure 19.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.2.jpg',\n",
        "\t'Figure 19.20': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.20.jpg',\n",
        "\t'Figure 19.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.21.jpg',\n",
        "\t'Figure 19.22': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.22.jpg',\n",
        "\t'Figure 19.23': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.23.jpg',\n",
        "\t'Figure 19.24': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.24.jpg',\n",
        "\t'Figure 19.26': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.26.jpg',\n",
        "\t'Figure 19.27': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.27.jpg',\n",
        "\t'Figure 19.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.3.jpg',\n",
        "\t'Figure 19.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.4.jpg',\n",
        "\t'Figure 19.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.6.jpg',\n",
        "\t'Figure 19.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.7.jpg',\n",
        "\t'Figure 19.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_19.9.jpg',\n",
        "\t'Figure 2.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.1.jpg',\n",
        "\t'Figure 2.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.11.jpg',\n",
        "\t'Figure 2.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.13.jpg',\n",
        "\t'Figure 2.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.14.jpg',\n",
        "\t'Figure 2.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.15.jpg',\n",
        "\t'Figure 2.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.16.jpg',\n",
        "\t'Figure 2.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.2.jpg',\n",
        "\t'Figure 2.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.3.jpg',\n",
        "\t'Figure 2.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.4.jpg',\n",
        "\t'Figure 2.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.5.jpg',\n",
        "\t'Figure 2.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.6.jpg',\n",
        "\t'Figure 2.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_2.9.jpg',\n",
        "\t'Figure 20.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.1.jpg',\n",
        "\t'Figure 20.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.10.jpg',\n",
        "\t'Figure 20.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.11.jpg',\n",
        "\t'Figure 20.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.12.jpg',\n",
        "\t'Figure 20.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.13.jpg',\n",
        "\t'Figure 20.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.14.jpg',\n",
        "\t'Figure 20.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.15.jpg',\n",
        "\t'Figure 20.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.2.jpg',\n",
        "\t'Figure 20.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.3.jpg',\n",
        "\t'Figure 20.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.4.jpg',\n",
        "\t'Figure 20.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.5.jpg',\n",
        "\t'Figure 20.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.6.jpg',\n",
        "\t'Figure 20.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.7.jpg',\n",
        "\t'Figure 20.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.8.jpg',\n",
        "\t'Figure 20.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_20.9.jpg',\n",
        "\t'Figure 21.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.1.jpg',\n",
        "\t'Figure 21.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.2.jpg',\n",
        "\t'Figure 21.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.3.jpg',\n",
        "\t'Figure 21.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.4.jpg',\n",
        "\t'Figure 21.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.5.jpg',\n",
        "\t'Figure 21.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.6.jpg',\n",
        "\t'Figure 21.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.7.jpg',\n",
        "\t'Figure 21.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.8.jpg',\n",
        "\t'Figure 21.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_21.9.jpg',\n",
        "\t'Figure 22.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.1.jpg',\n",
        "\t'Figure 22.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.3.jpg',\n",
        "\t'Figure 22.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.5.jpg',\n",
        "\t'Figure 22.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.6.jpg',\n",
        "\t'Figure 22.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.7.jpg',\n",
        "\t'Figure 22.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_22.9.jpg',\n",
        "\t'Figure 23.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.1.jpg',\n",
        "\t'Figure 23.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.11.jpg',\n",
        "\t'Figure 23.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.12.jpg',\n",
        "\t'Figure 23.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.2.jpg',\n",
        "\t'Figure 23.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.4.jpg',\n",
        "\t'Figure 23.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.6.jpg',\n",
        "\t'Figure 23.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_23.7.jpg',\n",
        "\t'Figure 24.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.1.jpg',\n",
        "\t'Figure 24.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.10.jpg',\n",
        "\t'Figure 24.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.11.jpg',\n",
        "\t'Figure 24.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.12.jpg',\n",
        "\t'Figure 24.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.2.jpg',\n",
        "\t'Figure 24.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.3.jpg',\n",
        "\t'Figure 24.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.4.jpg',\n",
        "\t'Figure 24.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.5.jpg',\n",
        "\t'Figure 24.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.6.jpg',\n",
        "\t'Figure 24.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.7.jpg',\n",
        "\t'Figure 24.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.8.jpg',\n",
        "\t'Figure 24.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_24.9.jpg',\n",
        "\t'Figure 25.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.1.jpg',\n",
        "\t'Figure 25.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.10.jpg',\n",
        "\t'Figure 25.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.11.jpg',\n",
        "\t'Figure 25.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.12.jpg',\n",
        "\t'Figure 25.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.13.jpg',\n",
        "\t'Figure 25.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.14.jpg',\n",
        "\t'Figure 25.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.15.jpg',\n",
        "\t'Figure 25.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.16.jpg',\n",
        "\t'Figure 25.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.17.jpg',\n",
        "\t'Figure 25.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.18.jpg',\n",
        "\t'Figure 25.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.19.jpg',\n",
        "\t'Figure 25.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.2.jpg',\n",
        "\t'Figure 25.20': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.20.jpg',\n",
        "\t'Figure 25.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.21.jpg',\n",
        "\t'Figure 25.22': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.22.jpg',\n",
        "\t'Figure 25.23': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.23.jpg',\n",
        "\t'Figure 25.24': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.24.jpg',\n",
        "\t'Figure 25.25': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.25.jpg',\n",
        "\t'Figure 25.26': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.26.jpg',\n",
        "\t'Figure 25.27': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.27.jpg',\n",
        "\t'Figure 25.28': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.28.jpg',\n",
        "\t'Figure 25.29': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.29.jpg',\n",
        "\t'Figure 25.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.3.jpg',\n",
        "\t'Figure 25.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.4.jpg',\n",
        "\t'Figure 25.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.5.jpg',\n",
        "\t'Figure 25.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.6.jpg',\n",
        "\t'Figure 25.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.7.jpg',\n",
        "\t'Figure 25.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.8.jpg',\n",
        "\t'Figure 25.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_25.9.jpg',\n",
        "\t'Figure 26.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.1.jpg',\n",
        "\t'Figure 26.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.10.jpg',\n",
        "\t'Figure 26.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.11.jpg',\n",
        "\t'Figure 26.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.12.jpg',\n",
        "\t'Figure 26.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.13.jpg',\n",
        "\t'Figure 26.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.15.jpg',\n",
        "\t'Figure 26.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.16.jpg',\n",
        "\t'Figure 26.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.17.jpg',\n",
        "\t'Figure 26.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.18.jpg',\n",
        "\t'Figure 26.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.19.jpg',\n",
        "\t'Figure 26.20': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.20.jpg',\n",
        "\t'Figure 26.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.21.jpg',\n",
        "\t'Figure 26.22': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.22.jpg',\n",
        "\t'Figure 26.23': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.23.jpg',\n",
        "\t'Figure 26.24': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.24.jpg',\n",
        "\t'Figure 26.25': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.25.jpg',\n",
        "\t'Figure 26.26': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.26.jpg',\n",
        "\t'Figure 26.27': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.27.jpg',\n",
        "\t'Figure 26.28': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.28.jpg',\n",
        "\t'Figure 26.29': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.29.jpg',\n",
        "\t'Figure 26.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.3.jpg',\n",
        "\t'Figure 26.30': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.30.jpg',\n",
        "\t'Figure 26.31': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.31.jpg',\n",
        "\t'Figure 26.32': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.32.jpg',\n",
        "\t'Figure 26.33': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.33.jpg',\n",
        "\t'Figure 26.34': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.34.jpg',\n",
        "\t'Figure 26.35': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.35.jpg',\n",
        "\t'Figure 26.36': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.36.jpg',\n",
        "\t'Figure 26.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.4.jpg',\n",
        "\t'Figure 26.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.5.jpg',\n",
        "\t'Figure 26.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.7.jpg',\n",
        "\t'Figure 26.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.8.jpg',\n",
        "\t'Figure 26.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_26.9.jpg',\n",
        "\t'Figure 3.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.1.jpg',\n",
        "\t'Figure 3.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.10.jpg',\n",
        "\t'Figure 3.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.11.jpg',\n",
        "\t'Figure 3.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.13.jpg',\n",
        "\t'Figure 3.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.16.jpg',\n",
        "\t'Figure 3.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.17.jpg',\n",
        "\t'Figure 3.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.18.jpg',\n",
        "\t'Figure 3.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.19.jpg',\n",
        "\t'Figure 3.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.2.jpg',\n",
        "\t'Figure 3.20': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.20.jpg',\n",
        "\t'Figure 3.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.21.jpg',\n",
        "\t'Figure 3.23': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.23.jpg',\n",
        "\t'Figure 3.24': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.24.jpg',\n",
        "\t'Figure 3.25': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.25.jpg',\n",
        "\t'Figure 3.26': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.26.jpg',\n",
        "\t'Figure 3.27': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.27.jpg',\n",
        "\t'Figure 3.28': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.28.jpg',\n",
        "\t'Figure 3.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.4.jpg',\n",
        "\t'Figure 3.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.5.jpg',\n",
        "\t'Figure 3.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.6.jpg',\n",
        "\t'Figure 3.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_3.8.jpg',\n",
        "\t'Figure 4.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.1.jpg',\n",
        "\t'Figure 4.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.11.jpg',\n",
        "\t'Figure 4.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.12.jpg',\n",
        "\t'Figure 4.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.13.jpg',\n",
        "\t'Figure 4.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.14.jpg',\n",
        "\t'Figure 4.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.15.jpg',\n",
        "\t'Figure 4.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.16.jpg',\n",
        "\t'Figure 4.17': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.17.jpg',\n",
        "\t'Figure 4.18': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.18.jpg',\n",
        "\t'Figure 4.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.19.jpg',\n",
        "\t'Figure 4.22': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.22.jpg',\n",
        "\t'Figure 4.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.3.jpg',\n",
        "\t'Figure 4.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.5.jpg',\n",
        "\t'Figure 4.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.6.jpg',\n",
        "\t'Figure 4.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.8.jpg',\n",
        "\t'Figure 4.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_4.9.jpg',\n",
        "\t'Figure 5.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.1.jpg',\n",
        "\t'Figure 5.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.10.jpg',\n",
        "\t'Figure 5.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.12.jpg',\n",
        "\t'Figure 5.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.13.jpg',\n",
        "\t'Figure 5.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.14.jpg',\n",
        "\t'Figure 5.15': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.15.jpg',\n",
        "\t'Figure 5.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.16.jpg',\n",
        "\t'Figure 5.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.2.jpg',\n",
        "\t'Figure 5.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.4.jpg',\n",
        "\t'Figure 5.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.5.jpg',\n",
        "\t'Figure 5.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.6.jpg',\n",
        "\t'Figure 5.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.8.jpg',\n",
        "\t'Figure 5.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_5.9.jpg',\n",
        "\t'Figure 6.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.1.jpg',\n",
        "\t'Figure 6.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.12.jpg',\n",
        "\t'Figure 6.13': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.13.jpg',\n",
        "\t'Figure 6.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.2.jpg',\n",
        "\t'Figure 6.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.4.jpg',\n",
        "\t'Figure 6.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.5.jpg',\n",
        "\t'Figure 6.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.6.jpg',\n",
        "\t'Figure 6.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.7.jpg',\n",
        "\t'Figure 6.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_6.8.jpg',\n",
        "\t'Figure 7.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.12.jpg',\n",
        "\t'Figure 7.14': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.14.jpg',\n",
        "\t'Figure 7.16': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.16.jpg',\n",
        "\t'Figure 7.19': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.19.jpg',\n",
        "\t'Figure 7.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.2.jpg',\n",
        "\t'Figure 7.21': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.21.jpg',\n",
        "\t'Figure 7.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.3.jpg',\n",
        "\t'Figure 7.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.4.jpg',\n",
        "\t'Figure 7.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.5.jpg',\n",
        "\t'Figure 7.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.6.jpg',\n",
        "\t'Figure 7.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.7.jpg',\n",
        "\t'Figure 7.8': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_7.8.jpg',\n",
        "\t'Figure 8.1': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.1.jpg',\n",
        "\t'Figure 8.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.2.jpg',\n",
        "\t'Figure 8.3': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.3.jpg',\n",
        "\t'Figure 8.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.4.jpg',\n",
        "\t'Figure 8.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.5.jpg',\n",
        "\t'Figure 8.6': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_8.6.jpg',\n",
        "\t'Figure 9.10': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.10.jpg',\n",
        "\t'Figure 9.11': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.11.jpg',\n",
        "\t'Figure 9.12': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.12.jpg',\n",
        "\t'Figure 9.2': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.2.jpg',\n",
        "\t'Figure 9.4': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.4.jpg',\n",
        "\t'Figure 9.5': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.5.jpg',\n",
        "\t'Figure 9.7': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.7.jpg',\n",
        "\t'Figure 9.9': 'https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_9.9.jpg',\n",
        "}\n",
        "\n",
        "def get_figure_markdown(figure, width_percent=75):\n",
        "  if figure in AIMA_figures:\n",
        "#    image_tag = '![{alt_text}]({url})'.format(alt_text=figure, url=AIMA_figures[figure])\n",
        "    image_tag = '<img src=\"{url}\" alt=\"{alt_text}\" width=\"{width}%\"/>'.format(\n",
        "        alt_text=figure, \n",
        "        url=AIMA_figures[figure],\n",
        "        width=width_percent)\n",
        "\n",
        "    return image_tag\n",
        "  else:\n",
        "    return \"## Error: {figure} not found in the list of AIMA figures\".format(figure=figure)\n",
        "    "
      ],
      "metadata": {
        "cellView": "form",
        "id": "_hwzGUzOSwJl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZZXyrV-xuAp"
      },
      "source": [
        "# Module 8 - Markov Decision Process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2NfE1CKxzS-"
      },
      "source": [
        "# Russell and Norvig, AIMA Chapter 16, “Making Simple Decisions” (16.1-16.3)\n",
        "sample_equation = r\"\"\" $P(\\textnormal{Result}(a)=s') = \\sum_s P(s)P(s'|s,a)$ \"\"\"\n",
        "chapter16_1 = \"\"\"\n",
        "Estimating the prob. distribution $P(s)$ over possible states requires: \\\\\n",
        "- perception \\\\\n",
        "- learning \\\\\n",
        "- knowledge representation \\\\\n",
        "- inference \\\\\n",
        "\n",
        "Decision Theory is not a panacea, but provides enough to define AI problem. \\\\\n",
        "\n",
        "Expected Utility formula: \\\\\n",
        "$EU(a) = \\sum_{s'} P(Result(a)=s') U(s')$\n",
        "\n",
        "Transition func:\n",
        "\"\"\"  + sample_equation\n",
        "chapter16_2 =  \"\"\" \n",
        "Utility Theory's goal is to understand how preferences between complex lotteries are related to preferences between underlying states. \\\\\n",
        "\n",
        "Axioms lead to: \\\\\n",
        "- Existence of Util. func. - if agent obey the axioms then there exist $U$ such that: \\\\\n",
        "$U(A) > U(B) \\iff A \\succ B$ and $U(A) = U(B) \\iff A \\sim B$ \\\\\n",
        "- EU of Lottery - the sum of the prob. of each outcome times util. of that outcome: \\\\\n",
        "$U([p_1, S_1 ... p_n, S_n]) = \\sum_i p_i U(S_i)$ \\\\\n",
        "\n",
        "Util. functions are not necesseraly unique \\\\\n",
        "\n",
        "Util. resembles temperatures, converting from one or another doesn't make you hotter or colder ($F \\iff C$)\n",
        "\"\"\"\n",
        "chapter16_3 =  \"\"\"  \n",
        "Utility func. map from lotteries to real numbers. \\\\\n",
        "They must obey the axioms of rationality. \n",
        "\n",
        "\\\\\n",
        "EU and possible post-decision disappointment \\\\\n",
        "The real outcome (of model of lottery) will usually be significantly worse than we estimated, even if the estimate was unbiased. \\\\\n",
        "Because we select the action with highest util. estimate, we are favoring the overly optimistic estimates, and that's the source of bias. \\\\\n",
        "\"\"\"\n",
        "\n",
        "# Russell and Norvig, AIMA Chapter 17, “Making Complex Decision” (17.1-17.2)\n",
        "chapter17_1 = \"\"\"\n",
        "$T$ are Markovian - the prob of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \\\\\n",
        "\n",
        "\\\\\n",
        "Environment History - seq. of states and actions \\\\\n",
        "Rewards may be positive or negative, but they are bounded by +/-$R_{max}$ \\\\\n",
        "Utility of Environment History - sum of rewards received \\\\\n",
        "\n",
        "\\\\\n",
        "Policy represents the agent function explicitly. Therefore, a description of simple reflex agent computed from the information used for a utility-based agent. \\\\\n",
        "\n",
        "\\\\\n",
        "Agent's preferences between state sequences are ***stationary*** \\\\\n",
        "If agent prefers one future to another starting tomorrow, then it will still prefer that future if it starts today. \\\\\n",
        "\n",
        "Optimal policy is ***independent*** of the starting state - that's a remarkable consequence of using discounted utilities with infinite horizons.\n",
        "\"\"\"\n",
        "chapter17_2 =  \"\"\" \n",
        "Discounted reward is not absolutely necessary. \\\\\n",
        "Proper policy might not always exist. \\\\\n",
        "\n",
        "\\\\\n",
        "Value Iteration converges due to contraction. \\\\\n",
        "Contraction - a func. of one argument that, when applied to two different inputs in turn, produces two outputs that are 'closer together' by at least some constant factor, than the original inputs. \\\\\n",
        "Example: 'Divide by two' is a contraction \\\\\n",
        "- Contraction has only one fixed point \\\\\n",
        "- When func. applied, the value must get closer to fixed point, so repeated application of contraction always reaches the fixed point \\\\\n",
        "\n",
        "Value Iteration converges to the correct utility. \\\\\n",
        "We can bound the errorr in utility, if we stop after a finite number of iterations, and we can bound the policy loss that results from executing corresponding MEU policy. PS. true for $\\gamma < 1$ \\\\\n",
        "\n",
        "\\\\\n",
        "Look for details on VI, PI, etc. in the Lecture Notes. \\\\\n",
        "\n",
        "Asynchronous Policy Iteration given certain conditions on the initial policy and util. func. is guaranteed to converge to an optimal policy. \\\\\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Lecture notes \n",
        "module8_lecture_notes = \"\"\"\n",
        "Stochastic Transition Model  \\\\\n",
        "$T(s, a, s')$ or $P(s'| s, a)$ - only considers the current state (ignoring all the states we took so far to get to this state) => makes this definition $Markovian$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Reward/utility function** \\\\\n",
        "$R(s, a, s')$ - seqential, so we have to specify utility/reward func. on a seq. of states and  actions. \\\\\n",
        "The agent can receive a reward at each time  step, based on its transition $s -> s'$ via $a$  \n",
        "\n",
        "Rewards are usually additive \\\\\n",
        "Reward func. shapes the optimal policy (Millenium Falcon example): \\\\\n",
        "- $r > 0$ - incentivies us to keep going through the loop w/o taking any risk of crushing / exiting \\\\\n",
        "- $-0.03 < r < 0$ - incentivies conservative behaviour & avoiding the risk of exiting \\\\\n",
        "- $-0.04$ - gives optimality \\\\\n",
        "- $r < -1.65$ - makes it easier to exit via -1 \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**MDP Definition:** \\\\\n",
        "- set of states \\\\\n",
        "- set of actions \\\\\n",
        "- transition func. \\\\\n",
        "- reward func. \\\\\n",
        "- initial state \\\\\n",
        "- Terminal state(s) \\\\\n",
        "\n",
        "As MDP is non-deterministic, we can't give just a seq. of actions as a solution. \\\\\n",
        "Instead, solution to MDP is a policy. \\\\\n",
        "Expectimax is one of the way to solve MDPs. \\\\\n",
        "\n",
        "We need to compute the *Expected Utility* of all possible paths, generated by policy. \\\\\n",
        "\n",
        "Combating the danger of game running forever: \\\\\n",
        "$(1)$ Finite horizons (depth-limited search) - terminate after fixed num of steps or give non-stationary policies (that depends on time left) \\\\\n",
        "$(2)$ Absorbing State - guaranties that in every policy, a terrminal state will eventually be reached \\\\\n",
        "$(3)$ Discounting \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "Discounting \\\\\n",
        "Reasonable agents: (1) max for sum of rewards, (2) prefer rewards now, not later (values of rewards decay exponentially) \\\\\n",
        "Each time we decay a level, we power the discount \\\\\n",
        "$U([1, 2, 3]) = 0.5^0 * 1 + 0.5^1 * 2 + 0.5^2 * 3 = 1 + 1 + 0.75 = 2.75$. \\\\\n",
        "+ It helps us to converge the alg. \\\\\n",
        "+ Helps to truncate som bottom parts of Tree Search due to its insignificance \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Value of states** \\\\\n",
        "$V^*(s)$ - expected utility, starting in $s$ and acting optimally \\\\\n",
        "$Q^*(s, a)$ - value of q-state(s, a), expected utility of taking $a$ from $s$ and acting optimally (thereafter) \\\\\n",
        "$\\pi^*(s)$ - optimal policy, optimal action from $s$\n",
        "$*$ - points to optimality \\\\\n",
        "\n",
        "Formulas: \\\\\n",
        "$V^*(s) = max_a Q^*(s, a)$ - choose the best action $a$ and take a max $Q$ value \\\\\n",
        "$Q^*(s, a) = \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - computes value of q-state via Expectimax \\\\\n",
        "$V^*(s) = max_a \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - ***Bellman equation*** - full merged formula of the expected utility \\\\\n",
        "\n",
        "\\\\\n",
        "To prevent the Search Tree getting very big very fast:\n",
        "- we use $\\gamma$ - nodes far from the root have tiny effects -> do Depth-limited search, with increasing $d$, until changes are small \\\\\n",
        "- as States will be repeated -> we only compute needed quantities once \\\\\n",
        "\n",
        "\\\\\n",
        "**Value Iteration Algorithm** \\\\\n",
        "- Starts with $V_0(s) = 0$ - game over, expected reward sum is 0 \\\\\n",
        "- Given vector of $V_k(s)$ values, do one ply of Expectimax from each state: \\\\\n",
        "$V_{k+1}(s) = max_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V_k(s')]$ \\\\\n",
        "- Repeat until convergence \\\\\n",
        "$O(s^2 * a)$ - complexity of EACH iteration ($s$ num of states, $a$ num of actions) \\\\\n",
        "- Updates the policy **implicitly**\n",
        "\n",
        "```\n",
        "def iterate(self):\n",
        "        # Runs single value iteration using Bellman equation: V_{k+1}(s) = max_a Q*(s,a)\n",
        "        # Then updates values: V*(s) = V_{k+1}(s)\n",
        "        temp = dict()\n",
        "        for state in self.game.states:\n",
        "            action = self.get_best_policy(state)\n",
        "            value_next = self.get_q_value(state, action)\n",
        "            temp[state] = value_next\n",
        "        for state in self.game.states:\n",
        "            new_value = temp[state]\n",
        "            self.state_values[state] = new_value\n",
        "```\n",
        "Theorem: \\\\\n",
        "Value Iteration alg. converge to unique optimal values: \\\\\n",
        "- Approximations get refined towards optimal value \\\\\n",
        "- Policy may converge long before values do (policy converges early) \\\\\n",
        "- Value may still change after the policy convergence \\\\\n",
        "- In general, values change more often than policy \\\\\n",
        "\n",
        "Problems with VI as it repeats Bellman equations: \\\\\n",
        "- It's slow! $O(s^2 * a)$ per iteration \\\\\n",
        "- The $max$ action rarely changes \\\\\n",
        "- The policy often converges long before the values => We can do it better \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Policy** \\\\\n",
        "**Policy Evaluation** \\\\\n",
        "- Fix the policy \\\\\n",
        "- Compute the util. of $s$ under $\\pi$ (not yet optimal policy) \\\\\n",
        "- Recurse - do 1-step look ahead \\\\\n",
        "$V^{\\pi}(s) = \\sum_{s'}{} T(s, \\pi(s), s') [R(s, \\pi(s), s') +$ $\\gamma$ $V^\\pi(s')]$ - dropped $max$ cause actions chosen under policy (usually the best ones) \\\\\n",
        "\n",
        "$O(s^2)$ - per iteration \\\\\n",
        "$O(n^3)$ - total (for $n$ states, we have $n$ linear eq. with $n$ unknowns), but we can use simplified VI \\\\\n",
        "\n",
        "\n",
        "**Policy Extraction** \\\\\n",
        "$\\pi^*(s) = argmax_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ \\\\\n",
        "$\\pi^*(s) = argmax_a Q^*(s, a)$ \\\\\n",
        "! Actions are easier to select from q-values than values \\\\\n",
        "\n",
        "\n",
        "**Policy Iteration** \\\\\n",
        "1) Policy Eval \\\\\n",
        "2) Policy Improvement - calculates a new MEU policy $\\pi_{i+1}$, using 1-step look ahead aka 1st Policy Extraction formula \\\\\n",
        "Repeat until policy converges \\\\\n",
        "- Optimal! \\\\\n",
        "- Can converge much faster \\\\\n",
        "\n",
        "```\n",
        "def iterate(self):\n",
        "        # Runs single policy iteration. Fixes curr policy, iterates through state values V(s) until |V_{k+1}(s) - V_k(s)| < ε\n",
        "        epsilon = 1e-6\n",
        "        policy = dict()\n",
        "        for state in self.game.states:\n",
        "            policy[state] = self.get_best_policy(state)\n",
        "\n",
        "        while True:\n",
        "            temp = 0\n",
        "            for state, value in self.state_values.items():\n",
        "                action = self.get_best_policy(state)\n",
        "                new_value = self.get_q_value(state, action)\n",
        "                self.state_values[state] = new_value\n",
        "\n",
        "                diff = abs(new_value - value)\n",
        "                if diff > temp: temp = diff\n",
        "\n",
        "            if temp < epsilon:\n",
        "                break\n",
        "```\n",
        "**Comparison** \\\\\n",
        "Both: \\\\\n",
        "- VI & PI both compute the same thing - all optimal values (via Bellman eq. and Expectimax) \\\\\n",
        "- Are dynamic programs for solving MDP \\\\\n",
        "\n",
        "VI (*max action vs*): \\\\\n",
        "- Every iteration updates both, the values & (implicitly) the policy \\\\\n",
        "- Taking $max$ action implicitely recomputes the policy, even if we don't track it \\\\\n",
        "\n",
        "PI (*fixed $\\pi$*): \\\\\n",
        "- We do several passes that update values with fixed $\\pi$. Each pass is fast, only 1 action \\\\\n",
        "- After the policy is evaluated, new policy is chosen. Slow like VI pass \\\\\n",
        "- The next policy will be better or we are done \\\\\n",
        "\n",
        "\\\\\n",
        "**Summary of MDP** \\\\\n",
        "- To compute optimal values => use VI or PI \\\\\n",
        "- To compute values for a particular policy => use PI \\\\\n",
        "- To turn values into a policy => use Policy Extaction \\\\\n",
        "- Expectimax & MDP = aveaging utilities \\\\\n",
        "\n",
        "\\\\\n",
        "**Transforming Utilities** \\\\\n",
        "Scaling: better states -> higher values + \\\\\n",
        "Getting the expectimax ordering right \\\\\n",
        "= \\\\\n",
        "Intensivity to Monotonic Transformations \\\\\n",
        "(remeber x^2=> led to better value: was $20 & 25 x^2=> 800 & 650$) \n",
        "\n",
        "\\\\\n",
        "**Utilities & Preferences** \\\\\n",
        "$\\succ$ - Preference $A \\succ B$ \\\\\n",
        "$\\sim$ - Indifference $A \\sim B$ \\\\\n",
        "\n",
        "\\\\\n",
        "Axioms of Rationality: \\\\\n",
        "$(A \\succ B) \\lor (B \\succ A) \\lor (A \\sim B) $ - Orderability \\\\\n",
        "$(A \\succ B) \\land (B \\succ C) \\implies (A \\succ C) $ - Transitivity \\\\\n",
        "$(A \\succ B \\succ C) \\implies \\exists p [p, A; 1 - p, C] \\sim B $ - Continuity, there exist some $p$ that equalizes taking either or $B$ \\\\\n",
        "$(A \\sim B) \\implies [p, A; 1 - p, C] \\sim [p, B; 1 - p, C] $ - Substitutability, in some lottery between $A & C$, I'd also be ok with pair $B & C$, given $P(A) = P(B)$ \\\\\n",
        "$(A \\succ B) \\implies (p \\geq q \\iff [p, A; 1 - p, B] \\succeq [q, A; 1 - q, B]) $ - Monotonocity, I prefer a lottery where probability of $A$ is higher \\\\\n",
        "$[p, A; 1 - p, [q, B; 1-q, C]] \\sim [p, A; (1-p)q, B; (1-p)(1-q), C]$ - Decomposability, compound lotteries could be reduced to simple ones using probability laws \\\\\n",
        "\n",
        "IF agent violates any axiom, it will exhibit irrational behavior in some situations. \\\\\n",
        "IF we aceept these axioms, our behavior will maximize EU. \\\\\n",
        "In other words, make us a Rational Agent. \\\\\n",
        "We can have a rational agent even if prob./util. are not explicitely modeled (~Reflex vacuum cleaner)\n",
        "\"\"\"\n",
        "\n",
        "# Definitions of concepts from Module 8.\n",
        "# You can define any term in the textbook or lecture, but you most use your own words for the definitions.\n",
        "# Each definition should be at most 25 words long.\n",
        "module8_definitions = {\n",
        "\"expected utility\" : \"ave utility value of the outcomes, weighted by the probability that the outcome occurs (16.1 equation)\",\n",
        "\"Explicit\" : \"expressed directly without anything being implied\",\n",
        "\"Implicit\" : \"indirectly stated or implied\",\n",
        "\"Markov Decision Process, MDP\": \"a sequential decision problem for a fully observable env. with stochastic  transition model that has additive rewards\",\n",
        "\"Utility\": \"sum of (discounted) rewards\",\n",
        "\"Absorbing State\": \"guarantee that in every policy, a Terminal state will eventually be reached\",\n",
        "\"Performance of an agent in MDP\": \"sum of rewards for transitions agent takes\",\n",
        "\"Policy\": \"choice of action for each state\",\n",
        "\"Proper Policy\": \"one that guarantees to reach the terminal state\",\n",
        "\"Policy Loss\": \"the most the agent can lose by executing $\\pi_i$ instead of optimal policy\",\n",
        "\"Non-stationary Policy\": \"depends on time (and points to finite horizon. Infitite horizons = easier as their policies are stationary ones)\",\n",
        "\"Fixed policy\" : \"do what $\\pi$ says to do\",\n",
        "\"Asynchronous Policy Iteration\" : \"on each iteration, we can pick any subset of states and apply either policy improvement or simplified VI\",\n",
        "\"Policy Evaluation\" : \"calculates values for some fixed policy until convergence\",\n",
        "\"Policy Extraction\" : \"doing 1-step Expectimax to choose the best action\",\n",
        "\"MEU Principle\" : \"given its knowledge, a rational agent Maximizes its Expected Utility\",\n",
        "\"****\" : \"\",\n",
        "\"Lottery\" : \"a situation with an uncertain prize. Set of outcomes where each action is a ticket\",\n",
        "\"Insurance Premium\" : \"difference between the expected monetary value of lottery and its certain equivalent\",\n",
        "\"Normative Theory\" : \"Describes how a rational agent should act\",\n",
        "\"Descriptive Theory\" : \"Describes how actual agent really acts\",\n",
        "\"Certainty Effect\" : \"people are strongly attracted to gains that are certain\",\n",
        "\"Ambiquity aversion\" : \"most people elect the known probability rather than the unknown unknowns\",\n",
        "\"Framing effect\" : \"exact wording of a decision problem has a big impact of the agent's choices\",\n",
        "\"Ancoring effect\" : \"people feel more comfortable making relative utility judgement rather than absolute ones\",\n",
        "\"\" : \"****\",\n",
        "\"Linear Programming\" : \"is a general approach for formulating constrained optimization problems\",\n",
        "\"Dynamic programming\" : \"simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces\",\n",
        "\"Real-Time Dynamic Programming\" : \"Transforming tree -> graph since root is also a leaf due to repeated states (see 17.10)\",\n",
        "\"Finite horizon\" : \"there is a fixed time $N$ after which nothing matters - the game is over\",\n",
        "\"Discount factor\" : \"preference of an agent for current reward over future rewards\",\n",
        "\"Additive Reward\" : \"what discounted rewards are reduce to, when $\\gamma = 1$\"\n",
        "}\n",
        "\n",
        "# You can include explanations for figures in the textbook (except figures with pseduocode).\n",
        "# The figure will appear in your notes along with your explanation.\n",
        "module8_figure_explanations = {\n",
        "\"Figure 16.1\" : \"(a) Nontransitive preferences $A \\succ B \\succ C \\succ A$ can result in irrational behevior (b) Decomposability Axiom\" ,\n",
        "\"Figure 16.2\" : \"This figure shows the perceived utility of money. After your first billion, your next billion doesn't matter as much. (b) From risk-seaking to risk-averse\", \n",
        "\"Figure 16.3\" : \"Unjustified optimism caused by choosing the best of $k$ options: utility distributed according to a unit normal (blue curve)\",\n",
        "\"Figure 17.3\" : \"4 x 3 world with $\\gamma = 1$ and $r = -0.04$ for transition to non-terminal states\",\n",
        "\"Figure 17.8\" : \"In practice, it's often that $\\pi_i$ becomes optimal long before $U_i$ has converged\",\n",
        "\"Figure 17.10\" : \"Expectimax Tree for 4 x 3: decision is extracted by taking ave at the chance nodes and max at the decision nodes\"\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elCCgY64spuq"
      },
      "source": [
        "## Preview of your notes for Module 8\n",
        "\n",
        "Here's a preview of your how your notes will be displayed.  At the end of this Python notebook, we'll aggregate your notes for all modules together into a single display.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYYeNcdM_maE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5ccc5608-005e-40bb-9fbb-2c70fe9dac64",
        "cellView": "form"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "from IPython.display import display, Markdown, Latex\n",
        "\n",
        "module8_sections = [\n",
        "  (\"AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\", chapter16_1),\n",
        "  (\"AIMA Chapter 16.2 The Basis of Utility Theory\", chapter16_2),\n",
        "  (\"AIMA Chapter 16.3 Utility Functions\", chapter16_3),\n",
        "  (\"AIMA Chapter 17.1 Sequential Decision Problems\", chapter17_1),\n",
        "  (\"AIMA Chapter 17.2 Algorithms for MDPs\", chapter17_2),\n",
        "  (\"Module 8 Lecture Notes\", module8_lecture_notes),\n",
        "]\n",
        "\n",
        "\n",
        "def length_check(sections):\n",
        "  warnings = \"\"\n",
        "  for section, summary in sections:\n",
        "    max_word_length = 500\n",
        "    max_character_length = 3000\n",
        "    if \"lecture\" in section.lower():\n",
        "      # It's OK to do longer summaries of the lectures\n",
        "      max_word_length = 2000\n",
        "      max_character_length = 12000  \n",
        "    if (len(summary) > max_character_length or len(summary.split(' ')) > max_word_length):\n",
        "      warnings += \"WARNING: Your summary of '{section}' is too long!\\n\".format(section=section)\n",
        "  return warnings\n",
        "\n",
        "def length_check_definitions(definitions):\n",
        "  warnings = \"\"\n",
        "  for concept, definition in definitions.items():\n",
        "    if(len(definition.split(' ')) > 25):\n",
        "      warnings += \"WARNING: Your definition of '{concept}' is too long!\\n\".format(concept=concept)\n",
        "  return warnings\n",
        "\n",
        "\n",
        "def include_figures(figures):\n",
        "  # Checks if there are any figures with explanations.\n",
        "  for figure, explanation in figures.items():\n",
        "    if explanation != \"\":\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def format_module(module_num, title, sections, definitions, figures, show_placeholder=False):\n",
        "  markdown = \"\"\n",
        "\n",
        "  # Check summary lengths\n",
        "  warnings = length_check(sections)\n",
        "  if warnings != \"\":\n",
        "    markdown += \"## \" + warnings\n",
        "\n",
        "  # Check definition lengths\n",
        "  warnings = length_check_definitions(definitions)\n",
        "  if warnings != \"\":\n",
        "    markdown += \"## \" + warnings\n",
        "\n",
        "  markdown += \"# Module {num}: {title}\\n\".format(num=module_num, title=title)\n",
        "\n",
        "\n",
        "  for section, summary in sections:\n",
        "    if show_placeholder and summary == \" Your summary here \":\n",
        "      summary = placeholder\n",
        "    markdown += \"## {section}\\n{summary}\\n\\n\".format(section=section, summary=summary)\n",
        "  \n",
        "  markdown += \"## Module {num} Definitions\\n\".format(num=module_num)\n",
        "  for concept, definition in definitions.items():\n",
        "    markdown += \"* **{concept}** - {definition}\\n\".format(concept=concept.capitalize(), definition=definition)\n",
        "\n",
        "  if include_figures(figures):\n",
        "    markdown += \"\\n\\n## Module {num} Figures\\n\".format(num=module_num)\n",
        "    for figure, explanation in figures.items():\n",
        "      if explanation == \"\":\n",
        "        continue\n",
        "      markdown += get_figure_markdown(figure) + \"\\n\\n\"\n",
        "      markdown += \"* **{figure}** - {explanation}\\n\".format(figure=figure.capitalize(), explanation=explanation)\n",
        "\n",
        "\n",
        "  return markdown\n",
        "\n",
        "module8_markdown = format_module(8, \"Markov Decision Processes\", module8_sections, module8_definitions, module8_figure_explanations)\n",
        "display(Markdown(module8_markdown))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 8: Markov Decision Processes\n## AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\n\nEstimating the prob. distribution $P(s)$ over possible states requires: \\\n- perception \\\n- learning \\\n- knowledge representation \\\n- inference \\\n\nDecision Theory is not a panacea, but provides enough to define AI problem. \\\n\nExpected Utility formula: \\\n$EU(a) = \\sum_{s'} P(Result(a)=s') U(s')$\n\nTransition func:\n $P(\\textnormal{Result}(a)=s') = \\sum_s P(s)P(s'|s,a)$ \n\n## AIMA Chapter 16.2 The Basis of Utility Theory\n \nUtility Theory's goal is to understand how preferences between complex lotteries are related to preferences between underlying states. \\\n\nAxioms lead to: \\\n- Existence of Util. func. - if agent obey the axioms then there exist $U$ such that: \\\n$U(A) > U(B) \\iff A \\succ B$ and $U(A) = U(B) \\iff A \\sim B$ \\\n- EU of Lottery - the sum of the prob. of each outcome times util. of that outcome: \\\n$U([p_1, S_1 ... p_n, S_n]) = \\sum_i p_i U(S_i)$ \\\n\nUtil. functions are not necesseraly unique \\\n\nUtil. resembles temperatures, converting from one or another doesn't make you hotter or colder ($F \\iff C$)\n\n\n## AIMA Chapter 16.3 Utility Functions\n  \nUtility func. map from lotteries to real numbers. \\\nThey must obey the axioms of rationality. \n\n\\\nEU and possible post-decision disappointment \\\nThe real outcome (of model of lottery) will usually be significantly worse than we estimated, even if the estimate was unbiased. \\\nBecause we select the action with highest util. estimate, we are favoring the overly optimistic estimates, and that's the source of bias. \\\n\n\n## AIMA Chapter 17.1 Sequential Decision Problems\n\n$T$ are Markovian - the prob of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \\\n\n\\\nEnvironment History - seq. of states and actions \\\nRewards may be positive or negative, but they are bounded by +/-$R_{max}$ \\\nUtility of Environment History - sum of rewards received \\\n\n\\\nPolicy represents the agent function explicitly. Therefore, a description of simple reflex agent computed from the information used for a utility-based agent. \\\n\n\\\nAgent's preferences between state sequences are ***stationary*** \\\nIf agent prefers one future to another starting tomorrow, then it will still prefer that future if it starts today. \\\n\nOptimal policy is ***independent*** of the starting state - that's a remarkable consequence of using discounted utilities with infinite horizons.\n\n\n## AIMA Chapter 17.2 Algorithms for MDPs\n \nDiscounted reward is not absolutely necessary. \\\nProper policy might not always exist. \\\n\n\\\nValue Iteration converges due to contraction. \\\nContraction - a func. of one argument that, when applied to two different inputs in turn, produces two outputs that are 'closer together' by at least some constant factor, than the original inputs. \\\nExample: 'Divide by two' is a contraction \\\n- Contraction has only one fixed point \\\n- When func. applied, the value must get closer to fixed point, so repeated application of contraction always reaches the fixed point \\\n\nValue Iteration converges to the correct utility. \\\nWe can bound the errorr in utility, if we stop after a finite number of iterations, and we can bound the policy loss that results from executing corresponding MEU policy. PS. true for $\\gamma < 1$ \\\n\n\\\nLook for details on VI, PI, etc. in the Lecture Notes. \\\n\nAsynchronous Policy Iteration given certain conditions on the initial policy and util. func. is guaranteed to converge to an optimal policy. \\\n\n\n## Module 8 Lecture Notes\n\nStochastic Transition Model  \\\n$T(s, a, s')$ or $P(s'| s, a)$ - only considers the current state (ignoring all the states we took so far to get to this state) => makes this definition $Markovian$ \\\n\n\\\n**Reward/utility function** \\\n$R(s, a, s')$ - seqential, so we have to specify utility/reward func. on a seq. of states and  actions. \\\nThe agent can receive a reward at each time  step, based on its transition $s -> s'$ via $a$  \n\nRewards are usually additive \\\nReward func. shapes the optimal policy (Millenium Falcon example): \\\n- $r > 0$ - incentivies us to keep going through the loop w/o taking any risk of crushing / exiting \\\n- $-0.03 < r < 0$ - incentivies conservative behaviour & avoiding the risk of exiting \\\n- $-0.04$ - gives optimality \\\n- $r < -1.65$ - makes it easier to exit via -1 \\\n\n\n\\\n**MDP Definition:** \\\n- set of states \\\n- set of actions \\\n- transition func. \\\n- reward func. \\\n- initial state \\\n- Terminal state(s) \\\n\nAs MDP is non-deterministic, we can't give just a seq. of actions as a solution. \\\nInstead, solution to MDP is a policy. \\\nExpectimax is one of the way to solve MDPs. \\\n\nWe need to compute the *Expected Utility* of all possible paths, generated by policy. \\\n\nCombating the danger of game running forever: \\\n$(1)$ Finite horizons (depth-limited search) - terminate after fixed num of steps or give non-stationary policies (that depends on time left) \\\n$(2)$ Absorbing State - guaranties that in every policy, a terrminal state will eventually be reached \\\n$(3)$ Discounting \\\n\n\n\\\nDiscounting \\\nReasonable agents: (1) max for sum of rewards, (2) prefer rewards now, not later (values of rewards decay exponentially) \\\nEach time we decay a level, we power the discount \\\n$U([1, 2, 3]) = 0.5^0 * 1 + 0.5^1 * 2 + 0.5^2 * 3 = 1 + 1 + 0.75 = 2.75$. \\\n+ It helps us to converge the alg. \\\n+ Helps to truncate som bottom parts of Tree Search due to its insignificance \\\n\n\n\\\n**Value of states** \\\n$V^*(s)$ - expected utility, starting in $s$ and acting optimally \\\n$Q^*(s, a)$ - value of q-state(s, a), expected utility of taking $a$ from $s$ and acting optimally (thereafter) \\\n$\\pi^*(s)$ - optimal policy, optimal action from $s$\n$*$ - points to optimality \\\n\nFormulas: \\\n$V^*(s) = max_a Q^*(s, a)$ - choose the best action $a$ and take a max $Q$ value \\\n$Q^*(s, a) = \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - computes value of q-state via Expectimax \\\n$V^*(s) = max_a \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - ***Bellman equation*** - full merged formula of the expected utility \\\n\n\\\nTo prevent the Search Tree getting very big very fast:\n- we use $\\gamma$ - nodes far from the root have tiny effects -> do Depth-limited search, with increasing $d$, until changes are small \\\n- as States will be repeated -> we only compute needed quantities once \\\n\n\\\n**Value Iteration Algorithm** \\\n- Starts with $V_0(s) = 0$ - game over, expected reward sum is 0 \\\n- Given vector of $V_k(s)$ values, do one ply of Expectimax from each state: \\\n$V_{k+1}(s) = max_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V_k(s')]$ \\\n- Repeat until convergence \\\n$O(s^2 * a)$ - complexity of EACH iteration ($s$ num of states, $a$ num of actions) \\\n- Updates the policy **implicitly**\n\n```\ndef iterate(self):\n        # Runs single value iteration using Bellman equation: V_{k+1}(s) = max_a Q*(s,a)\n        # Then updates values: V*(s) = V_{k+1}(s)\n        temp = dict()\n        for state in self.game.states:\n            action = self.get_best_policy(state)\n            value_next = self.get_q_value(state, action)\n            temp[state] = value_next\n        for state in self.game.states:\n            new_value = temp[state]\n            self.state_values[state] = new_value\n```\nTheorem: \\\nValue Iteration alg. converge to unique optimal values: \\\n- Approximations get refined towards optimal value \\\n- Policy may converge long before values do (policy converges early) \\\n- Value may still change after the policy convergence \\\n- In general, values change more often than policy \\\n\nProblems with VI as it repeats Bellman equations: \\\n- It's slow! $O(s^2 * a)$ per iteration \\\n- The $max$ action rarely changes \\\n- The policy often converges long before the values => We can do it better \\\n\n\n\\\n**Policy** \\\n**Policy Evaluation** \\\n- Fix the policy \\\n- Compute the util. of $s$ under $\\pi$ (not yet optimal policy) \\\n- Recurse - do 1-step look ahead \\\n$V^{\\pi}(s) = \\sum_{s'}{} T(s, \\pi(s), s') [R(s, \\pi(s), s') +$ $\\gamma$ $V^\\pi(s')]$ - dropped $max$ cause actions chosen under policy (usually the best ones) \\\n\n$O(s^2)$ - per iteration \\\n$O(n^3)$ - total (for $n$ states, we have $n$ linear eq. with $n$ unknowns), but we can use simplified VI \\\n\n\n**Policy Extraction** \\\n$\\pi^*(s) = argmax_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ \\\n$\\pi^*(s) = argmax_a Q^*(s, a)$ \\\n! Actions are easier to select from q-values than values \\\n\n\n**Policy Iteration** \\\n1) Policy Eval \\\n2) Policy Improvement - calculates a new MEU policy $\\pi_{i+1}$, using 1-step look ahead aka 1st Policy Extraction formula \\\nRepeat until policy converges \\\n- Optimal! \\\n- Can converge much faster \\\n\n```\ndef iterate(self):\n        # Runs single policy iteration. Fixes curr policy, iterates through state values V(s) until |V_{k+1}(s) - V_k(s)| < ε\n        epsilon = 1e-6\n        policy = dict()\n        for state in self.game.states:\n            policy[state] = self.get_best_policy(state)\n\n        while True:\n            temp = 0\n            for state, value in self.state_values.items():\n                action = self.get_best_policy(state)\n                new_value = self.get_q_value(state, action)\n                self.state_values[state] = new_value\n\n                diff = abs(new_value - value)\n                if diff > temp: temp = diff\n\n            if temp < epsilon:\n                break\n```\n**Comparison** \\\nBoth: \\\n- VI & PI both compute the same thing - all optimal values (via Bellman eq. and Expectimax) \\\n- Are dynamic programs for solving MDP \\\n\nVI (*max action vs*): \\\n- Every iteration updates both, the values & (implicitly) the policy \\\n- Taking $max$ action implicitely recomputes the policy, even if we don't track it \\\n\nPI (*fixed $\\pi$*): \\\n- We do several passes that update values with fixed $\\pi$. Each pass is fast, only 1 action \\\n- After the policy is evaluated, new policy is chosen. Slow like VI pass \\\n- The next policy will be better or we are done \\\n\n\\\n**Summary of MDP** \\\n- To compute optimal values => use VI or PI \\\n- To compute values for a particular policy => use PI \\\n- To turn values into a policy => use Policy Extaction \\\n- Expectimax & MDP = aveaging utilities \\\n\n\\\n**Transforming Utilities** \\\nScaling: better states -> higher values + \\\nGetting the expectimax ordering right \\\n= \\\nIntensivity to Monotonic Transformations \\\n(remeber x^2=> led to better value: was $20 & 25 x^2=> 800 & 650$) \n\n\\\n**Utilities & Preferences** \\\n$\\succ$ - Preference $A \\succ B$ \\\n$\\sim$ - Indifference $A \\sim B$ \\\n\n\\\nAxioms of Rationality: \\\n$(A \\succ B) \\lor (B \\succ A) \\lor (A \\sim B) $ - Orderability \\\n$(A \\succ B) \\land (B \\succ C) \\implies (A \\succ C) $ - Transitivity \\\n$(A \\succ B \\succ C) \\implies \\exists p [p, A; 1 - p, C] \\sim B $ - Continuity, there exist some $p$ that equalizes taking either or $B$ \\\n$(A \\sim B) \\implies [p, A; 1 - p, C] \\sim [p, B; 1 - p, C] $ - Substitutability, in some lottery between $A & C$, I'd also be ok with pair $B & C$, given $P(A) = P(B)$ \\\n$(A \\succ B) \\implies (p \\geq q \\iff [p, A; 1 - p, B] \\succeq [q, A; 1 - q, B]) $ - Monotonocity, I prefer a lottery where probability of $A$ is higher \\\n$[p, A; 1 - p, [q, B; 1-q, C]] \\sim [p, A; (1-p)q, B; (1-p)(1-q), C]$ - Decomposability, compound lotteries could be reduced to simple ones using probability laws \\\n\nIF agent violates any axiom, it will exhibit irrational behavior in some situations. \\\nIF we aceept these axioms, our behavior will maximize EU. \\\nIn other words, make us a Rational Agent. \\\nWe can have a rational agent even if prob./util. are not explicitely modeled (~Reflex vacuum cleaner)\n\n\n## Module 8 Definitions\n* **Expected utility** - ave utility value of the outcomes, weighted by the probability that the outcome occurs (16.1 equation)\n* **Explicit** - expressed directly without anything being implied\n* **Implicit** - indirectly stated or implied\n* **Markov decision process, mdp** - a sequential decision problem for a fully observable env. with stochastic  transition model that has additive rewards\n* **Utility** - sum of (discounted) rewards\n* **Absorbing state** - guarantee that in every policy, a Terminal state will eventually be reached\n* **Performance of an agent in mdp** - sum of rewards for transitions agent takes\n* **Policy** - choice of action for each state\n* **Proper policy** - one that guarantees to reach the terminal state\n* **Policy loss** - the most the agent can lose by executing $\\pi_i$ instead of optimal policy\n* **Non-stationary policy** - depends on time (and points to finite horizon. Infitite horizons = easier as their policies are stationary ones)\n* **Fixed policy** - do what $\\pi$ says to do\n* **Asynchronous policy iteration** - on each iteration, we can pick any subset of states and apply either policy improvement or simplified VI\n* **Policy evaluation** - calculates values for some fixed policy until convergence\n* **Policy extraction** - doing 1-step Expectimax to choose the best action\n* **Meu principle** - given its knowledge, a rational agent Maximizes its Expected Utility\n* ******** - \n* **Lottery** - a situation with an uncertain prize. Set of outcomes where each action is a ticket\n* **Insurance premium** - difference between the expected monetary value of lottery and its certain equivalent\n* **Normative theory** - Describes how a rational agent should act\n* **Descriptive theory** - Describes how actual agent really acts\n* **Certainty effect** - people are strongly attracted to gains that are certain\n* **Ambiquity aversion** - most people elect the known probability rather than the unknown unknowns\n* **Framing effect** - exact wording of a decision problem has a big impact of the agent's choices\n* **Ancoring effect** - people feel more comfortable making relative utility judgement rather than absolute ones\n* **** - ****\n* **Linear programming** - is a general approach for formulating constrained optimization problems\n* **Dynamic programming** - simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces\n* **Real-time dynamic programming** - Transforming tree -> graph since root is also a leaf due to repeated states (see 17.10)\n* **Finite horizon** - there is a fixed time $N$ after which nothing matters - the game is over\n* **Discount factor** - preference of an agent for current reward over future rewards\n* **Additive reward** - what discounted rewards are reduce to, when $\\gamma = 1$\n\n\n## Module 8 Figures\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.1.jpg\" alt=\"Figure 16.1\" width=\"75%\"/>\n\n* **Figure 16.1** - (a) Nontransitive preferences $A \\succ B \\succ C \\succ A$ can result in irrational behevior (b) Decomposability Axiom\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.2.jpg\" alt=\"Figure 16.2\" width=\"75%\"/>\n\n* **Figure 16.2** - This figure shows the perceived utility of money. After your first billion, your next billion doesn't matter as much. (b) From risk-seaking to risk-averse\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.3.jpg\" alt=\"Figure 16.3\" width=\"75%\"/>\n\n* **Figure 16.3** - Unjustified optimism caused by choosing the best of $k$ options: utility distributed according to a unit normal (blue curve)\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.3.jpg\" alt=\"Figure 17.3\" width=\"75%\"/>\n\n* **Figure 17.3** - 4 x 3 world with $\\gamma = 1$ and $r = -0.04$ for transition to non-terminal states\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.8.jpg\" alt=\"Figure 17.8\" width=\"75%\"/>\n\n* **Figure 17.8** - In practice, it's often that $\\pi_i$ becomes optimal long before $U_i$ has converged\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.10.jpg\" alt=\"Figure 17.10\" width=\"75%\"/>\n\n* **Figure 17.10** - Expectimax Tree for 4 x 3: decision is extracted by taking ave at the chance nodes and max at the decision nodes\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xRbdqDox2zD"
      },
      "source": [
        "#Module 9 - Reinforcement Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpBvu7EMx7sy"
      },
      "source": [
        "# Russell and Norvig, AIMA Section 17.3 “Bandit Problems”\n",
        "chapter17_3 =  \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Russell and Norvig, AIMA Chapter 22 “Reinforcement Learning” (Sections 22.1-22.5)\n",
        "chapter22_1 =  \"\"\"\n",
        "In RL, a deterministic policy is a mapping from states to actions\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "chapter22_2 =  \"\"\" Your summary here \"\"\"\n",
        "chapter22_3 =  \"\"\" Your summary here \"\"\"\n",
        "chapter22_4 =  \"\"\" Your summary here \"\"\"\n",
        "chapter22_5 =  \"\"\" Your summary here \"\"\"\n",
        "\n",
        "\n",
        "# Lecture notes \n",
        "module9_lecture_notes = \"\"\"\n",
        "Basic idea of RL: \\\\\n",
        "- Receive feedback in the form of rewards \\\\\n",
        "- Agent's utility is defined by reward function \\\\\n",
        "- Learns to act to maximize expected rrewards \\\\\n",
        "- All learning is based on observed samples of outcomes \\\\\n",
        "\n",
        "It still assumed MDP: \\\\\n",
        "- set of states & actions; transition model and reward function (are not given) \\\\\n",
        "- still looks for policy $\\pi(s)$ \\\\\n",
        "- we don't know which states are good or what actions do \\\\\n",
        "- we must try $a$ and $s$ in order to learn \\\\\n",
        "\n",
        "MDP is offline (we can compute ahead of time & choose the best policy) \\\\\n",
        "RL is online (we have to try in order to learn) \\\\\n",
        "\n",
        "\\\\\n",
        "**Model-based Learning** \\\\\n",
        "- Learn an approximate model, based on experiences \\\\\n",
        "- Solve for values, as if learned model is correct \\\\\n",
        "\n",
        "(1) Learn empirical MDP model: \\\\\n",
        "- Count outcomes $s'$ for each $s, a$ \\\\\n",
        "- Normalize to give an estimate of $\\overline{T}(s, a, s')$ \\\\\n",
        "(2) Solve learned MDP \\\\\n",
        "\n",
        "\\\\\n",
        "**Model-free Learning** \\\\\n",
        "- Take $a$ and compare the results between what we thought was to happen with what actually happened \\\\\n",
        "- If smth deviates, adjust the results up or down \\\\\n",
        "- Not keeping track of neither $T$ nor $R$ \\\\\n",
        "Passive Reinforcement Learning (RL) and Active RL \\\\\n",
        "\n",
        "\\\\\n",
        "**Passive RL** \\\\\n",
        "Idea: Simplified Policy Eval that doesn't know $T$ or $R$ and is real online, you have to take actions in the world \\\\\n",
        "Given: \\\\\n",
        "- $\\pi$ \\\\\n",
        "- all the states that can be reached by agent \\\\\n",
        "Has 2 approaches: \\\\\n",
        "- Direct Evaluation \\\\\n",
        "- Temporal Difference Learning \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Direct Evaluation** \\\\\n",
        "Idea: \\\\\n",
        "- act, according to $\\pi$ \\\\\n",
        "- every time you visit the state, record the real sum of discounted rewards \\\\\n",
        "- average those samples \\\\\n",
        "Goal: Compute values for each state under $\\pi$ \\\\\n",
        "Example: \n",
        "$V(C) = \\\\frac{9 + 9 + 9 - 11}{4} = 16 / 4 = 4$ \\\\\n",
        "\n",
        "+/- of Direct Evaluation: \\\\\n",
        "$+$ Easy to understand \\\\\n",
        "$+$ No knowledge of $T$ and $R$ is required \\\\\n",
        "$+$ Eventually, it computes the correct ave values, using just sample transition \\\\\n",
        "$-$ Wastes information about states connections \\\\\n",
        "$-$ Each state must be learned separately \\\\\n",
        "$-$ Takes long time to learn as it needs a lot of samples \\\\\n",
        "$-$ Sample-based Policy Eval is a good idea, but there is no quarantee we can be in $s' n$ times. We can't rewind( \\\\\n",
        "\n",
        "To improve, we can try to play around with Policy Eval, \\\\\n",
        "but instead of $T$ and $R$, we can try to ave them without knowing them \\\\\n",
        "Idea: Take samples of outcomes $s'$ (by doing the action) and ave the outcomes \\\\\n",
        "$sample_n = R(s, \\pi(s), s'_n) + \\gamma V^{\\pi}_k(s'_n)$ \\\\\n",
        "$V^{\\pi}_{k+1}(s) = \\\\frac{1}{n} \\\\sum_i sample_i$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Temporal Difference Learning (TD)** \\\\\n",
        "The utility of a state is adjusted locally to agree with the utility of at least one successor state for both Adaptive dynamic programming (ADP) and TD learning \\\\\n",
        "Idea: \\\\\n",
        "- Update value $V(s)$ every time we experience a transition $(s, a, s', r)$ \\\\\n",
        "- Likely outcomes $s'$ will contribute updates more often \\\\\n",
        "- Policy is still fixed, still doing eval \\\\\n",
        "- Moves values toward value of whatever successor occurs, running average \\\\\n",
        "\n",
        "sample = $R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')$ - sample of $V(s)$ \\\\\n",
        "$V^{\\pi}(s) = V^{\\pi}(s) + \\\\alpha (sample - V^{\\pi}(s))$ - update to $V(s)$ \\\\\n",
        "Updates for TD Q-Learning: \\\\\n",
        "$Q(s, a) = Q(s, a) + \\\\alpha(R(s) + \\\\gamma \\\\max_{a^1} Q(s^1,a^1) - Q(s,a))$ \\\\\n",
        "\n",
        "$\\\\alpha$ - learning rate,\n",
        "how much we allow the sample (experimentation) to contribute \\\\\n",
        "$\\\\alpha = 1$ - all sample-dependent \\\\\n",
        "$\\\\alpha = 0$ - doesn't matter at all \\\\\n",
        "\n",
        "**Exponential/Moving Ave** \\\\\n",
        "- Running intropolation update: \n",
        "$x_n = (1 - \\\\alpha)x_{n-1} + \\\\alpha x_n$ \\\\\n",
        "- Makes recent samples more important \\\\\n",
        "$-$ Forgets about the past (and distant past values are wrong anyway) \\\\\n",
        "$+$ Decreasing learning rate $\\\\alpha$ can give converging averages \\\\\n",
        "\n",
        "+/- of Temporal Difference: \\\\\n",
        "$+$ Model-free way to do Policy Eval \\\\\n",
        "$-$ We can't turn values into the Policy \\\\\n",
        "To combat:\n",
        "- learn q-values, not values \\\\\n",
        "- that will make action selection model free too \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Active RL** \\\\\n",
        "Idea: instead of getting the policy, we gonna make all the choices, including what $a$ to take \\\\\n",
        "Has 2 approaches: \\\\\n",
        "- Q-Learning \\\\\n",
        "- Approximate Q-Learning \\\\\n",
        "\n",
        "\\\\\n",
        "**Q-Learning** \\\\\n",
        "Sample-based q-value iteration. \\\\\n",
        "~ Off-policy learning \\\\\n",
        "~ We need to take a lot of samples to understand $T$ and $R$ \\\\\n",
        "\n",
        "Idea: Learn $Q(s,a)$ as you go:\n",
        "- Receive a sample $(s, a, s', r)$ \\\\\n",
        "- Consider old estimate: $Q(s, a)$ \\\\\n",
        "- Consider new sample estimate: \\\\\n",
        "sample = $R(s, a, s') + \\gamma * max_{a'} Q(s', a')$ \\\\\n",
        "- Incorporate this new estimate into a running ave: \\\\\n",
        "$Q(s, a) = (1 - \\\\alpha)Q(s, a) + \\\\alpha [sample]$ \\\\\n",
        "\n",
        "Q-Learning Properties: \\\\\n",
        "$+$ Q-Learning converges to optimal policy, even if we're acting suboptimally \\\\\n",
        "$o$ Caveats: \\\\\n",
        "- You have to explore enough \\\\\n",
        "- You have to eventually make the learning rate small enough \\\\\n",
        "- But not decrease it too quickly \\\\\n",
        "- In a limit, you will learn the optimal policy no matter how you selected you actions \\\\\n",
        "\n",
        "\\\\\n",
        "**Exploration vs Explotation** \\\\\n",
        "2 ways to enforce Exploration Policy: \\\\\n",
        "\n",
        "$(1)$ ***Epsilon-greedy random action*** \\\\\n",
        "Idea:\n",
        "- Every time step, flip a coin \\\\\n",
        "- With $\\epsilon$ (small probability), act randomly \\\\\n",
        "- With $1 - \\epsilon$, act on current policy \\\\\n",
        "\n",
        "Downsides of Exploration: \\\\\n",
        "$-$ We eventually explored the space, but will keep picking things that no longer needed \\\\\n",
        "$=>$ Lower $\\epsilon$ over time \\\\\n",
        "$=>$ Exploration functions \\\\\n",
        "\n",
        "\n",
        "$(2)$ ***Exploration functions*** \\\\\n",
        "Idea:\n",
        "Explore areas whose badness is not established yet, eventually stop exploring \\\\\n",
        "\n",
        "$f(u, n) = u + \\\\frac{k}{n}$, \\\\\n",
        "where $u$ - curr. estimate of util \\\\\n",
        "$k$ - bonus \\\\\n",
        "$n$ - num of times the state was visited \\\\\n",
        "\n",
        "Leads to modified Q: \\\\\n",
        "$Q(s, a) = R(s, a, s') + \\gamma * max_{a'} f(Q(s', a'), N(s', a'))$ \\\\\n",
        "$#$ as more we visit $s$, as better util. estimate we get \\\\\n",
        "$#$ as more we visit $s$, as smaller bonus we get there $=>$ Encouraging us to visit unvisited states \\\\\n",
        "\n",
        "\\\\\n",
        "**Regret** \\\\\n",
        "Regret - measures our total cost of mistakes. \\\\\n",
        "Difference between our rewards (expected) and optimal rewards (expected) \\\\\n",
        "$o$ Random Exploration and Explotation both end of optimal, but first has higher regret \\\\\n",
        "\n",
        "\n",
        "\\\\\n",
        "**Approximate Q-Learning** \\\\\n",
        "Idea (fundamental to ML): \n",
        "- Learn about small number of training states from the experience \\\\\n",
        "- Generalize this exeprience to new similar situations \\\\\n",
        "- However, in reality we can't visit every single state/hold all q-tables in memory $=>$ Evaluation Function \\\\\n",
        "\n",
        "***Linear Value Functions*** \\\\\n",
        "Using feature representation, we can write q-value func for any state, using a few weights \\\\\n",
        "$V(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$ \\\\\n",
        "$Q(s, a) = w_1f_1(s, a) + w_2f_2(s, a) + ... + w_nf_n(s, a)$\n",
        "$+$ Our experience summed up in a few powerful nums \\\\\n",
        "$-$ Q-learning with Linear Function Approximation (weighted linear function of a set of features) will NOT always converge to the optimal policy (there is no guarantee) \\\\\n",
        "$-$ States may share features, but actually have very different values \\\\\n",
        "  => Approximate Q-Learning, evaluating both $s$ and $a$ allows us to combat it \\\\\n",
        "\n",
        "Intuitevely: \\\\\n",
        "- Adjusts w on active features \\\\\n",
        "- If smth bad happens, disprefers all states with that feature \\\\\n",
        "(~ disprefered all states, where pacman was too close to the ghost) \\\\\n",
        "\n",
        "```\n",
        "# Approximate Q-Learning update() func. \n",
        "def update(self, state, action, next_state, reward):\n",
        "        # Updates weights using least-squares approximation.\n",
        "        # Δ = R + γ V(s') - Q(s,a)\n",
        "        # Then updates the weights: w_i = w_i + α * Δ * f_i(s, a)\n",
        "        q = self.get_q_value(state, action)             # took delta out of the loop\n",
        "        new_value = self.get_value(next_state)\n",
        "        delta = reward + (self.discount * new_value) - q\n",
        "        for feature, value in self.extractor(state, action).items():\n",
        "            old_w = self.get_weight(feature)\n",
        "            self.w_table[feature] = old_w + self.learning_rate * delta * value\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "# Each definition should be at most 25 words long.\n",
        "module9_definitions = {\n",
        "\"term\" : \"your definition here\",\n",
        "\"Reinforcement Learning\" : \"ML training method, based on rewarding desired behavior or punishing undesired one\",\n",
        "\"Exploration\" : \"try unknown actions to get information\",\n",
        "\"Exploitation\" : \"use what you\",\n",
        "\"Regret\" : \"even if you learn intelligently, you make mistakes\",\n",
        "\"Sampling\" : \"you have to try things repeatedly, because of chance\",\n",
        "\"Difficulty\" : \"learning can be much harder than solving a known MDP\",\n",
        "\"****\" : \"\",\n",
        "\"Passive RL\" : \"takes & executes fixed policy with the goal to learn state values\",\n",
        "\"Direct Utility Estimation\" : \"Uses total observed reward-to-go for a given state as direct evidence for learning its utility\",\n",
        "\"Adaptive Dynamic Programming (ADP)\" : \"learns model and reward func. from observations => uses value/policy iteration to obtain utilities or optimal policy\",\n",
        "\"Active RL\" : \"instead of getting the policy, we gonna make all the choices, including what $a$ to take\",\n",
        "\"Q-Learning\" : \"Sample-based q-value iteration\",\n",
        "\"Regret\" : \"measures our total cost of mistakes\",\n",
        "\"Approximate Q-Learning\" : \"generalize learned data, instead of visiting every single state\"\n",
        "}\n",
        "\n",
        "\n",
        "# You can include explanations for figures in the textbook (except figures with pseduocode).\n",
        "# The figure will appear in your notes along with your explanation.\n",
        "module9_figure_explanations = {\n",
        "\"\" : \"\",\n",
        "}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydOK__lwWgdp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "32df9574-d1ed-4ad0-ae14-c7422870c2b8"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "\n",
        "module9_sections = [\n",
        "  (\"AIMA Chapter 17.3 Bandit Problems\", chapter17_3),\n",
        "  (\"AIMA Chapter 22.1 Learning from Rewards\", chapter22_1),\n",
        "  (\"AIMA Chapter 22.2 Passive Reinforcement Learning\", chapter22_2),\n",
        "  (\"AIMA Chapter 22.3 Active Reinforcement Learning\", chapter22_3),\n",
        "  (\"AIMA Chapter 22.4 Generalization in Reinforcement Learning\", chapter22_4),\n",
        "  (\"Module 9 Lecture Notes\", module9_lecture_notes),\n",
        "]\n",
        "\n",
        "module9_markdown = format_module(9, \"Reinforcement Learning\", module9_sections, module9_definitions, module9_figure_explanations)\n",
        "display(Markdown(module9_markdown))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 9: Reinforcement Learning\n## AIMA Chapter 17.3 Bandit Problems\n Your summary here \n\n## AIMA Chapter 22.1 Learning from Rewards\n\nIn RL, a deterministic policy is a mapping from states to actions\n\n\n## AIMA Chapter 22.2 Passive Reinforcement Learning\n Your summary here \n\n## AIMA Chapter 22.3 Active Reinforcement Learning\n Your summary here \n\n## AIMA Chapter 22.4 Generalization in Reinforcement Learning\n Your summary here \n\n## Module 9 Lecture Notes\n\nBasic idea of RL: \\\n- Receive feedback in the form of rewards \\\n- Agent's utility is defined by reward function \\\n- Learns to act to maximize expected rrewards \\\n- All learning is based on observed samples of outcomes \\\n\nIt still assumed MDP: \\\n- set of states & actions; transition model and reward function (are not given) \\\n- still looks for policy $\\pi(s)$ \\\n- we don't know which states are good or what actions do \\\n- we must try $a$ and $s$ in order to learn \\\n\nMDP is offline (we can compute ahead of time & choose the best policy) \\\nRL is online (we have to try in order to learn) \\\n\n\\\n**Model-based Learning** \\\n- Learn an approximate model, based on experiences \\\n- Solve for values, as if learned model is correct \\\n\n(1) Learn empirical MDP model: \\\n- Count outcomes $s'$ for each $s, a$ \\\n- Normalize to give an estimate of $\\overline{T}(s, a, s')$ \\\n(2) Solve learned MDP \\\n\n\\\n**Model-free Learning** \\\n- Take $a$ and compare the results between what we thought was to happen with what actually happened \\\n- If smth deviates, adjust the results up or down \\\n- Not keeping track of neither $T$ nor $R$ \\\nPassive Reinforcement Learning (RL) and Active RL \\\n\n\\\n**Passive RL** \\\nIdea: Simplified Policy Eval that doesn't know $T$ or $R$ and is real online, you have to take actions in the world \\\nGiven: \\\n- $\\pi$ \\\n- all the states that can be reached by agent \\\nHas 2 approaches: \\\n- Direct Evaluation \\\n- Temporal Difference Learning \\\n\n\n\\\n**Direct Evaluation** \\\nIdea: \\\n- act, according to $\\pi$ \\\n- every time you visit the state, record the real sum of discounted rewards \\\n- average those samples \\\nGoal: Compute values for each state under $\\pi$ \\\nExample: \n$V(C) = \\frac{9 + 9 + 9 - 11}{4} = 16 / 4 = 4$ \\\n\n+/- of Direct Evaluation: \\\n$+$ Easy to understand \\\n$+$ No knowledge of $T$ and $R$ is required \\\n$+$ Eventually, it computes the correct ave values, using just sample transition \\\n$-$ Wastes information about states connections \\\n$-$ Each state must be learned separately \\\n$-$ Takes long time to learn as it needs a lot of samples \\\n$-$ Sample-based Policy Eval is a good idea, but there is no quarantee we can be in $s' n$ times. We can't rewind( \\\n\nTo improve, we can try to play around with Policy Eval, \\\nbut instead of $T$ and $R$, we can try to ave them without knowing them \\\nIdea: Take samples of outcomes $s'$ (by doing the action) and ave the outcomes \\\n$sample_n = R(s, \\pi(s), s'_n) + \\gamma V^{\\pi}_k(s'_n)$ \\\n$V^{\\pi}_{k+1}(s) = \\frac{1}{n} \\sum_i sample_i$ \\\n\n\\\n**Temporal Difference Learning (TD)** \\\nThe utility of a state is adjusted locally to agree with the utility of at least one successor state for both Adaptive dynamic programming (ADP) and TD learning \\\nIdea: \\\n- Update value $V(s)$ every time we experience a transition $(s, a, s', r)$ \\\n- Likely outcomes $s'$ will contribute updates more often \\\n- Policy is still fixed, still doing eval \\\n- Moves values toward value of whatever successor occurs, running average \\\n\nsample = $R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')$ - sample of $V(s)$ \\\n$V^{\\pi}(s) = V^{\\pi}(s) + \\alpha (sample - V^{\\pi}(s))$ - update to $V(s)$ \\\nUpdates for TD Q-Learning: \\\n$Q(s, a) = Q(s, a) + \\alpha(R(s) + \\gamma \\max_{a^1} Q(s^1,a^1) - Q(s,a))$ \\\n\n$\\alpha$ - learning rate,\nhow much we allow the sample (experimentation) to contribute \\\n$\\alpha = 1$ - all sample-dependent \\\n$\\alpha = 0$ - doesn't matter at all \\\n\n**Exponential/Moving Ave** \\\n- Running intropolation update: \n$x_n = (1 - \\alpha)x_{n-1} + \\alpha x_n$ \\\n- Makes recent samples more important \\\n$-$ Forgets about the past (and distant past values are wrong anyway) \\\n$+$ Decreasing learning rate $\\alpha$ can give converging averages \\\n\n+/- of Temporal Difference: \\\n$+$ Model-free way to do Policy Eval \\\n$-$ We can't turn values into the Policy \\\nTo combat:\n- learn q-values, not values \\\n- that will make action selection model free too \\\n\n\n\\\n**Active RL** \\\nIdea: instead of getting the policy, we gonna make all the choices, including what $a$ to take \\\nHas 2 approaches: \\\n- Q-Learning \\\n- Approximate Q-Learning \\\n\n\\\n**Q-Learning** \\\nSample-based q-value iteration. \\\n~ Off-policy learning \\\n~ We need to take a lot of samples to understand $T$ and $R$ \\\n\nIdea: Learn $Q(s,a)$ as you go:\n- Receive a sample $(s, a, s', r)$ \\\n- Consider old estimate: $Q(s, a)$ \\\n- Consider new sample estimate: \\\nsample = $R(s, a, s') + \\gamma * max_{a'} Q(s', a')$ \\\n- Incorporate this new estimate into a running ave: \\\n$Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha [sample]$ \\\n\nQ-Learning Properties: \\\n$+$ Q-Learning converges to optimal policy, even if we're acting suboptimally \\\n$o$ Caveats: \\\n- You have to explore enough \\\n- You have to eventually make the learning rate small enough \\\n- But not decrease it too quickly \\\n- In a limit, you will learn the optimal policy no matter how you selected you actions \\\n\n\\\n**Exploration vs Explotation** \\\n2 ways to enforce Exploration Policy: \\\n\n$(1)$ ***Epsilon-greedy random action*** \\\nIdea:\n- Every time step, flip a coin \\\n- With $\\epsilon$ (small probability), act randomly \\\n- With $1 - \\epsilon$, act on current policy \\\n\nDownsides of Exploration: \\\n$-$ We eventually explored the space, but will keep picking things that no longer needed \\\n$=>$ Lower $\\epsilon$ over time \\\n$=>$ Exploration functions \\\n\n\n$(2)$ ***Exploration functions*** \\\nIdea:\nExplore areas whose badness is not established yet, eventually stop exploring \\\n\n$f(u, n) = u + \\frac{k}{n}$, \\\nwhere $u$ - curr. estimate of util \\\n$k$ - bonus \\\n$n$ - num of times the state was visited \\\n\nLeads to modified Q: \\\n$Q(s, a) = R(s, a, s') + \\gamma * max_{a'} f(Q(s', a'), N(s', a'))$ \\\n$#$ as more we visit $s$, as better util. estimate we get \\\n$#$ as more we visit $s$, as smaller bonus we get there $=>$ Encouraging us to visit unvisited states \\\n\n\\\n**Regret** \\\nRegret - measures our total cost of mistakes. \\\nDifference between our rewards (expected) and optimal rewards (expected) \\\n$o$ Random Exploration and Explotation both end of optimal, but first has higher regret \\\n\n\n\\\n**Approximate Q-Learning** \\\nIdea (fundamental to ML): \n- Learn about small number of training states from the experience \\\n- Generalize this exeprience to new similar situations \\\n- However, in reality we can't visit every single state/hold all q-tables in memory $=>$ Evaluation Function \\\n\n***Linear Value Functions*** \\\nUsing feature representation, we can write q-value func for any state, using a few weights \\\n$V(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$ \\\n$Q(s, a) = w_1f_1(s, a) + w_2f_2(s, a) + ... + w_nf_n(s, a)$\n$+$ Our experience summed up in a few powerful nums \\\n$-$ Q-learning with Linear Function Approximation (weighted linear function of a set of features) will NOT always converge to the optimal policy (there is no guarantee) \\\n$-$ States may share features, but actually have very different values \\\n  => Approximate Q-Learning, evaluating both $s$ and $a$ allows us to combat it \\\n\nIntuitevely: \\\n- Adjusts w on active features \\\n- If smth bad happens, disprefers all states with that feature \\\n(~ disprefered all states, where pacman was too close to the ghost) \\\n\n```\n# Approximate Q-Learning update() func. \ndef update(self, state, action, next_state, reward):\n        # Updates weights using least-squares approximation.\n        # Δ = R + γ V(s') - Q(s,a)\n        # Then updates the weights: w_i = w_i + α * Δ * f_i(s, a)\n        q = self.get_q_value(state, action)             # took delta out of the loop\n        new_value = self.get_value(next_state)\n        delta = reward + (self.discount * new_value) - q\n        for feature, value in self.extractor(state, action).items():\n            old_w = self.get_weight(feature)\n            self.w_table[feature] = old_w + self.learning_rate * delta * value\n```\n\n\n## Module 9 Definitions\n* **Term** - your definition here\n* **Reinforcement learning** - ML training method, based on rewarding desired behavior or punishing undesired one\n* **Exploration** - try unknown actions to get information\n* **Exploitation** - use what you\n* **Regret** - measures our total cost of mistakes\n* **Sampling** - you have to try things repeatedly, because of chance\n* **Difficulty** - learning can be much harder than solving a known MDP\n* ******** - \n* **Passive rl** - takes & executes fixed policy with the goal to learn state values\n* **Direct utility estimation** - Uses total observed reward-to-go for a given state as direct evidence for learning its utility\n* **Adaptive dynamic programming (adp)** - learns model and reward func. from observations => uses value/policy iteration to obtain utilities or optimal policy\n* **Active rl** - instead of getting the policy, we gonna make all the choices, including what $a$ to take\n* **Q-learning** - Sample-based q-value iteration\n* **Approximate q-learning** - generalize learned data, instead of visiting every single state\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46MQLR-Gx8dZ"
      },
      "source": [
        "#Module 10 - Probabilities and Language Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BN0b1eVYx-nd"
      },
      "source": [
        "# Russell and Norvig, AIMA 4th Edition, AIMA Chapter 12 “Quantifying Uncertainty” (Sections 12.1-12.7)\n",
        "chapter12_1 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_2 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_3 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_4 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_5 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_6 = \"\"\" Your summary here \"\"\"\n",
        "chapter12_7 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "\n",
        "# Jurafsky and Martin, Speech and Language Processing Chapter 3 “N-Gram Language Models”  (Sections 3.1-3.5,3.7)\n",
        "SLP_chapter3_1 = \"\"\" Your summary here \"\"\"\n",
        "SLP_chapter3_2 = \"\"\" Your summary here \"\"\"\n",
        "SLP_chapter3_3 = \"\"\" Your summary here \"\"\"\n",
        "SLP_chapter3_4 = \"\"\" Your summary here \"\"\"\n",
        "SLP_chapter3_5 = \"\"\" Your summary here \"\"\"\n",
        "SLP_chapter3_7 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "\n",
        "# Lecture notes \n",
        "module10_lecture_notes = \"\"\" \n",
        "**Axiom of Probability** \\\\\n",
        "$0 \\leq P(w) \\leq 1$ \\\\\n",
        "$\\sum_w P(w) = 1$ - total probability of sample space is always 1 \\\\\n",
        "\n",
        "$d^n$ - size of distribution of $n$ vars with domain size $d$ \\\\\n",
        "\n",
        "Conditional Probability: \\\\\n",
        "$P(a | b) = \\\\frac{P(a, b)}{P(b)}$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Probabalistic Inference** \\\\\n",
        "\n",
        "$(1)$ Inference by Enumeration \\\\\n",
        "- Select entries, consistent with evidence \\\\\n",
        "- Sum out hidden vars to get joint of $Q$ and evidence: $P(a, c) = \\\\sum b P(a, b, c)$ \\\\\n",
        "- Normalize: $\\\\frac{1}{z}$ \\\\\n",
        "\n",
        "$(2)$ Product Rule \\\\\n",
        "$P(y) P(x | y) = P(x, y)$ \\\\\n",
        "\n",
        "$(3)$ Chain Rule \\\\\n",
        "$P(x, y, z) = P(x) P(y | x) P (z | x, y)$ \\\\\n",
        "$P(x_1, x_2, ... x_i) = \\\\prod_i P(x_i | x_1, x_2, ... x_{i - 1})$ \\\\\n",
        "\n",
        "Examples for $P(X1, X2, X3, X4)$: \\\\\n",
        "$P(X1, X2, X3, X4) = P(X1) \\cdot P(X2 | X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\\\n",
        "$P(X1, X2, X3, X4) = P(X1 | X2, X3, X4) \\cdot P(X4) \\cdot P(X3 | X4) \\cdot P(X2 | X3, X4)$ \\\\\n",
        "$P(X1, X2, X3, X4) = P(X2, X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\\\n",
        "\n",
        "$(4)$ Bayes Rule \\\\\n",
        "$P(x, y) = P(x | y) P(y) = P(y | x) P(x)$ \\\\\n",
        "$P(x | y) = \\\\frac{P(y, x)}{P(y)} P(cx)$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Probabalistic Language Models** \\\\\n",
        "Markov Assumption - prob. of upcoming word only depends on previous $k$ words, not the whole context \\\\\n",
        "Markov Assumption makes it possible to estimate the probabilities from data \\\\\n",
        "\n",
        "***n-grams*** \\\\\n",
        "$(1)$ Unigram - no history: $P(w) = \\\\frac{count(w)}{all words}$ \\\\\n",
        "$(2)$ Bigram - 1 word as history \\\\\n",
        "$(4)$ 4-gram - 3 words as history: $P(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\\\frac{C(w_{i-3} w_{i-2} w_{i-1} w_i)}{C(w_{i-3} w_{i-2} w_{i - 1})}$ \\\\\n",
        "\n",
        "$-$ don't take into account any long-distance dependencies \\\\\n",
        "$-$ unobsereved words and sequences => Laplace Smoothing \\\\\n",
        "$P(w_i | w_{i-1}) = \\\\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$, where $C(w_{i-1})$ - aka positive/negative, $V$ - total original words in the whole sample \\\\\n",
        "\n",
        "$-$ underflow => log space \\\\\n",
        "$\\log(p_1 \\cdot p_2 \\cdot p_3) = \\log p_1 + \\log p_2 + \\log p_3$ - addresses the risk of underflow and p = 0\n",
        "\n",
        "\\\\\n",
        "**Perplexity** \\\\\n",
        "(Intrinsic eval metric) \\\\\n",
        "\n",
        "$PP(w) = P(w_1 w_2 ... w_n)^{- \\\\frac{1}{N}} = \\\\sqrt[N]{\\\\frac{1}{P(w_1 w_2 ... w_n)}}$ \\\\\n",
        "$PP(w) = \\\\sqrt[N]{\\\\prod^N \\\\frac{1}{P(w_i | w_{i-1})}}$ - for Bi-gram \\\\\n",
        "\n",
        "Random 10-word perplexity: \\\\\n",
        "$PP(w) = P(w_1 w_2 ... w_{10})^{- \\\\frac{1}{10}} = \\\\frac{1}{10}^{- \\\\frac{1}{10} \\cdot 10} = \\\\frac{1}{10}^{-1} = 10$ - very high preplexity:( \\\\\n",
        "\n",
        "- The best LM is the one that best predicts an unseen test set - gives the highest P(sentence) \\\\\n",
        "- Max prob = min perplexity \\\\\n",
        "- As smaller the perplexity as better the model \\\\\n",
        "\n",
        "\\\\\n",
        "**n-grams Summary:** \\\\\n",
        "$+$ n-grams only work well for word prediction, when test corpus looks like the training corpus \\\\\n",
        "$-$ Rarely happen in real life \\\\\n",
        "$-$ Difficult to train robust models that generalize \\\\\n",
        "$-$ Zeroes is the issue \\\\\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Each definition should be at most 25 words long.\n",
        "module10_definitions = {\n",
        "\"Model\" : \"knowledge how known vars relate to unknown ones\",\n",
        "\"Sample Space\" : \"all possible worlds\",\n",
        "\"Marginalization\" : \"summing out, collapsing the rows and adding them together\",\n",
        "\"Conditional Probability\" : \"a measure of probability of the event occuring, given another event that has occured already\",\n",
        "\"Conditional Distribution\" : \"probability distribution over some vars, given fixed prob. of others\",\n",
        "\"Probabalistic Inference\" : \"computes desired prob. from other known probabilities\",\n",
        "\"Perplexity\" : \"(how well we can predict the next word) inverse prob. of the test set, normilized by the num of words\",\n",
        "\"Intrinsic\" : \"Essential, necessary and intentional\",\n",
        "\"Explicit\" : \"something that is stated plainly\", \n",
        "\"Implicit\" : \"something that is implied and not stated directly\"\n",
        "}\n",
        "\n",
        "# You can include explanations for figures in the textbook (except figures with pseduocode).\n",
        "# The figure will appear in your notes along with your explanation.\n",
        "module10_figure_explanations = {\n",
        "\"\" : \"\",\n",
        "}\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvtN3p7ET3Tx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "75cc3fa7-d53c-40f3-d731-2191b0d20047"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "module10_sections = [\n",
        "  (\"AIMA Chapter 12.1 Acting under Uncertainty\", chapter12_1),\n",
        "  (\"AIMA Chapter 12.2 Basic Probability Notation\", chapter12_2),\n",
        "  (\"AIMA Chapter 12.3 Inference Using Full Joint Distributions\", chapter12_3),\n",
        "  (\"AIMA Chapter 12.4 Independence\", chapter12_4),\n",
        "  (\"AIMA Chapter 12.5 Bayes’ Rule and Its Use\", chapter12_5),\n",
        "  (\"AIMA Chapter 12.6 Naive Bayes Models\", chapter12_6),\n",
        "  (\"AIMA Chapter 12.7 The Wumpus World Revisited\", chapter12_7),\n",
        "  (\"SLP Chapter 3.1 N-Grams\", SLP_chapter3_1),\n",
        "  (\"SLP Chapter 3.2 Evaluating Language Models\", SLP_chapter3_2),\n",
        "  (\"SLP Chapter 3.3 Sampling sentences from a language model\", SLP_chapter3_3),\n",
        "  (\"SLP Chapter 3.4 Generalization and Zeros\", SLP_chapter3_4),\n",
        "  (\"SLP Chapter 3.5 Smoothing\", SLP_chapter3_5),\n",
        "  (\"SLP Chapter 3.7 Huge Language Models and Stupid Backoff\", SLP_chapter3_7),\n",
        "  (\"Module 10 Lecture Notes\", module10_lecture_notes),\n",
        "]\n",
        "\n",
        "module10_markdown = format_module(10, \"Probabilies and Language Models\", module10_sections, module10_definitions, module10_figure_explanations)\n",
        "display(Markdown(module10_markdown))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 10: Probabilies and Language Models\n## AIMA Chapter 12.1 Acting under Uncertainty\n Your summary here \n\n## AIMA Chapter 12.2 Basic Probability Notation\n Your summary here \n\n## AIMA Chapter 12.3 Inference Using Full Joint Distributions\n Your summary here \n\n## AIMA Chapter 12.4 Independence\n Your summary here \n\n## AIMA Chapter 12.5 Bayes’ Rule and Its Use\n Your summary here \n\n## AIMA Chapter 12.6 Naive Bayes Models\n Your summary here \n\n## AIMA Chapter 12.7 The Wumpus World Revisited\n Your summary here \n\n## SLP Chapter 3.1 N-Grams\n Your summary here \n\n## SLP Chapter 3.2 Evaluating Language Models\n Your summary here \n\n## SLP Chapter 3.3 Sampling sentences from a language model\n Your summary here \n\n## SLP Chapter 3.4 Generalization and Zeros\n Your summary here \n\n## SLP Chapter 3.5 Smoothing\n Your summary here \n\n## SLP Chapter 3.7 Huge Language Models and Stupid Backoff\n Your summary here \n\n## Module 10 Lecture Notes\n \n**Axiom of Probability** \\\n$0 \\leq P(w) \\leq 1$ \\\n$\\sum_w P(w) = 1$ - total probability of sample space is always 1 \\\n\n$d^n$ - size of distribution of $n$ vars with domain size $d$ \\\n\nConditional Probability: \\\n$P(a | b) = \\frac{P(a, b)}{P(b)}$ \\\n\n\\\n**Probabalistic Inference** \\\n\n$(1)$ Inference by Enumeration \\\n- Select entries, consistent with evidence \\\n- Sum out hidden vars to get joint of $Q$ and evidence: $P(a, c) = \\sum b P(a, b, c)$ \\\n- Normalize: $\\frac{1}{z}$ \\\n\n$(2)$ Product Rule \\\n$P(y) P(x | y) = P(x, y)$ \\\n\n$(3)$ Chain Rule \\\n$P(x, y, z) = P(x) P(y | x) P (z | x, y)$ \\\n$P(x_1, x_2, ... x_i) = \\prod_i P(x_i | x_1, x_2, ... x_{i - 1})$ \\\n\nExamples for $P(X1, X2, X3, X4)$: \\\n$P(X1, X2, X3, X4) = P(X1) \\cdot P(X2 | X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\n$P(X1, X2, X3, X4) = P(X1 | X2, X3, X4) \\cdot P(X4) \\cdot P(X3 | X4) \\cdot P(X2 | X3, X4)$ \\\n$P(X1, X2, X3, X4) = P(X2, X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\n\n$(4)$ Bayes Rule \\\n$P(x, y) = P(x | y) P(y) = P(y | x) P(x)$ \\\n$P(x | y) = \\frac{P(y, x)}{P(y)} P(cx)$ \\\n\n\\\n**Probabalistic Language Models** \\\nMarkov Assumption - prob. of upcoming word only depends on previous $k$ words, not the whole context \\\nMarkov Assumption makes it possible to estimate the probabilities from data \\\n\n***n-grams*** \\\n$(1)$ Unigram - no history: $P(w) = \\frac{count(w)}{all words}$ \\\n$(2)$ Bigram - 1 word as history \\\n$(4)$ 4-gram - 3 words as history: $P(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{C(w_{i-3} w_{i-2} w_{i-1} w_i)}{C(w_{i-3} w_{i-2} w_{i - 1})}$ \\\n\n$-$ don't take into account any long-distance dependencies \\\n$-$ unobsereved words and sequences => Laplace Smoothing \\\n$P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$, where $C(w_{i-1})$ - aka positive/negative, $V$ - total original words in the whole sample \\\n\n$-$ underflow => log space \\\n$\\log(p_1 \\cdot p_2 \\cdot p_3) = \\log p_1 + \\log p_2 + \\log p_3$ - addresses the risk of underflow and p = 0\n\n\\\n**Perplexity** \\\n(Intrinsic eval metric) \\\n\n$PP(w) = P(w_1 w_2 ... w_n)^{- \\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_n)}}$ \\\n$PP(w) = \\sqrt[N]{\\prod^N \\frac{1}{P(w_i | w_{i-1})}}$ - for Bi-gram \\\n\nRandom 10-word perplexity: \\\n$PP(w) = P(w_1 w_2 ... w_{10})^{- \\frac{1}{10}} = \\frac{1}{10}^{- \\frac{1}{10} \\cdot 10} = \\frac{1}{10}^{-1} = 10$ - very high preplexity:( \\\n\n- The best LM is the one that best predicts an unseen test set - gives the highest P(sentence) \\\n- Max prob = min perplexity \\\n- As smaller the perplexity as better the model \\\n\n\\\n**n-grams Summary:** \\\n$+$ n-grams only work well for word prediction, when test corpus looks like the training corpus \\\n$-$ Rarely happen in real life \\\n$-$ Difficult to train robust models that generalize \\\n$-$ Zeroes is the issue \\\n\n\n\n## Module 10 Definitions\n* **Model** - knowledge how known vars relate to unknown ones\n* **Sample space** - all possible worlds\n* **Marginalization** - summing out, collapsing the rows and adding them together\n* **Conditional probability** - a measure of probability of the event occuring, given another event that has occured already\n* **Conditional distribution** - probability distribution over some vars, given fixed prob. of others\n* **Probabalistic inference** - computes desired prob. from other known probabilities\n* **Perplexity** - (how well we can predict the next word) inverse prob. of the test set, normilized by the num of words\n* **Intrinsic** - Essential, necessary and intentional\n* **Explicit** - something that is stated plainly\n* **Implicit** - something that is implied and not stated directly\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHREsHSSyA-h"
      },
      "source": [
        "#Module 11 - Probabilistic Reasoning and Bayes’ Nets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuFC8w7CyCns"
      },
      "source": [
        "# Russell and Norvig, Russell and Norvig, AIMA Chapter 13 “Probabilistic Reasoning” (13.1, 13.2, 13.3)\n",
        "chapter13_1 = \"\"\"\n",
        "If two nodes X and Y in a BN do not share a path, X and Y are conditionally independent. \n",
        "\"\"\"\n",
        "chapter13_2 = \"\"\"\n",
        "Locally structured (sparse) system grows linearly in complexity (rather than exponentially). \\\\\n",
        "\n",
        "More than one BN can be used to represent the same joint distribution.\n",
        "\"\"\"\n",
        "chapter13_3 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "\n",
        "# Lecture notes \n",
        "module11_lecture_notes = \"\"\"\n",
        "Joint Probability: \\\\\n",
        "$P(x, y) = P(y) P(x|y) = P(x) P(y|x)$ \\\\\n",
        "\n",
        "Independence $\\perp\\!\\!\\!\\perp$: \\\\\n",
        "$P(A, B) = P(A) P(B)$ - if $\\\\neq$ they are not Independent \\\\\n",
        "$P(x | y) = P(x)$ - given $y$ adds no new information => P(x) \\\\\n",
        "- Independence (I) significantly simplifies our model assumptions \\\\\n",
        "- $2^n$ - to represent the joint distribution of n fair coin flips \\\\\n",
        "- $2n$ - if coins are independent \\\\\n",
        "- However, I is rare in practice, so we will rely more on the Conditional Independence (CI) \\\\\n",
        "\n",
        "\\\\\n",
        "**Conditional Independence** \\\\\n",
        "$A$ and $B$ are conditionally independent given C, iff $P(A, B | C) = P(A | C) \\cdot P(B | C)$ \\\\\n",
        "=> \\\\\n",
        "$P(A | B, C) = P(A | C) $  - $B$ doesn't add any new information, like Fire in case of Alarm when we already see the smoke \\\\\n",
        "\n",
        "$(T \\perp\\!\\!\\!\\perp U) | R$ - traffic is CI of umbrella if we observed that it's raining \\\\\n",
        "\n",
        "\\\\\n",
        "**CI and Chain Rule** \\\\\n",
        "$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | T, R)$ - traffic, rain, and umbrella  \\\\\n",
        "$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | R)$ - assuming CI $(T \\perp\\!\\!\\!\\perp U) | R$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Bayes' Nets (BN)** \\\\\n",
        "Bayes' Nets = Bayesian Network \\\\\n",
        "BN = Graph + Local Conditional probabilities: \\\\\n",
        "- Set of nodes - one per var \\\\\n",
        "- DAG - Directed Acyclic Graph \\\\\n",
        "- Conditional Distribution for each node \\\\\n",
        "\n",
        "Also, BN implicitely represent Joint prob. distribution as a product of local conditional distributions: \\\\\n",
        "$P(x_1, x_2 ... x_n) = \\prod^n_{i = 1} P(x_i | parents(X_i))$ \\\\\n",
        "\n",
        "Why BN result in proper Joint Probability Distribution (JPD)?\n",
        "- JPD definition is made using Chain Rule \\\\\n",
        "- When we assume CI, we simplify => BN representing JDP \\\\\n",
        "- Not every BN can represent every JDP as topology enforces certain CI \\\\\n",
        "\n",
        "***Advantages of BN*** \\\\\n",
        "$o$ $2^n$ - size of full Joint Distribution table of $n$ Boolean vars \\\\\n",
        "$o$ Both give us power to calc JPD \\\\\n",
        "$+$ $N \\cdot 2^{k+1}$ - size of BN with $N$ nodes with upto $k$ parents \\\\\n",
        "$+$ BN gives us a huge space saving \\\\\n",
        "$+$ Easier to elicit conditional prob. tables \\\\\n",
        "$+$ Faster way to answer quiries \\\\\n",
        "\n",
        "$o$ BN with no arcs shows absolute independence of nodes \\\\\n",
        "\n",
        "***Causality in BN*** \\\\\n",
        "- Causality helps to logically understand and explain BNs, but makes no math difference \\\\\n",
        "- Causality helps to simplify graphs \\\\\n",
        "- Topology and arrows may encode causality, but they really **encode CI** \\\\\n",
        "\n",
        "***CI in BN*** \\\\\n",
        "$(X) -> (Y) -> (Z) -> (W)$ - Causal Chain \\\\\n",
        "\n",
        "$(Z \\perp\\!\\!\\!\\perp X) | Y$ \\\\\n",
        "$P(Z | X, Y) = P(Z | Y)$ \\\\\n",
        "\n",
        "$(W \\perp\\!\\!\\!\\perp X, Y) | Z$ \\\\\n",
        "$P(W | X, Y, Z) = P(W | Z)$ \\\\\n",
        "- Conditional prob. distribution of a var in BN is based on prob. distribution of its parents \\\\\n",
        "\n",
        "\\\\\n",
        "**Decendents D-Separation** \\\\\n",
        "\n",
        "\\\\\n",
        "***Causal Chain*** \\\\\n",
        "$(X) -> (Y) -> (Z)$ \\\\\n",
        "$P(X, Y, Z) = P(X) \\cdot P(Y | X) \\cdot P(Z | Y)$ \\\\\n",
        "\n",
        "$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\\\n",
        "$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\\\n",
        "$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\\\n",
        "\n",
        "$o$ Example: Once we've seen the rain, observing Low Pressure gives us no new information \\\\\n",
        "$o$ Evidence along the chain \"blocks\" the influence \\\\\n",
        "\n",
        "\\\\\n",
        "***Common Cause*** \\\\\n",
        "$....(Y)...$ \\\\\n",
        "$.../ ...\\setminus ...$ \\\\\n",
        "$(X)...(Z)$ \\\\\n",
        "$P(X, Y, Z) = P(Y) \\cdot P(X | Y) \\cdot P(Z | Y)$ \\\\\n",
        "$Y:$ Project Due \\\\\n",
        "$X:$ Ed busy \\\\\n",
        "$Z:$ OH are full \\\\\n",
        "\n",
        "$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\\\n",
        "$o$ If we know $X$, then we also know $Z$ \\\\\n",
        "\n",
        "$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\\\n",
        "$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\\\n",
        "$o$ If we haven't observed $Y$, observing $X$ tells us smth useful \\\\\n",
        "$o$ But once we have observed $Y$, seeing $X$ adds no additional info \\\\\n",
        "$o$ Observing the cause \"blocks\" the influence between two different effects \\\\\n",
        "\n",
        "\\\\\n",
        "***Common Effect*** \\\\\n",
        "$(X)...(Y)$ \\\\\n",
        "$...\\setminus ../...$ \\\\\n",
        "$....(Z)....$ \\\\\n",
        "\n",
        "$P(X, Y, Z) = P(X) \\cdot P(Y) \\cdot P(Z | X) \\cdot P(Z | Y)$ \\\\\n",
        "$X:$ Rain \\\\\n",
        "$Y:$ Ballgame \\\\\n",
        "$Z:$ Traffic \\\\\n",
        "\n",
        "$(1)$ Are $X$ and $Y$ fully independent? $\\implies$ Yes \\\\\n",
        "$X \\perp\\!\\!\\!\\perp Y$ \\\\\n",
        "$o$ The Rain and Ballgame cause traffic, but they are not correlated \\\\\n",
        "\n",
        "$(2)$ Are $X$ and $Y$ independent given $Z$? $\\implies$ No \\\\\n",
        "$o$ Seeing Traffic puts Rain & Ballgame in competition as explanation \\\\\n",
        "$o$ Observing effect \"activates\" the influence between possible causes \\\\\n",
        "\n",
        "\\\\\n",
        "**D-Separation Inference** \\\\\n",
        "Analyze the Graph and Decompose it into repetition of these 3 canonical cases. \\\\\n",
        "\n",
        "\\\\\n",
        "**Active Triples** .... | ... **Inactive Triples** \\\\\n",
        "$ () - () - ()$ ..... | .... $() - (\\\\oplus) - ()$ \\\\\n",
        "\n",
        "$....()...$............... | .......$...(\\\\oplus)...$... \\\\\n",
        "$../..\\setminus..$............. | .......$../...\\setminus..$.. \\\\\n",
        "$()....()$............... | ........$()....()$...... \\\\\n",
        "\n",
        "$()....()$............... | ........$()....()$...... \\\\\n",
        "$.\\setminus../$................. | .......$.\\setminus../$..... \\\\\n",
        "$..(\\\\oplus)...$.............. | .......$...()...$... \\\\\n",
        "\n",
        "$()....()$............... | \\\\\n",
        "$.\\setminus../$................. | \\\\\n",
        "$....()...$............... | \\\\\n",
        "$....\\downarrow...$.............. | \\\\\n",
        "$...(\\\\oplus)...$............. | \\\\\n",
        "\n",
        "$(\\oplus)$ - given, observed node \\\\\n",
        "- A path is active, if each triple is active \\\\\n",
        "- Active path = independence is not guaranteed \\\\\n",
        "- Single inactive segment makes the whole path inactive \\\\\n",
        "- No active path = independence \\\\\n",
        "- Only consider path that is formed from assignment nodes, all other (upper) nodes can be ignored \\\\\n",
        "- Every variable in a BN is independent given its parents. And not independent of all of its descendants given its children. \\\\\n",
        "\n",
        "\\\\\n",
        "**Probabalistic Inference** \\\\\n",
        "\n",
        "$(5)$ Variable Elimination \\\\\n",
        "Advance technique of interleaves joining & marginalizing \\\\\n",
        "- Usually, much faster than Inference by Enumeration but still NP-Hard \\\\\n",
        "\"\"\"\n",
        "\n",
        "# Definitions\n",
        "module11_definitions = {\n",
        "\"Independence\" : \"two events are independent, if occurence of one doesn't affect prob. of occurence of the other\",\n",
        "\"Bayes' Nets\" : \"graphical models that allow us to express CI assumptions explicitely\"    \n",
        "}\n",
        "\n",
        "# You can include explanations for figures in the textbook (except figures with pseduocode).\n",
        "# The figure will appear in your notes along with your explanation.\n",
        "module11_figure_explanations = {\n",
        "\"\" : \"\",\n",
        "}"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqtEs0gHs9sO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "eb30ca5e-7989-4237-d649-e21494cf0a57"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "module11_sections = [\n",
        "  (\"AIMA Chapter 13.1 Representing Knowledge in an Uncertain Domain\", chapter13_1),\n",
        "  (\"AIMA Chapter 13.2 The Semantics of Bayesian Networks\", chapter13_2),\n",
        "  (\"AIMA 13.3 Exact Inference in Bayesian Networks\", chapter13_3),\n",
        "  (\"Module 11 Lecture Notes\", module11_lecture_notes),\n",
        "]\n",
        "\n",
        "module11_markdown = format_module(11, \"Probabilistic Reasoning and Bayes’ Nets\", \n",
        "                         module11_sections, \n",
        "                         module11_definitions,\n",
        "                         module11_figure_explanations)\n",
        "display(Markdown(module11_markdown))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 11: Probabilistic Reasoning and Bayes’ Nets\n## AIMA Chapter 13.1 Representing Knowledge in an Uncertain Domain\n\nIf two nodes X and Y in a BN do not share a path, X and Y are conditionally independent. \n\n\n## AIMA Chapter 13.2 The Semantics of Bayesian Networks\n\nLocally structured (sparse) system grows linearly in complexity (rather than exponentially). \\\n\nMore than one BN can be used to represent the same joint distribution.\n\n\n## AIMA 13.3 Exact Inference in Bayesian Networks\n Your summary here \n\n## Module 11 Lecture Notes\n\nJoint Probability: \\\n$P(x, y) = P(y) P(x|y) = P(x) P(y|x)$ \\\n\nIndependence $\\perp\\!\\!\\!\\perp$: \\\n$P(A, B) = P(A) P(B)$ - if $\\neq$ they are not Independent \\\n$P(x | y) = P(x)$ - given $y$ adds no new information => P(x) \\\n- Independence (I) significantly simplifies our model assumptions \\\n- $2^n$ - to represent the joint distribution of n fair coin flips \\\n- $2n$ - if coins are independent \\\n- However, I is rare in practice, so we will rely more on the Conditional Independence (CI) \\\n\n\\\n**Conditional Independence** \\\n$A$ and $B$ are conditionally independent given C, iff $P(A, B | C) = P(A | C) \\cdot P(B | C)$ \\\n=> \\\n$P(A | B, C) = P(A | C) $  - $B$ doesn't add any new information, like Fire in case of Alarm when we already see the smoke \\\n\n$(T \\perp\\!\\!\\!\\perp U) | R$ - traffic is CI of umbrella if we observed that it's raining \\\n\n\\\n**CI and Chain Rule** \\\n$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | T, R)$ - traffic, rain, and umbrella  \\\n$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | R)$ - assuming CI $(T \\perp\\!\\!\\!\\perp U) | R$ \\\n\n\\\n**Bayes' Nets (BN)** \\\nBayes' Nets = Bayesian Network \\\nBN = Graph + Local Conditional probabilities: \\\n- Set of nodes - one per var \\\n- DAG - Directed Acyclic Graph \\\n- Conditional Distribution for each node \\\n\nAlso, BN implicitely represent Joint prob. distribution as a product of local conditional distributions: \\\n$P(x_1, x_2 ... x_n) = \\prod^n_{i = 1} P(x_i | parents(X_i))$ \\\n\nWhy BN result in proper Joint Probability Distribution (JPD)?\n- JPD definition is made using Chain Rule \\\n- When we assume CI, we simplify => BN representing JDP \\\n- Not every BN can represent every JDP as topology enforces certain CI \\\n\n***Advantages of BN*** \\\n$o$ $2^n$ - size of full Joint Distribution table of $n$ Boolean vars \\\n$o$ Both give us power to calc JPD \\\n$+$ $N \\cdot 2^{k+1}$ - size of BN with $N$ nodes with upto $k$ parents \\\n$+$ BN gives us a huge space saving \\\n$+$ Easier to elicit conditional prob. tables \\\n$+$ Faster way to answer quiries \\\n\n$o$ BN with no arcs shows absolute independence of nodes \\\n\n***Causality in BN*** \\\n- Causality helps to logically understand and explain BNs, but makes no math difference \\\n- Causality helps to simplify graphs \\\n- Topology and arrows may encode causality, but they really **encode CI** \\\n\n***CI in BN*** \\\n$(X) -> (Y) -> (Z) -> (W)$ - Causal Chain \\\n\n$(Z \\perp\\!\\!\\!\\perp X) | Y$ \\\n$P(Z | X, Y) = P(Z | Y)$ \\\n\n$(W \\perp\\!\\!\\!\\perp X, Y) | Z$ \\\n$P(W | X, Y, Z) = P(W | Z)$ \\\n- Conditional prob. distribution of a var in BN is based on prob. distribution of its parents \\\n\n\\\n**Decendents D-Separation** \\\n\n\\\n***Causal Chain*** \\\n$(X) -> (Y) -> (Z)$ \\\n$P(X, Y, Z) = P(X) \\cdot P(Y | X) \\cdot P(Z | Y)$ \\\n\n$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\n$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\n$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\n\n$o$ Example: Once we've seen the rain, observing Low Pressure gives us no new information \\\n$o$ Evidence along the chain \"blocks\" the influence \\\n\n\\\n***Common Cause*** \\\n$....(Y)...$ \\\n$.../ ...\\setminus ...$ \\\n$(X)...(Z)$ \\\n$P(X, Y, Z) = P(Y) \\cdot P(X | Y) \\cdot P(Z | Y)$ \\\n$Y:$ Project Due \\\n$X:$ Ed busy \\\n$Z:$ OH are full \\\n\n$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\n$o$ If we know $X$, then we also know $Z$ \\\n\n$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\n$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\n$o$ If we haven't observed $Y$, observing $X$ tells us smth useful \\\n$o$ But once we have observed $Y$, seeing $X$ adds no additional info \\\n$o$ Observing the cause \"blocks\" the influence between two different effects \\\n\n\\\n***Common Effect*** \\\n$(X)...(Y)$ \\\n$...\\setminus ../...$ \\\n$....(Z)....$ \\\n\n$P(X, Y, Z) = P(X) \\cdot P(Y) \\cdot P(Z | X) \\cdot P(Z | Y)$ \\\n$X:$ Rain \\\n$Y:$ Ballgame \\\n$Z:$ Traffic \\\n\n$(1)$ Are $X$ and $Y$ fully independent? $\\implies$ Yes \\\n$X \\perp\\!\\!\\!\\perp Y$ \\\n$o$ The Rain and Ballgame cause traffic, but they are not correlated \\\n\n$(2)$ Are $X$ and $Y$ independent given $Z$? $\\implies$ No \\\n$o$ Seeing Traffic puts Rain & Ballgame in competition as explanation \\\n$o$ Observing effect \"activates\" the influence between possible causes \\\n\n\\\n**D-Separation Inference** \\\nAnalyze the Graph and Decompose it into repetition of these 3 canonical cases. \\\n\n\\\n**Active Triples** .... | ... **Inactive Triples** \\\n$ () - () - ()$ ..... | .... $() - (\\oplus) - ()$ \\\n\n$....()...$............... | .......$...(\\oplus)...$... \\\n$../..\\setminus..$............. | .......$../...\\setminus..$.. \\\n$()....()$............... | ........$()....()$...... \\\n\n$()....()$............... | ........$()....()$...... \\\n$.\\setminus../$................. | .......$.\\setminus../$..... \\\n$..(\\oplus)...$.............. | .......$...()...$... \\\n\n$()....()$............... | \\\n$.\\setminus../$................. | \\\n$....()...$............... | \\\n$....\\downarrow...$.............. | \\\n$...(\\oplus)...$............. | \\\n\n$(\\oplus)$ - given, observed node \\\n- A path is active, if each triple is active \\\n- Active path = independence is not guaranteed \\\n- Single inactive segment makes the whole path inactive \\\n- No active path = independence \\\n- Only consider path that is formed from assignment nodes, all other (upper) nodes can be ignored \\\n- Every variable in a BN is independent given its parents. And not independent of all of its descendants given its children. \\\n\n\\\n**Probabalistic Inference** \\\n\n$(5)$ Variable Elimination \\\nAdvance technique of interleaves joining & marginalizing \\\n- Usually, much faster than Inference by Enumeration but still NP-Hard \\\n\n\n## Module 11 Definitions\n* **Independence** - two events are independent, if occurence of one doesn't affect prob. of occurence of the other\n* **Bayes' nets** - graphical models that allow us to express CI assumptions explicitely\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3FPMzG9yUls"
      },
      "source": [
        "#Module 12 - Naive Bayes and Perceptrons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEIOpL7vyWcc"
      },
      "source": [
        "# Russell and Norvig, AIMA Chapter 19 “Learning from Examples” (Sections 19.1-19.6)\n",
        "\n",
        "chapter19_1 = \"\"\" Your summary here \"\"\"\n",
        "chapter19_2 = \"\"\" Your summary here \"\"\"\n",
        "chapter19_3 = \"\"\" Your summary here \"\"\"\n",
        "chapter19_4 = \"\"\" Your summary here \"\"\"\n",
        "chapter19_5 = \"\"\" Your summary here \"\"\"\n",
        "chapter19_6 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Speech and Language Processing Chapter 4 “Naive Bayes and Sentiment Classification”\n",
        "slp_chapter4_1 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_2 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_3 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_4 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_5 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_6 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_7 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_8 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_9 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter4_10 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Lecture notes \n",
        "module12_lecture_notes = \"\"\"\n",
        "Machine Learning (ML) - how to acquire model from data/experience \\\\\n",
        "~ Learning params (prob.) \\\\\n",
        "~ Learning structure (BN Graphs) \\\\\n",
        "~ Learning hidden concepts (clustering) \\\\\n",
        "vs before we learned how to use a model to make decisions \\\\\n",
        "\n",
        "\\\\\n",
        "**Model-based Classification** \\\\\n",
        "$(1)$ Build a model (eg. BN), where label & features are random vars \\\\\n",
        "$(2)$ Instantiate any observed feature \\\\\n",
        "$(3)$ Query for distribution of the label, conditioned on the features \\\\\n",
        "$(4)$ Use Inference to give us info about query var \\\\ \n",
        "\n",
        "\\\\\n",
        "**Naive Bayes (NB)** \\\\\n",
        "Model: $P(Y, F_1 ... F_n) = P(Y) \\prod_i P(F_i | Y)$ \\\\\n",
        "\n",
        "$+$ We only specify how each feature depends on the class $Y$ \\\\\n",
        "$+$ Num of params we have to estimate is pretty small (linear in num of features) \\\\\n",
        "$-$ Model is very simplistic \\\\\n",
        "$+$ But empirically it works well anyway \\\\\n",
        "\n",
        "How fast: \\\\\n",
        "$|Y| \\cdot |F|^n$ - general JPD, exponential with respect to num. of features \\\\\n",
        "$n|F| \\cdot |Y|$ - NB model, linear with respect to num. of features \\\\\n",
        "\n",
        "***Inference for NB*** \\\\\n",
        "Goal: compute posterior distribution over label $Y$ \\\\\n",
        "$(1)$ Get Joint prob. of a label and evidence for each label \\\\\n",
        "$(2)$ Sum to get prob. of evidence: $P(f_1 ... f_n)$ \\\\\n",
        "$(3)$ Normilize: \\\\\n",
        "$\\\\frac{P(Y, f_1 ... f_n)}{P(f_1 ... f_n)} = P(Y | f_1 ... f_n)$ \\\\\n",
        "\n",
        "***Bag-of-Words*** - model that is insensitive to word order or reordering \\\\\n",
        "Assumes: \\\\\n",
        "~ each position is identically distributed \\\\\n",
        "~ each position shares the same $P(w | Y)$ \\\\\n",
        "\n",
        "\\\\\n",
        "**Basic Concepts of ML** \\\\\n",
        "\n",
        "***Overfitting*** \\\\\n",
        "$\\implies$ not generalizing well to unseen data \\\\\n",
        "- To generalize better, smooth/regulate the estimates \\\\\n",
        "Hyperparam ~ amount / type of smoothing to do \\\\\n",
        "\n",
        "***Param Estimation Techniques*** \\\\\n",
        "$(1)$ Elicitation - askinng DRI/expert \\\\\n",
        "$(2)$ Empirically - using training data \\\\\n",
        "- but we still have to address unseen events \\\\\n",
        "\n",
        "$(3)$ Laplace Smoothing \\\\\n",
        "$-$ performs poorly on $P(X | Y)$ when $|X|$ or $|Y|$ are very large \\\\\n",
        "\n",
        "$(4)$ Linear Interpolation - weighting cond. prob. vs prior prob. \\\\\n",
        "$P_{LIN}(x | y) = \\\\alpha \\\\overline{P}(x | y) + (1 - \\\\alpha)\\\\overline{P}(x)$ \\\\\n",
        "\n",
        "$(5)$ Adding Feature \\\\\n",
        "\n",
        "$(6)$ Baseline - picking super simple baseline \\\\\n",
        "- helps to determine how hard the task is \\\\\n",
        "- helps to know what \"good\" accuracy is \\\\\n",
        "- in research, previous work usually serves as a strong baseline \\\\\n",
        "\n",
        "***Confidence and Calibration*** \\\\\n",
        "condifence(x) = $max_y P(y | x)$ \\\\\n",
        "- important caveat, there is no guarantee that confidence is correct \\\\\n",
        "\n",
        "Weak calibration: High Confidence = High Accuracy \\\\\n",
        "Strong calibration: confidence predicts Accuracy rate \\\\\n",
        "\n",
        "\\\\\n",
        "**Perceptron** \\\\\n",
        "Perceptron - doesn't deal with prob., instead it consists of a set of weights, input values, and a threshold (bias). \\\\\n",
        "~ Decision-making device \\\\\n",
        "\n",
        "$ (x_1) - (w_1) . \\setminus ................$ activation func. \\\\\n",
        "$ (x_2) - (w_2) - (\\\\sum w_i \\\\cdot x_i) - (\\\\phi) -$ output \\\\\n",
        "$ (x_n) - (w_n) . /$ \\\\\n",
        "The Inference procedure for Perceptron: compute a weighted sum, then apply a threshold. \\\\\n",
        "\n",
        "- Feature values are fixed, but we can choose diff. w vectors \\\\\n",
        "- Depending on the w vector we pick, we get diff. classifier \\\\\n",
        "$2D$ - line \\\\\n",
        "$3D$ - plane \\\\\n",
        "$4D+$ - separating hyperplane \\\\\n",
        "- Bias - allows us to move decision boundary around as it helps to compare with 0 \\\\\n",
        "- Error-driven learning - allows us to learn & adjust the weights \\\\\n",
        "\n",
        "***Binary Perceptron*** \\\\\n",
        "Start with $\\overline{w} = 0$ \\\\\n",
        "For each example: \\\\\n",
        "$o$ Classify with curr weights (identify $y$ as 0 or 1) \\\\\n",
        "$o$ If correct, no change \\\\\n",
        "$o$ Else, adjust $\\overline{w}$ \\\\\n",
        "\n",
        "***Multiclass Perceptron*** \\\\\n",
        "$o$ Take a $w$ vector for each label $w_y$ \\\\\n",
        "$o$ Score activation of a label: $w_y \\cdot f(x)$ \\\\\n",
        "$o$ The highest score prediction wins: $y = argmax_y w_y \\cdot f(x) $ \\\\\n",
        "$o$ If correct, no changes \\\\\n",
        "$o$ Else: \\\\\n",
        "$oo$ Raise the score of the right answer \\\\\n",
        "$oo$ Lower the score of the wrong one \\\\\n",
        "\n",
        "***Properties of Perceptron*** \\\\\n",
        "- Separability \\\\\n",
        "True, if some params get the training set perfectly correct \\\\\n",
        "- Convergence \\\\\n",
        "If the trainninng set is separable, perceptron will eventually converge \\\\\n",
        "- Mistake Bound \\\\\n",
        "The max num of mistakes, related to margin or degree of separability \\\\\n",
        "$mistakes < \\\\frac{k}{\\\\sigma^2}$, where \\\\\n",
        "$k$ - some constant \\\\\n",
        "$\\sigma$ - margin \\\\\n",
        "if space between points is large, we will have a small number of mistakes \\\\\n",
        "\n",
        "In 2-dimensional data distribution where points belonging to class A are arranged around the origin in a circle with radius $r_a$, \\\\\n",
        "and points belonging to class B are arranged around the origin in a circle with radius $r_b$, where $r_a < r_b$. \\\\\n",
        "Perceptron algorithm applied to this classification problem can separate the two classes if we use feature augmentation (remember Tensorflow). \\\\\n",
        "\n",
        "***Problems with Perceptron:*** \\\\\n",
        "$-$ Noise \\\\\n",
        "If the data is not separatable, weights might trash (go back and forth) \\\\\n",
        "$\\implies$ Averaging $w$ vector over time can help. \\\\\n",
        "$-$ Mediocre Generalization \\\\\n",
        "Finds a barely separable solution. \\\\\n",
        "$-$ Overtraining \\\\\n",
        "Test/held-out accuracy usually rises, then falls. That can lead to overfitting. \\\\\n",
        "Easy to fix though: Stop iterating as soon as tests on held-out start declining. \\\\\n",
        "\n",
        "\\\\\n",
        "**Fixing the Perceptron** \\\\\n",
        "\n",
        "***MIRA*** \\\\\n",
        "MIRA - Margin-Infused Relaxed Algorithm \\\\\n",
        "Idea: \\\\\n",
        "- Adjust the $w$ updates to mitigate Perceptron problems \\\\\n",
        "- Choose an update size that fixes curr. mistake, but minimizes changes to $w$ \\\\\n",
        "\n",
        "$\\\\tau = \\\\frac{(w'y - w'y^*) \\\\cdot f + 1}{2f \\\\cdot f}$, where \\\\\n",
        "$+1$ - helps to generalize\\\\\n",
        "$y$ - guessed\\\\\n",
        "$y^*$ - truth\\\\\n",
        "$w_y = w'y - \\\\tau f(x)$ \\\\\n",
        "$wy^* = w'y^* + \\\\tau f(x)$ \\\\\n",
        "$\\\\tau$ - helps to amplify $f(x)$, minimally affecting $w$. With $\\\\tau, w'$ will be closer to initial $w_y$  \\\\\n",
        "\n",
        "Downsides: \\\\\n",
        "$-$ But even with $\\tau$ sometimes it could update a little too much \\\\\n",
        "$-$ Examples maybe labeled incorrectly \\\\\n",
        "$-$ We might not have enough features \\\\\n",
        "\n",
        "\\\\\n",
        "***Max step size*** \\\\\n",
        "Max step size - cap the max possible value of $\\tau$ with some constant $C$ \\\\\n",
        "$\\\\tau^* = min (\\\\frac{(w'y - w'y^*) \\\\cdot f + 1}{2f \\\\cdot f}, C)$ such that $\\\\frac{()}{()}$ or $C$ \\\\\n",
        "- Helps a lot when we have non-separatable data \\\\\n",
        "- Usually converges faster than Perceptron \\\\\n",
        "- Is usually better, especially on the noisy data \\\\\n",
        "\n",
        "\\\\\n",
        "***Support Vector Machine*** \\\\\n",
        "Support Vector Machine - helps to choose the best Linear Separator when there are a few of them \\\\\n",
        "- Draws 2 additional support vectors & maximizes the margin between them \\\\\n",
        "- Only support vectors that matter, ignoring others \\\\\n",
        "- Finds the separator with max margins \\\\\n",
        "~ Similar to MIRA, where we also optimize over all examples at once \\\\\n",
        "\n",
        "~ Non-linear-separatable data in $2D$? Take it to $3D+$ & find an angle! \\\\\n",
        "\n",
        "\\\\\n",
        "**Comparing Classification Approaches** \\\\\n",
        "\n",
        "***NB:*** \\\\\n",
        "- Builds a model training data \\\\\n",
        "- Gives prediction prob. \\\\\n",
        "- Strong assumptions about feature independence \\\\\n",
        "- One pass through the data (counting) \\\\\n",
        "\n",
        "***Percepton / MIRA:*** \\\\\n",
        "- Makes less assumptions about the data \\\\\n",
        "- Mistake-driven learning \\\\\n",
        "- Multiple passes through the data (prediction) \\\\\n",
        "- Often more accurate \\\\\n",
        "\"\"\"\n",
        "\n",
        "# Definitions\n",
        "module12_definitions = {\n",
        "\"Naive Bayes\" : \"makes strong (naive) Independence assumptions between features - assumes all features are independent from each other and only depend on a label\",\n",
        "\"Overfitting\" : \"modeling the training data too closely\",   \n",
        "\"Confidence\" : \"represents how sure the classifier is of its classification\",\n",
        "\"Calibration\" : \"comparison between the standard and measurement given by the instrument\",\n",
        "\"Perceptron\" : \"consists of a set of weights, input values, and a threshold (bias)\",\n",
        "\"Bias\" : \"allows us to move decision boundary around as it helps to compare with 0\",\n",
        "\"\" : \"\"\n",
        "}\n",
        "\n",
        "# You can include explanations for figures in the textbook (except figures with pseduocode).\n",
        "# The figure will appear in your notes along with your explanation.\n",
        "# We don't currently have figures for SLP.\n",
        "module12_figure_explanations = {\n",
        "\"\" : \"\"\n",
        "}\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUc_LgzDvWf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "e5286b0e-6a64-49b5-d1c5-5cd39eab84ec"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "module12_sections = [\n",
        "  (\"AIMA Chapter 5.1 Game Theory\", chapter19_1),\n",
        "  (\"AIMA Chapter 5.2 Optimal Decisions in Games\", chapter19_2),\n",
        "  (\"AIMA Chapter 5.3 Heuristic Alpha–Beta Tree Search\", chapter19_3),\n",
        "  (\"AIMA Chapter 5.5 Stochastic Games\", chapter19_4),\n",
        "  (\"AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\", chapter19_5),\n",
        "  (\"AIMA Chapter 16.2 The Basis of Utility Theory\", chapter19_6),\n",
        "  (\"SLP Chapter 4.1 Naive Bayes Classifiers\", slp_chapter4_1),\n",
        "  (\"SLP Chapter 4.2 Training the Naive Bayes Classifier\", slp_chapter4_2),\n",
        "  (\"SLP Chapter 4.3 Worked Example\", slp_chapter4_3),\n",
        "  (\"SLP Chapter 4.4 Optimizing for Sentiment Analysis\", slp_chapter4_4),\n",
        "  (\"SLP Chapter 4.5 Naive Bayes for other text classification tasks\", slp_chapter4_5),\n",
        "  (\"SLP Chapter 4.6 Naive Bayes as a Language Model\", slp_chapter4_6),\n",
        "  (\"SLP Chapter 4.7 Evaluation: Precision, Recall, F-measure\", slp_chapter4_7),\n",
        "  (\"SLP Chapter 4.8 Test sets and Cross-validation\", slp_chapter4_8),\n",
        "  (\"SLP Chapter 4.9 Statistical Significance Testing\", slp_chapter4_9),\n",
        "  (\"SLP Chapter 4.10 Avoiding Harms in Classification\", slp_chapter4_10),\n",
        "  (\"Module 12 Lecture Notes\", module12_lecture_notes),\n",
        "]\n",
        "\n",
        "module12_markdown = format_module(12, \"Naive Bayes and Perceptrons\", \n",
        "                         module12_sections, \n",
        "                         module12_definitions,\n",
        "                         module12_figure_explanations)\n",
        "display(Markdown(module12_markdown))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 12: Naive Bayes and Perceptrons\n## AIMA Chapter 5.1 Game Theory\n Your summary here \n\n## AIMA Chapter 5.2 Optimal Decisions in Games\n Your summary here \n\n## AIMA Chapter 5.3 Heuristic Alpha–Beta Tree Search\n Your summary here \n\n## AIMA Chapter 5.5 Stochastic Games\n Your summary here \n\n## AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\n Your summary here \n\n## AIMA Chapter 16.2 The Basis of Utility Theory\n Your summary here \n\n## SLP Chapter 4.1 Naive Bayes Classifiers\n Your summary here \n\n## SLP Chapter 4.2 Training the Naive Bayes Classifier\n Your summary here \n\n## SLP Chapter 4.3 Worked Example\n Your summary here \n\n## SLP Chapter 4.4 Optimizing for Sentiment Analysis\n Your summary here \n\n## SLP Chapter 4.5 Naive Bayes for other text classification tasks\n Your summary here \n\n## SLP Chapter 4.6 Naive Bayes as a Language Model\n Your summary here \n\n## SLP Chapter 4.7 Evaluation: Precision, Recall, F-measure\n Your summary here \n\n## SLP Chapter 4.8 Test sets and Cross-validation\n Your summary here \n\n## SLP Chapter 4.9 Statistical Significance Testing\n Your summary here \n\n## SLP Chapter 4.10 Avoiding Harms in Classification\n Your summary here \n\n## Module 12 Lecture Notes\n\nMachine Learning (ML) - how to acquire model from data/experience \\\n~ Learning params (prob.) \\\n~ Learning structure (BN Graphs) \\\n~ Learning hidden concepts (clustering) \\\nvs before we learned how to use a model to make decisions \\\n\n\\\n**Model-based Classification** \\\n$(1)$ Build a model (eg. BN), where label & features are random vars \\\n$(2)$ Instantiate any observed feature \\\n$(3)$ Query for distribution of the label, conditioned on the features \\\n$(4)$ Use Inference to give us info about query var \\ \n\n\\\n**Naive Bayes (NB)** \\\nModel: $P(Y, F_1 ... F_n) = P(Y) \\prod_i P(F_i | Y)$ \\\n\n$+$ We only specify how each feature depends on the class $Y$ \\\n$+$ Num of params we have to estimate is pretty small (linear in num of features) \\\n$-$ Model is very simplistic \\\n$+$ But empirically it works well anyway \\\n\nHow fast: \\\n$|Y| \\cdot |F|^n$ - general JPD, exponential with respect to num. of features \\\n$n|F| \\cdot |Y|$ - NB model, linear with respect to num. of features \\\n\n***Inference for NB*** \\\nGoal: compute posterior distribution over label $Y$ \\\n$(1)$ Get Joint prob. of a label and evidence for each label \\\n$(2)$ Sum to get prob. of evidence: $P(f_1 ... f_n)$ \\\n$(3)$ Normilize: \\\n$\\frac{P(Y, f_1 ... f_n)}{P(f_1 ... f_n)} = P(Y | f_1 ... f_n)$ \\\n\n***Bag-of-Words*** - model that is insensitive to word order or reordering \\\nAssumes: \\\n~ each position is identically distributed \\\n~ each position shares the same $P(w | Y)$ \\\n\n\\\n**Basic Concepts of ML** \\\n\n***Overfitting*** \\\n$\\implies$ not generalizing well to unseen data \\\n- To generalize better, smooth/regulate the estimates \\\nHyperparam ~ amount / type of smoothing to do \\\n\n***Param Estimation Techniques*** \\\n$(1)$ Elicitation - askinng DRI/expert \\\n$(2)$ Empirically - using training data \\\n- but we still have to address unseen events \\\n\n$(3)$ Laplace Smoothing \\\n$-$ performs poorly on $P(X | Y)$ when $|X|$ or $|Y|$ are very large \\\n\n$(4)$ Linear Interpolation - weighting cond. prob. vs prior prob. \\\n$P_{LIN}(x | y) = \\alpha \\overline{P}(x | y) + (1 - \\alpha)\\overline{P}(x)$ \\\n\n$(5)$ Adding Feature \\\n\n$(6)$ Baseline - picking super simple baseline \\\n- helps to determine how hard the task is \\\n- helps to know what \"good\" accuracy is \\\n- in research, previous work usually serves as a strong baseline \\\n\n***Confidence and Calibration*** \\\ncondifence(x) = $max_y P(y | x)$ \\\n- important caveat, there is no guarantee that confidence is correct \\\n\nWeak calibration: High Confidence = High Accuracy \\\nStrong calibration: confidence predicts Accuracy rate \\\n\n\\\n**Perceptron** \\\nPerceptron - doesn't deal with prob., instead it consists of a set of weights, input values, and a threshold (bias). \\\n~ Decision-making device \\\n\n$ (x_1) - (w_1) . \\setminus ................$ activation func. \\\n$ (x_2) - (w_2) - (\\sum w_i \\cdot x_i) - (\\phi) -$ output \\\n$ (x_n) - (w_n) . /$ \\\nThe Inference procedure for Perceptron: compute a weighted sum, then apply a threshold. \\\n\n- Feature values are fixed, but we can choose diff. w vectors \\\n- Depending on the w vector we pick, we get diff. classifier \\\n$2D$ - line \\\n$3D$ - plane \\\n$4D+$ - separating hyperplane \\\n- Bias - allows us to move decision boundary around as it helps to compare with 0 \\\n- Error-driven learning - allows us to learn & adjust the weights \\\n\n***Binary Perceptron*** \\\nStart with $\\overline{w} = 0$ \\\nFor each example: \\\n$o$ Classify with curr weights (identify $y$ as 0 or 1) \\\n$o$ If correct, no change \\\n$o$ Else, adjust $\\overline{w}$ \\\n\n***Multiclass Perceptron*** \\\n$o$ Take a $w$ vector for each label $w_y$ \\\n$o$ Score activation of a label: $w_y \\cdot f(x)$ \\\n$o$ The highest score prediction wins: $y = argmax_y w_y \\cdot f(x) $ \\\n$o$ If correct, no changes \\\n$o$ Else: \\\n$oo$ Raise the score of the right answer \\\n$oo$ Lower the score of the wrong one \\\n\n***Properties of Perceptron*** \\\n- Separability \\\nTrue, if some params get the training set perfectly correct \\\n- Convergence \\\nIf the trainninng set is separable, perceptron will eventually converge \\\n- Mistake Bound \\\nThe max num of mistakes, related to margin or degree of separability \\\n$mistakes < \\frac{k}{\\sigma^2}$, where \\\n$k$ - some constant \\\n$\\sigma$ - margin \\\nif space between points is large, we will have a small number of mistakes \\\n\nIn 2-dimensional data distribution where points belonging to class A are arranged around the origin in a circle with radius $r_a$, \\\nand points belonging to class B are arranged around the origin in a circle with radius $r_b$, where $r_a < r_b$. \\\nPerceptron algorithm applied to this classification problem can separate the two classes if we use feature augmentation (remember Tensorflow). \\\n\n***Problems with Perceptron:*** \\\n$-$ Noise \\\nIf the data is not separatable, weights might trash (go back and forth) \\\n$\\implies$ Averaging $w$ vector over time can help. \\\n$-$ Mediocre Generalization \\\nFinds a barely separable solution. \\\n$-$ Overtraining \\\nTest/held-out accuracy usually rises, then falls. That can lead to overfitting. \\\nEasy to fix though: Stop iterating as soon as tests on held-out start declining. \\\n\n\\\n**Fixing the Perceptron** \\\n\n***MIRA*** \\\nMIRA - Margin-Infused Relaxed Algorithm \\\nIdea: \\\n- Adjust the $w$ updates to mitigate Perceptron problems \\\n- Choose an update size that fixes curr. mistake, but minimizes changes to $w$ \\\n\n$\\tau = \\frac{(w'y - w'y^*) \\cdot f + 1}{2f \\cdot f}$, where \\\n$+1$ - helps to generalize\\\n$y$ - guessed\\\n$y^*$ - truth\\\n$w_y = w'y - \\tau f(x)$ \\\n$wy^* = w'y^* + \\tau f(x)$ \\\n$\\tau$ - helps to amplify $f(x)$, minimally affecting $w$. With $\\tau, w'$ will be closer to initial $w_y$  \\\n\nDownsides: \\\n$-$ But even with $\tau$ sometimes it could update a little too much \\\n$-$ Examples maybe labeled incorrectly \\\n$-$ We might not have enough features \\\n\n\\\n***Max step size*** \\\nMax step size - cap the max possible value of $\tau$ with some constant $C$ \\\n$\\tau^* = min (\\frac{(w'y - w'y^*) \\cdot f + 1}{2f \\cdot f}, C)$ such that $\\frac{()}{()}$ or $C$ \\\n- Helps a lot when we have non-separatable data \\\n- Usually converges faster than Perceptron \\\n- Is usually better, especially on the noisy data \\\n\n\\\n***Support Vector Machine*** \\\nSupport Vector Machine - helps to choose the best Linear Separator when there are a few of them \\\n- Draws 2 additional support vectors & maximizes the margin between them \\\n- Only support vectors that matter, ignoring others \\\n- Finds the separator with max margins \\\n~ Similar to MIRA, where we also optimize over all examples at once \\\n\n~ Non-linear-separatable data in $2D$? Take it to $3D+$ & find an angle! \\\n\n\\\n**Comparing Classification Approaches** \\\n\n***NB:*** \\\n- Builds a model training data \\\n- Gives prediction prob. \\\n- Strong assumptions about feature independence \\\n- One pass through the data (counting) \\\n\n***Percepton / MIRA:*** \\\n- Makes less assumptions about the data \\\n- Mistake-driven learning \\\n- Multiple passes through the data (prediction) \\\n- Often more accurate \\\n\n\n## Module 12 Definitions\n* **Naive bayes** - makes strong (naive) Independence assumptions between features - assumes all features are independent from each other and only depend on a label\n* **Overfitting** - modeling the training data too closely\n* **Confidence** - represents how sure the classifier is of its classification\n* **Calibration** - comparison between the standard and measurement given by the instrument\n* **Perceptron** - consists of a set of weights, input values, and a threshold (bias)\n* **Bias** - allows us to move decision boundary around as it helps to compare with 0\n* **** - \n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVSQhTVSyYYO"
      },
      "source": [
        "# Module 13 - Neural Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm4mfW82yaRH"
      },
      "source": [
        "# Jurafsky and Martin, Speech and Language Processing Chapter 5 “Logistic Regression” (5.1-5.6)\n",
        "\n",
        "slp_chapter5_1 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter5_2 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter5_3 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter5_4 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter5_5 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter5_6 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Jurafsky and Martin, Chapter 7 “Neural Networks and Neural Language Models”\n",
        "\n",
        "slp_chapter7_1 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_2 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_3 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_4 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_5 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_6 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter7_7 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Lecture notes \n",
        "module13_lecture_notes = \"\"\"\n",
        "**Classifier Components** \\\\\n",
        "ML Classifiers require a training corpus of $m$ observations input/output pairs $(x^i, y^i)$ \\\\\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Definitions\n",
        "module13_definitions = {\n",
        "\"Logistic Regression (LR)\" : \"(another type of classifier) estimaties the params of logistic model\", \n",
        "\"Logistic Model\" : \"models the prob. of the event, using log-odds of the event\"   \n",
        "}\n",
        "\n",
        "# We don't currently have figures for SLP.\n",
        "module13_figure_explanations = {\n",
        "\"\" : \"\",\n",
        "}\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKWbCsbfxL3R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "cellView": "form",
        "outputId": "6a1e5853-97f3-421d-9659-19dc50b1fcf0"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "module13_sections = [\n",
        "  (\"SLP Chapter 5.1 Classification: the sigmoid\", slp_chapter5_1),\n",
        "  (\"SLP Chapter 5.2 Learning in Logistic Regression\", slp_chapter5_2),\n",
        "  (\"SLP Chapter 5.3 The cross-entropy loss function\", slp_chapter5_3),\n",
        "  (\"SLP Chapter 5.4 Gradient Descent\", slp_chapter5_4),\n",
        "  (\"SLP Chapter 5.5 Regularization\", slp_chapter5_5),\n",
        "  (\"SLP Chapter 5.6 Multinomial logistic regression\", slp_chapter5_6),\n",
        "\n",
        "  (\"SLP Chapter 7.1 Units\", slp_chapter7_1),\n",
        "  (\"SLP Chapter 7.2 The XOR problem\", slp_chapter7_2),\n",
        "  (\"SLP Chapter 7.3 Feedforward Neural Networks\", slp_chapter7_3),\n",
        "  (\"SLP Chapter 7.4 Feedforward networks for NLP: Classification\", slp_chapter7_4),\n",
        "  (\"SLP Chapter 7.5 Feedforward Neural Language Modeling\", slp_chapter7_5),\n",
        "  (\"SLP Chapter 7.6 Training Neural Nets\", slp_chapter7_6),\n",
        "  (\"SLP Chapter 7.7 Training the neural language model\", slp_chapter7_7),\n",
        "  (\"Module 13 Lecture Notes\", module13_lecture_notes),\n",
        "]\n",
        "\n",
        "module13_markdown = format_module(13, \"Neural Networks\", \n",
        "                         module13_sections, \n",
        "                         module13_definitions,\n",
        "                         module13_figure_explanations)\n",
        "display(Markdown(module13_markdown))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 13: Neural Networks\n## SLP Chapter 5.1 Classification: the sigmoid\n Your summary here \n\n## SLP Chapter 5.2 Learning in Logistic Regression\n Your summary here \n\n## SLP Chapter 5.3 The cross-entropy loss function\n Your summary here \n\n## SLP Chapter 5.4 Gradient Descent\n Your summary here \n\n## SLP Chapter 5.5 Regularization\n Your summary here \n\n## SLP Chapter 5.6 Multinomial logistic regression\n Your summary here \n\n## SLP Chapter 7.1 Units\n Your summary here \n\n## SLP Chapter 7.2 The XOR problem\n Your summary here \n\n## SLP Chapter 7.3 Feedforward Neural Networks\n Your summary here \n\n## SLP Chapter 7.4 Feedforward networks for NLP: Classification\n Your summary here \n\n## SLP Chapter 7.5 Feedforward Neural Language Modeling\n Your summary here \n\n## SLP Chapter 7.6 Training Neural Nets\n Your summary here \n\n## SLP Chapter 7.7 Training the neural language model\n Your summary here \n\n## Module 13 Lecture Notes\n\n**Classifier Components** \\\nML Classifiers require a training corpus of $m$ observations input/output pairs $(x^i, y^i)$ \\\n\n\n\n## Module 13 Definitions\n* **Logistic regression (lr)** - (another type of classifier) estimaties the params of logistic model\n* **Logistic model** - models the prob. of the event, using log-odds of the event\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYESi2kMybL8"
      },
      "source": [
        "# Module 14 - Natural Language Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIRJ4YYCzNHL"
      },
      "source": [
        "# Jurafsky and Martin, Chapter 9 “Sequence Processing with Recurrent Networks” (Sections 9.1-9.3)\n",
        "\n",
        "slp_chapter9_1 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter9_2 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter9_3 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Jurafsky and Martin, Chapter 6 “Vector Semantics and Embeddings” (6.1-6.4, 6.8, 6.10-6.12)\n",
        "\n",
        "slp_chapter6_1 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_2 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_3 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_4 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_8 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_10 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_11 = \"\"\" Your summary here \"\"\"\n",
        "slp_chapter6_12 = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# Lecture notes \n",
        "module14_lecture_notes = \"\"\" Your summary here \"\"\"\n",
        "\n",
        "# Definitions\n",
        "module14_definitions = {\n",
        "\"concept\" : \"definition\",    \n",
        "}\n",
        "\n",
        "\n",
        "# We don't currently have figures for SLP.\n",
        "module14_figure_explanations = {\n",
        "\"\" : \"\",\n",
        "}"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXNcHuKjyNXa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "outputId": "3f53b49e-4db2-4f78-9378-33d61878af6e",
        "cellView": "form"
      },
      "source": [
        "#@title Preview of your summary\n",
        "#@markdown You don't need to modify anything in this cell. Just press play.\n",
        "module14_sections = [\n",
        "  (\"SLP Chapter 9.1 Language Models Revisited\", slp_chapter9_1),\n",
        "  (\"SLP Chapter 9.2 Recurrent Neural Networks\", slp_chapter9_2),\n",
        "  (\"SLP Chapter 9.3 RNNs as Language Models\", slp_chapter9_3),\n",
        "\n",
        "  (\"SLP Chapter 6.1 Lexical Semantics\", slp_chapter6_1),\n",
        "  (\"SLP Chapter 6.2 Vector Semantics\", slp_chapter6_2),\n",
        "  (\"SLP Chapter 6.3 Words and Vectors\", slp_chapter6_3),\n",
        "  (\"SLP Chapter 6.4 Cosine for measuring similarity\", slp_chapter6_4),\n",
        "  (\"SLP Chapter 6.8 Word2vec\", slp_chapter6_8),\n",
        "  (\"SLP Chapter 6.10 Semantic properties of embeddings\", slp_chapter6_10),\n",
        "  (\"SLP Chapter 6.11 Bias and Embeddings\", slp_chapter6_11),\n",
        "  (\"SLP Chapter 6.12 Evaluating Vector Models\", slp_chapter6_12),\n",
        "  (\"Module 13 Lecture Notes\", module13_lecture_notes),\n",
        "]\n",
        "\n",
        "module14_markdown = format_module(14, \"Natural Language Processing\", \n",
        "                         module14_sections, \n",
        "                         module14_definitions,\n",
        "                         module14_figure_explanations)\n",
        "display(Markdown(module14_markdown))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 14: Natural Language Processing\n## SLP Chapter 9.1 Language Models Revisited\n Your summary here \n\n## SLP Chapter 9.2 Recurrent Neural Networks\n Your summary here \n\n## SLP Chapter 9.3 RNNs as Language Models\n Your summary here \n\n## SLP Chapter 6.1 Lexical Semantics\n Your summary here \n\n## SLP Chapter 6.2 Vector Semantics\n Your summary here \n\n## SLP Chapter 6.3 Words and Vectors\n Your summary here \n\n## SLP Chapter 6.4 Cosine for measuring similarity\n Your summary here \n\n## SLP Chapter 6.8 Word2vec\n Your summary here \n\n## SLP Chapter 6.10 Semantic properties of embeddings\n Your summary here \n\n## SLP Chapter 6.11 Bias and Embeddings\n Your summary here \n\n## SLP Chapter 6.12 Evaluating Vector Models\n Your summary here \n\n## Module 13 Lecture Notes\n\n**Classifier Components** \\\nML Classifiers require a training corpus of $m$ observations input/output pairs $(x^i, y^i)$ \\\n\n\n\n## Module 14 Definitions\n* **Concept** - definition\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX7lbnJd4sT5"
      },
      "source": [
        "# Your Exam Notes\n",
        "\n",
        "Here are your full set of exam notes that you can use on your midterm.  Choose **Runtime > Run all**, then your notes will be displayed.\n",
        "\n",
        "If you are taking the exam in person, then you must print a copy of your notes for yourself.   Here's how to print:\n",
        "* Run the cell below. \n",
        "* Click its \"Mirror cell in tab\" button (this is located in the upper right corner of the cell, immediately to the right of the trash can).  This will open the cell.  \n",
        "* In Chrome, you can then print your notes using the **File > Print** option from the browser's menu.  \n",
        "* In the print options, change the layout from Landscape to Portrait. \n",
        "* Clicking the Save, will save a PDF that you can then print.\n",
        "* If you're having trouble printing directly from Colab, try copying and pasting the formatted output (below) into a Google doc and printing from there. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "T3cm1WV542_a",
        "outputId": "13b7b50f-e4df-48d7-b6f9-d3c96f733ef5"
      },
      "source": [
        "# This will generate the full set of notes that you can use for your exam.\n",
        "full_markdown = \"\\n\\n\".join([module8_markdown, \n",
        "                             module9_markdown, \n",
        "                             module10_markdown, \n",
        "                             module11_markdown, \n",
        "                             module12_markdown,\n",
        "                             module13_markdown,\n",
        "                             module14_markdown,]\n",
        "                       )\n",
        "display(Markdown(full_markdown))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Module 8: Markov Decision Processes\n## AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\n\nEstimating the prob. distribution $P(s)$ over possible states requires: \\\n- perception \\\n- learning \\\n- knowledge representation \\\n- inference \\\n\nDecision Theory is not a panacea, but provides enough to define AI problem. \\\n\nExpected Utility formula: \\\n$EU(a) = \\sum_{s'} P(Result(a)=s') U(s')$\n\nTransition func:\n $P(\\textnormal{Result}(a)=s') = \\sum_s P(s)P(s'|s,a)$ \n\n## AIMA Chapter 16.2 The Basis of Utility Theory\n \nUtility Theory's goal is to understand how preferences between complex lotteries are related to preferences between underlying states. \\\n\nAxioms lead to: \\\n- Existence of Util. func. - if agent obey the axioms then there exist $U$ such that: \\\n$U(A) > U(B) \\iff A \\succ B$ and $U(A) = U(B) \\iff A \\sim B$ \\\n- EU of Lottery - the sum of the prob. of each outcome times util. of that outcome: \\\n$U([p_1, S_1 ... p_n, S_n]) = \\sum_i p_i U(S_i)$ \\\n\nUtil. functions are not necesseraly unique \\\n\nUtil. resembles temperatures, converting from one or another doesn't make you hotter or colder ($F \\iff C$)\n\n\n## AIMA Chapter 16.3 Utility Functions\n  \nUtility func. map from lotteries to real numbers. \\\nThey must obey the axioms of rationality. \n\n\\\nEU and possible post-decision disappointment \\\nThe real outcome (of model of lottery) will usually be significantly worse than we estimated, even if the estimate was unbiased. \\\nBecause we select the action with highest util. estimate, we are favoring the overly optimistic estimates, and that's the source of bias. \\\n\n\n## AIMA Chapter 17.1 Sequential Decision Problems\n\n$T$ are Markovian - the prob of reaching $s'$ from $s$ depends only on $s$ and not on the history of earlier states. \\\n\n\\\nEnvironment History - seq. of states and actions \\\nRewards may be positive or negative, but they are bounded by +/-$R_{max}$ \\\nUtility of Environment History - sum of rewards received \\\n\n\\\nPolicy represents the agent function explicitly. Therefore, a description of simple reflex agent computed from the information used for a utility-based agent. \\\n\n\\\nAgent's preferences between state sequences are ***stationary*** \\\nIf agent prefers one future to another starting tomorrow, then it will still prefer that future if it starts today. \\\n\nOptimal policy is ***independent*** of the starting state - that's a remarkable consequence of using discounted utilities with infinite horizons.\n\n\n## AIMA Chapter 17.2 Algorithms for MDPs\n \nDiscounted reward is not absolutely necessary. \\\nProper policy might not always exist. \\\n\n\\\nValue Iteration converges due to contraction. \\\nContraction - a func. of one argument that, when applied to two different inputs in turn, produces two outputs that are 'closer together' by at least some constant factor, than the original inputs. \\\nExample: 'Divide by two' is a contraction \\\n- Contraction has only one fixed point \\\n- When func. applied, the value must get closer to fixed point, so repeated application of contraction always reaches the fixed point \\\n\nValue Iteration converges to the correct utility. \\\nWe can bound the errorr in utility, if we stop after a finite number of iterations, and we can bound the policy loss that results from executing corresponding MEU policy. PS. true for $\\gamma < 1$ \\\n\n\\\nLook for details on VI, PI, etc. in the Lecture Notes. \\\n\nAsynchronous Policy Iteration given certain conditions on the initial policy and util. func. is guaranteed to converge to an optimal policy. \\\n\n\n## Module 8 Lecture Notes\n\nStochastic Transition Model  \\\n$T(s, a, s')$ or $P(s'| s, a)$ - only considers the current state (ignoring all the states we took so far to get to this state) => makes this definition $Markovian$ \\\n\n\\\n**Reward/utility function** \\\n$R(s, a, s')$ - seqential, so we have to specify utility/reward func. on a seq. of states and  actions. \\\nThe agent can receive a reward at each time  step, based on its transition $s -> s'$ via $a$  \n\nRewards are usually additive \\\nReward func. shapes the optimal policy (Millenium Falcon example): \\\n- $r > 0$ - incentivies us to keep going through the loop w/o taking any risk of crushing / exiting \\\n- $-0.03 < r < 0$ - incentivies conservative behaviour & avoiding the risk of exiting \\\n- $-0.04$ - gives optimality \\\n- $r < -1.65$ - makes it easier to exit via -1 \\\n\n\n\\\n**MDP Definition:** \\\n- set of states \\\n- set of actions \\\n- transition func. \\\n- reward func. \\\n- initial state \\\n- Terminal state(s) \\\n\nAs MDP is non-deterministic, we can't give just a seq. of actions as a solution. \\\nInstead, solution to MDP is a policy. \\\nExpectimax is one of the way to solve MDPs. \\\n\nWe need to compute the *Expected Utility* of all possible paths, generated by policy. \\\n\nCombating the danger of game running forever: \\\n$(1)$ Finite horizons (depth-limited search) - terminate after fixed num of steps or give non-stationary policies (that depends on time left) \\\n$(2)$ Absorbing State - guaranties that in every policy, a terrminal state will eventually be reached \\\n$(3)$ Discounting \\\n\n\n\\\nDiscounting \\\nReasonable agents: (1) max for sum of rewards, (2) prefer rewards now, not later (values of rewards decay exponentially) \\\nEach time we decay a level, we power the discount \\\n$U([1, 2, 3]) = 0.5^0 * 1 + 0.5^1 * 2 + 0.5^2 * 3 = 1 + 1 + 0.75 = 2.75$. \\\n+ It helps us to converge the alg. \\\n+ Helps to truncate som bottom parts of Tree Search due to its insignificance \\\n\n\n\\\n**Value of states** \\\n$V^*(s)$ - expected utility, starting in $s$ and acting optimally \\\n$Q^*(s, a)$ - value of q-state(s, a), expected utility of taking $a$ from $s$ and acting optimally (thereafter) \\\n$\\pi^*(s)$ - optimal policy, optimal action from $s$\n$*$ - points to optimality \\\n\nFormulas: \\\n$V^*(s) = max_a Q^*(s, a)$ - choose the best action $a$ and take a max $Q$ value \\\n$Q^*(s, a) = \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - computes value of q-state via Expectimax \\\n$V^*(s) = max_a \\sum_{}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ - ***Bellman equation*** - full merged formula of the expected utility \\\n\n\\\nTo prevent the Search Tree getting very big very fast:\n- we use $\\gamma$ - nodes far from the root have tiny effects -> do Depth-limited search, with increasing $d$, until changes are small \\\n- as States will be repeated -> we only compute needed quantities once \\\n\n\\\n**Value Iteration Algorithm** \\\n- Starts with $V_0(s) = 0$ - game over, expected reward sum is 0 \\\n- Given vector of $V_k(s)$ values, do one ply of Expectimax from each state: \\\n$V_{k+1}(s) = max_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V_k(s')]$ \\\n- Repeat until convergence \\\n$O(s^2 * a)$ - complexity of EACH iteration ($s$ num of states, $a$ num of actions) \\\n- Updates the policy **implicitly**\n\n```\ndef iterate(self):\n        # Runs single value iteration using Bellman equation: V_{k+1}(s) = max_a Q*(s,a)\n        # Then updates values: V*(s) = V_{k+1}(s)\n        temp = dict()\n        for state in self.game.states:\n            action = self.get_best_policy(state)\n            value_next = self.get_q_value(state, action)\n            temp[state] = value_next\n        for state in self.game.states:\n            new_value = temp[state]\n            self.state_values[state] = new_value\n```\nTheorem: \\\nValue Iteration alg. converge to unique optimal values: \\\n- Approximations get refined towards optimal value \\\n- Policy may converge long before values do (policy converges early) \\\n- Value may still change after the policy convergence \\\n- In general, values change more often than policy \\\n\nProblems with VI as it repeats Bellman equations: \\\n- It's slow! $O(s^2 * a)$ per iteration \\\n- The $max$ action rarely changes \\\n- The policy often converges long before the values => We can do it better \\\n\n\n\\\n**Policy** \\\n**Policy Evaluation** \\\n- Fix the policy \\\n- Compute the util. of $s$ under $\\pi$ (not yet optimal policy) \\\n- Recurse - do 1-step look ahead \\\n$V^{\\pi}(s) = \\sum_{s'}{} T(s, \\pi(s), s') [R(s, \\pi(s), s') +$ $\\gamma$ $V^\\pi(s')]$ - dropped $max$ cause actions chosen under policy (usually the best ones) \\\n\n$O(s^2)$ - per iteration \\\n$O(n^3)$ - total (for $n$ states, we have $n$ linear eq. with $n$ unknowns), but we can use simplified VI \\\n\n\n**Policy Extraction** \\\n$\\pi^*(s) = argmax_a \\sum_{s'}{} T(s, a, s') [R(s, a, s') +$ $\\gamma$ $V^*(s')]$ \\\n$\\pi^*(s) = argmax_a Q^*(s, a)$ \\\n! Actions are easier to select from q-values than values \\\n\n\n**Policy Iteration** \\\n1) Policy Eval \\\n2) Policy Improvement - calculates a new MEU policy $\\pi_{i+1}$, using 1-step look ahead aka 1st Policy Extraction formula \\\nRepeat until policy converges \\\n- Optimal! \\\n- Can converge much faster \\\n\n```\ndef iterate(self):\n        # Runs single policy iteration. Fixes curr policy, iterates through state values V(s) until |V_{k+1}(s) - V_k(s)| < ε\n        epsilon = 1e-6\n        policy = dict()\n        for state in self.game.states:\n            policy[state] = self.get_best_policy(state)\n\n        while True:\n            temp = 0\n            for state, value in self.state_values.items():\n                action = self.get_best_policy(state)\n                new_value = self.get_q_value(state, action)\n                self.state_values[state] = new_value\n\n                diff = abs(new_value - value)\n                if diff > temp: temp = diff\n\n            if temp < epsilon:\n                break\n```\n**Comparison** \\\nBoth: \\\n- VI & PI both compute the same thing - all optimal values (via Bellman eq. and Expectimax) \\\n- Are dynamic programs for solving MDP \\\n\nVI (*max action vs*): \\\n- Every iteration updates both, the values & (implicitly) the policy \\\n- Taking $max$ action implicitely recomputes the policy, even if we don't track it \\\n\nPI (*fixed $\\pi$*): \\\n- We do several passes that update values with fixed $\\pi$. Each pass is fast, only 1 action \\\n- After the policy is evaluated, new policy is chosen. Slow like VI pass \\\n- The next policy will be better or we are done \\\n\n\\\n**Summary of MDP** \\\n- To compute optimal values => use VI or PI \\\n- To compute values for a particular policy => use PI \\\n- To turn values into a policy => use Policy Extaction \\\n- Expectimax & MDP = aveaging utilities \\\n\n\\\n**Transforming Utilities** \\\nScaling: better states -> higher values + \\\nGetting the expectimax ordering right \\\n= \\\nIntensivity to Monotonic Transformations \\\n(remeber x^2=> led to better value: was $20 & 25 x^2=> 800 & 650$) \n\n\\\n**Utilities & Preferences** \\\n$\\succ$ - Preference $A \\succ B$ \\\n$\\sim$ - Indifference $A \\sim B$ \\\n\n\\\nAxioms of Rationality: \\\n$(A \\succ B) \\lor (B \\succ A) \\lor (A \\sim B) $ - Orderability \\\n$(A \\succ B) \\land (B \\succ C) \\implies (A \\succ C) $ - Transitivity \\\n$(A \\succ B \\succ C) \\implies \\exists p [p, A; 1 - p, C] \\sim B $ - Continuity, there exist some $p$ that equalizes taking either or $B$ \\\n$(A \\sim B) \\implies [p, A; 1 - p, C] \\sim [p, B; 1 - p, C] $ - Substitutability, in some lottery between $A & C$, I'd also be ok with pair $B & C$, given $P(A) = P(B)$ \\\n$(A \\succ B) \\implies (p \\geq q \\iff [p, A; 1 - p, B] \\succeq [q, A; 1 - q, B]) $ - Monotonocity, I prefer a lottery where probability of $A$ is higher \\\n$[p, A; 1 - p, [q, B; 1-q, C]] \\sim [p, A; (1-p)q, B; (1-p)(1-q), C]$ - Decomposability, compound lotteries could be reduced to simple ones using probability laws \\\n\nIF agent violates any axiom, it will exhibit irrational behavior in some situations. \\\nIF we aceept these axioms, our behavior will maximize EU. \\\nIn other words, make us a Rational Agent. \\\nWe can have a rational agent even if prob./util. are not explicitely modeled (~Reflex vacuum cleaner)\n\n\n## Module 8 Definitions\n* **Expected utility** - ave utility value of the outcomes, weighted by the probability that the outcome occurs (16.1 equation)\n* **Explicit** - expressed directly without anything being implied\n* **Implicit** - indirectly stated or implied\n* **Markov decision process, mdp** - a sequential decision problem for a fully observable env. with stochastic  transition model that has additive rewards\n* **Utility** - sum of (discounted) rewards\n* **Absorbing state** - guarantee that in every policy, a Terminal state will eventually be reached\n* **Performance of an agent in mdp** - sum of rewards for transitions agent takes\n* **Policy** - choice of action for each state\n* **Proper policy** - one that guarantees to reach the terminal state\n* **Policy loss** - the most the agent can lose by executing $\\pi_i$ instead of optimal policy\n* **Non-stationary policy** - depends on time (and points to finite horizon. Infitite horizons = easier as their policies are stationary ones)\n* **Fixed policy** - do what $\\pi$ says to do\n* **Asynchronous policy iteration** - on each iteration, we can pick any subset of states and apply either policy improvement or simplified VI\n* **Policy evaluation** - calculates values for some fixed policy until convergence\n* **Policy extraction** - doing 1-step Expectimax to choose the best action\n* **Meu principle** - given its knowledge, a rational agent Maximizes its Expected Utility\n* ******** - \n* **Lottery** - a situation with an uncertain prize. Set of outcomes where each action is a ticket\n* **Insurance premium** - difference between the expected monetary value of lottery and its certain equivalent\n* **Normative theory** - Describes how a rational agent should act\n* **Descriptive theory** - Describes how actual agent really acts\n* **Certainty effect** - people are strongly attracted to gains that are certain\n* **Ambiquity aversion** - most people elect the known probability rather than the unknown unknowns\n* **Framing effect** - exact wording of a decision problem has a big impact of the agent's choices\n* **Ancoring effect** - people feel more comfortable making relative utility judgement rather than absolute ones\n* **** - ****\n* **Linear programming** - is a general approach for formulating constrained optimization problems\n* **Dynamic programming** - simplifying a problem by recursively breaking it into smaller pieces and remembering the optimal solutions to the pieces\n* **Real-time dynamic programming** - Transforming tree -> graph since root is also a leaf due to repeated states (see 17.10)\n* **Finite horizon** - there is a fixed time $N$ after which nothing matters - the game is over\n* **Discount factor** - preference of an agent for current reward over future rewards\n* **Additive reward** - what discounted rewards are reduce to, when $\\gamma = 1$\n\n\n## Module 8 Figures\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.1.jpg\" alt=\"Figure 16.1\" width=\"75%\"/>\n\n* **Figure 16.1** - (a) Nontransitive preferences $A \\succ B \\succ C \\succ A$ can result in irrational behevior (b) Decomposability Axiom\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.2.jpg\" alt=\"Figure 16.2\" width=\"75%\"/>\n\n* **Figure 16.2** - This figure shows the perceived utility of money. After your first billion, your next billion doesn't matter as much. (b) From risk-seaking to risk-averse\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_16.3.jpg\" alt=\"Figure 16.3\" width=\"75%\"/>\n\n* **Figure 16.3** - Unjustified optimism caused by choosing the best of $k$ options: utility distributed according to a unit normal (blue curve)\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.3.jpg\" alt=\"Figure 17.3\" width=\"75%\"/>\n\n* **Figure 17.3** - 4 x 3 world with $\\gamma = 1$ and $r = -0.04$ for transition to non-terminal states\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.8.jpg\" alt=\"Figure 17.8\" width=\"75%\"/>\n\n* **Figure 17.8** - In practice, it's often that $\\pi_i$ becomes optimal long before $U_i$ has converged\n<img src=\"https://www.cis.upenn.edu/~ccb/data/smart-textbook/AIMA_figures/figure_17.10.jpg\" alt=\"Figure 17.10\" width=\"75%\"/>\n\n* **Figure 17.10** - Expectimax Tree for 4 x 3: decision is extracted by taking ave at the chance nodes and max at the decision nodes\n\n\n# Module 9: Reinforcement Learning\n## AIMA Chapter 17.3 Bandit Problems\n Your summary here \n\n## AIMA Chapter 22.1 Learning from Rewards\n\nIn RL, a deterministic policy is a mapping from states to actions\n\n\n## AIMA Chapter 22.2 Passive Reinforcement Learning\n Your summary here \n\n## AIMA Chapter 22.3 Active Reinforcement Learning\n Your summary here \n\n## AIMA Chapter 22.4 Generalization in Reinforcement Learning\n Your summary here \n\n## Module 9 Lecture Notes\n\nBasic idea of RL: \\\n- Receive feedback in the form of rewards \\\n- Agent's utility is defined by reward function \\\n- Learns to act to maximize expected rrewards \\\n- All learning is based on observed samples of outcomes \\\n\nIt still assumed MDP: \\\n- set of states & actions; transition model and reward function (are not given) \\\n- still looks for policy $\\pi(s)$ \\\n- we don't know which states are good or what actions do \\\n- we must try $a$ and $s$ in order to learn \\\n\nMDP is offline (we can compute ahead of time & choose the best policy) \\\nRL is online (we have to try in order to learn) \\\n\n\\\n**Model-based Learning** \\\n- Learn an approximate model, based on experiences \\\n- Solve for values, as if learned model is correct \\\n\n(1) Learn empirical MDP model: \\\n- Count outcomes $s'$ for each $s, a$ \\\n- Normalize to give an estimate of $\\overline{T}(s, a, s')$ \\\n(2) Solve learned MDP \\\n\n\\\n**Model-free Learning** \\\n- Take $a$ and compare the results between what we thought was to happen with what actually happened \\\n- If smth deviates, adjust the results up or down \\\n- Not keeping track of neither $T$ nor $R$ \\\nPassive Reinforcement Learning (RL) and Active RL \\\n\n\\\n**Passive RL** \\\nIdea: Simplified Policy Eval that doesn't know $T$ or $R$ and is real online, you have to take actions in the world \\\nGiven: \\\n- $\\pi$ \\\n- all the states that can be reached by agent \\\nHas 2 approaches: \\\n- Direct Evaluation \\\n- Temporal Difference Learning \\\n\n\n\\\n**Direct Evaluation** \\\nIdea: \\\n- act, according to $\\pi$ \\\n- every time you visit the state, record the real sum of discounted rewards \\\n- average those samples \\\nGoal: Compute values for each state under $\\pi$ \\\nExample: \n$V(C) = \\frac{9 + 9 + 9 - 11}{4} = 16 / 4 = 4$ \\\n\n+/- of Direct Evaluation: \\\n$+$ Easy to understand \\\n$+$ No knowledge of $T$ and $R$ is required \\\n$+$ Eventually, it computes the correct ave values, using just sample transition \\\n$-$ Wastes information about states connections \\\n$-$ Each state must be learned separately \\\n$-$ Takes long time to learn as it needs a lot of samples \\\n$-$ Sample-based Policy Eval is a good idea, but there is no quarantee we can be in $s' n$ times. We can't rewind( \\\n\nTo improve, we can try to play around with Policy Eval, \\\nbut instead of $T$ and $R$, we can try to ave them without knowing them \\\nIdea: Take samples of outcomes $s'$ (by doing the action) and ave the outcomes \\\n$sample_n = R(s, \\pi(s), s'_n) + \\gamma V^{\\pi}_k(s'_n)$ \\\n$V^{\\pi}_{k+1}(s) = \\frac{1}{n} \\sum_i sample_i$ \\\n\n\\\n**Temporal Difference Learning (TD)** \\\nThe utility of a state is adjusted locally to agree with the utility of at least one successor state for both Adaptive dynamic programming (ADP) and TD learning \\\nIdea: \\\n- Update value $V(s)$ every time we experience a transition $(s, a, s', r)$ \\\n- Likely outcomes $s'$ will contribute updates more often \\\n- Policy is still fixed, still doing eval \\\n- Moves values toward value of whatever successor occurs, running average \\\n\nsample = $R(s, \\pi(s), s') + \\gamma V^{\\pi}(s')$ - sample of $V(s)$ \\\n$V^{\\pi}(s) = V^{\\pi}(s) + \\alpha (sample - V^{\\pi}(s))$ - update to $V(s)$ \\\nUpdates for TD Q-Learning: \\\n$Q(s, a) = Q(s, a) + \\alpha(R(s) + \\gamma \\max_{a^1} Q(s^1,a^1) - Q(s,a))$ \\\n\n$\\alpha$ - learning rate,\nhow much we allow the sample (experimentation) to contribute \\\n$\\alpha = 1$ - all sample-dependent \\\n$\\alpha = 0$ - doesn't matter at all \\\n\n**Exponential/Moving Ave** \\\n- Running intropolation update: \n$x_n = (1 - \\alpha)x_{n-1} + \\alpha x_n$ \\\n- Makes recent samples more important \\\n$-$ Forgets about the past (and distant past values are wrong anyway) \\\n$+$ Decreasing learning rate $\\alpha$ can give converging averages \\\n\n+/- of Temporal Difference: \\\n$+$ Model-free way to do Policy Eval \\\n$-$ We can't turn values into the Policy \\\nTo combat:\n- learn q-values, not values \\\n- that will make action selection model free too \\\n\n\n\\\n**Active RL** \\\nIdea: instead of getting the policy, we gonna make all the choices, including what $a$ to take \\\nHas 2 approaches: \\\n- Q-Learning \\\n- Approximate Q-Learning \\\n\n\\\n**Q-Learning** \\\nSample-based q-value iteration. \\\n~ Off-policy learning \\\n~ We need to take a lot of samples to understand $T$ and $R$ \\\n\nIdea: Learn $Q(s,a)$ as you go:\n- Receive a sample $(s, a, s', r)$ \\\n- Consider old estimate: $Q(s, a)$ \\\n- Consider new sample estimate: \\\nsample = $R(s, a, s') + \\gamma * max_{a'} Q(s', a')$ \\\n- Incorporate this new estimate into a running ave: \\\n$Q(s, a) = (1 - \\alpha)Q(s, a) + \\alpha [sample]$ \\\n\nQ-Learning Properties: \\\n$+$ Q-Learning converges to optimal policy, even if we're acting suboptimally \\\n$o$ Caveats: \\\n- You have to explore enough \\\n- You have to eventually make the learning rate small enough \\\n- But not decrease it too quickly \\\n- In a limit, you will learn the optimal policy no matter how you selected you actions \\\n\n\\\n**Exploration vs Explotation** \\\n2 ways to enforce Exploration Policy: \\\n\n$(1)$ ***Epsilon-greedy random action*** \\\nIdea:\n- Every time step, flip a coin \\\n- With $\\epsilon$ (small probability), act randomly \\\n- With $1 - \\epsilon$, act on current policy \\\n\nDownsides of Exploration: \\\n$-$ We eventually explored the space, but will keep picking things that no longer needed \\\n$=>$ Lower $\\epsilon$ over time \\\n$=>$ Exploration functions \\\n\n\n$(2)$ ***Exploration functions*** \\\nIdea:\nExplore areas whose badness is not established yet, eventually stop exploring \\\n\n$f(u, n) = u + \\frac{k}{n}$, \\\nwhere $u$ - curr. estimate of util \\\n$k$ - bonus \\\n$n$ - num of times the state was visited \\\n\nLeads to modified Q: \\\n$Q(s, a) = R(s, a, s') + \\gamma * max_{a'} f(Q(s', a'), N(s', a'))$ \\\n$#$ as more we visit $s$, as better util. estimate we get \\\n$#$ as more we visit $s$, as smaller bonus we get there $=>$ Encouraging us to visit unvisited states \\\n\n\\\n**Regret** \\\nRegret - measures our total cost of mistakes. \\\nDifference between our rewards (expected) and optimal rewards (expected) \\\n$o$ Random Exploration and Explotation both end of optimal, but first has higher regret \\\n\n\n\\\n**Approximate Q-Learning** \\\nIdea (fundamental to ML): \n- Learn about small number of training states from the experience \\\n- Generalize this exeprience to new similar situations \\\n- However, in reality we can't visit every single state/hold all q-tables in memory $=>$ Evaluation Function \\\n\n***Linear Value Functions*** \\\nUsing feature representation, we can write q-value func for any state, using a few weights \\\n$V(s) = w_1f_1(s) + w_2f_2(s) + ... + w_nf_n(s)$ \\\n$Q(s, a) = w_1f_1(s, a) + w_2f_2(s, a) + ... + w_nf_n(s, a)$\n$+$ Our experience summed up in a few powerful nums \\\n$-$ Q-learning with Linear Function Approximation (weighted linear function of a set of features) will NOT always converge to the optimal policy (there is no guarantee) \\\n$-$ States may share features, but actually have very different values \\\n  => Approximate Q-Learning, evaluating both $s$ and $a$ allows us to combat it \\\n\nIntuitevely: \\\n- Adjusts w on active features \\\n- If smth bad happens, disprefers all states with that feature \\\n(~ disprefered all states, where pacman was too close to the ghost) \\\n\n```\n# Approximate Q-Learning update() func. \ndef update(self, state, action, next_state, reward):\n        # Updates weights using least-squares approximation.\n        # Δ = R + γ V(s') - Q(s,a)\n        # Then updates the weights: w_i = w_i + α * Δ * f_i(s, a)\n        q = self.get_q_value(state, action)             # took delta out of the loop\n        new_value = self.get_value(next_state)\n        delta = reward + (self.discount * new_value) - q\n        for feature, value in self.extractor(state, action).items():\n            old_w = self.get_weight(feature)\n            self.w_table[feature] = old_w + self.learning_rate * delta * value\n```\n\n\n## Module 9 Definitions\n* **Term** - your definition here\n* **Reinforcement learning** - ML training method, based on rewarding desired behavior or punishing undesired one\n* **Exploration** - try unknown actions to get information\n* **Exploitation** - use what you\n* **Regret** - measures our total cost of mistakes\n* **Sampling** - you have to try things repeatedly, because of chance\n* **Difficulty** - learning can be much harder than solving a known MDP\n* ******** - \n* **Passive rl** - takes & executes fixed policy with the goal to learn state values\n* **Direct utility estimation** - Uses total observed reward-to-go for a given state as direct evidence for learning its utility\n* **Adaptive dynamic programming (adp)** - learns model and reward func. from observations => uses value/policy iteration to obtain utilities or optimal policy\n* **Active rl** - instead of getting the policy, we gonna make all the choices, including what $a$ to take\n* **Q-learning** - Sample-based q-value iteration\n* **Approximate q-learning** - generalize learned data, instead of visiting every single state\n\n\n# Module 10: Probabilies and Language Models\n## AIMA Chapter 12.1 Acting under Uncertainty\n Your summary here \n\n## AIMA Chapter 12.2 Basic Probability Notation\n Your summary here \n\n## AIMA Chapter 12.3 Inference Using Full Joint Distributions\n Your summary here \n\n## AIMA Chapter 12.4 Independence\n Your summary here \n\n## AIMA Chapter 12.5 Bayes’ Rule and Its Use\n Your summary here \n\n## AIMA Chapter 12.6 Naive Bayes Models\n Your summary here \n\n## AIMA Chapter 12.7 The Wumpus World Revisited\n Your summary here \n\n## SLP Chapter 3.1 N-Grams\n Your summary here \n\n## SLP Chapter 3.2 Evaluating Language Models\n Your summary here \n\n## SLP Chapter 3.3 Sampling sentences from a language model\n Your summary here \n\n## SLP Chapter 3.4 Generalization and Zeros\n Your summary here \n\n## SLP Chapter 3.5 Smoothing\n Your summary here \n\n## SLP Chapter 3.7 Huge Language Models and Stupid Backoff\n Your summary here \n\n## Module 10 Lecture Notes\n \n**Axiom of Probability** \\\n$0 \\leq P(w) \\leq 1$ \\\n$\\sum_w P(w) = 1$ - total probability of sample space is always 1 \\\n\n$d^n$ - size of distribution of $n$ vars with domain size $d$ \\\n\nConditional Probability: \\\n$P(a | b) = \\frac{P(a, b)}{P(b)}$ \\\n\n\\\n**Probabalistic Inference** \\\n\n$(1)$ Inference by Enumeration \\\n- Select entries, consistent with evidence \\\n- Sum out hidden vars to get joint of $Q$ and evidence: $P(a, c) = \\sum b P(a, b, c)$ \\\n- Normalize: $\\frac{1}{z}$ \\\n\n$(2)$ Product Rule \\\n$P(y) P(x | y) = P(x, y)$ \\\n\n$(3)$ Chain Rule \\\n$P(x, y, z) = P(x) P(y | x) P (z | x, y)$ \\\n$P(x_1, x_2, ... x_i) = \\prod_i P(x_i | x_1, x_2, ... x_{i - 1})$ \\\n\nExamples for $P(X1, X2, X3, X4)$: \\\n$P(X1, X2, X3, X4) = P(X1) \\cdot P(X2 | X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\n$P(X1, X2, X3, X4) = P(X1 | X2, X3, X4) \\cdot P(X4) \\cdot P(X3 | X4) \\cdot P(X2 | X3, X4)$ \\\n$P(X1, X2, X3, X4) = P(X2, X1) \\cdot P(X3 | X1, X2) \\cdot P(X4 | X1, X2, X3)$ \\\n\n$(4)$ Bayes Rule \\\n$P(x, y) = P(x | y) P(y) = P(y | x) P(x)$ \\\n$P(x | y) = \\frac{P(y, x)}{P(y)} P(cx)$ \\\n\n\\\n**Probabalistic Language Models** \\\nMarkov Assumption - prob. of upcoming word only depends on previous $k$ words, not the whole context \\\nMarkov Assumption makes it possible to estimate the probabilities from data \\\n\n***n-grams*** \\\n$(1)$ Unigram - no history: $P(w) = \\frac{count(w)}{all words}$ \\\n$(2)$ Bigram - 1 word as history \\\n$(4)$ 4-gram - 3 words as history: $P(w_i | w_{i-3}, w_{i-2}, w_{i-1}) = \\frac{C(w_{i-3} w_{i-2} w_{i-1} w_i)}{C(w_{i-3} w_{i-2} w_{i - 1})}$ \\\n\n$-$ don't take into account any long-distance dependencies \\\n$-$ unobsereved words and sequences => Laplace Smoothing \\\n$P(w_i | w_{i-1}) = \\frac{C(w_{i-1}, w_i) + 1}{C(w_{i-1}) + V}$, where $C(w_{i-1})$ - aka positive/negative, $V$ - total original words in the whole sample \\\n\n$-$ underflow => log space \\\n$\\log(p_1 \\cdot p_2 \\cdot p_3) = \\log p_1 + \\log p_2 + \\log p_3$ - addresses the risk of underflow and p = 0\n\n\\\n**Perplexity** \\\n(Intrinsic eval metric) \\\n\n$PP(w) = P(w_1 w_2 ... w_n)^{- \\frac{1}{N}} = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_n)}}$ \\\n$PP(w) = \\sqrt[N]{\\prod^N \\frac{1}{P(w_i | w_{i-1})}}$ - for Bi-gram \\\n\nRandom 10-word perplexity: \\\n$PP(w) = P(w_1 w_2 ... w_{10})^{- \\frac{1}{10}} = \\frac{1}{10}^{- \\frac{1}{10} \\cdot 10} = \\frac{1}{10}^{-1} = 10$ - very high preplexity:( \\\n\n- The best LM is the one that best predicts an unseen test set - gives the highest P(sentence) \\\n- Max prob = min perplexity \\\n- As smaller the perplexity as better the model \\\n\n\\\n**n-grams Summary:** \\\n$+$ n-grams only work well for word prediction, when test corpus looks like the training corpus \\\n$-$ Rarely happen in real life \\\n$-$ Difficult to train robust models that generalize \\\n$-$ Zeroes is the issue \\\n\n\n\n## Module 10 Definitions\n* **Model** - knowledge how known vars relate to unknown ones\n* **Sample space** - all possible worlds\n* **Marginalization** - summing out, collapsing the rows and adding them together\n* **Conditional probability** - a measure of probability of the event occuring, given another event that has occured already\n* **Conditional distribution** - probability distribution over some vars, given fixed prob. of others\n* **Probabalistic inference** - computes desired prob. from other known probabilities\n* **Perplexity** - (how well we can predict the next word) inverse prob. of the test set, normilized by the num of words\n* **Intrinsic** - Essential, necessary and intentional\n* **Explicit** - something that is stated plainly\n* **Implicit** - something that is implied and not stated directly\n\n\n# Module 11: Probabilistic Reasoning and Bayes’ Nets\n## AIMA Chapter 13.1 Representing Knowledge in an Uncertain Domain\n Your summary here \n\n## AIMA Chapter 13.2 The Semantics of Bayesian Networks\n Your summary here \n\n## AIMA 13.3 Exact Inference in Bayesian Networks\n Your summary here \n\n## Module 11 Lecture Notes\n\nJoint Probability: \\\n$P(x, y) = P(y) P(x|y) = P(x) P(y|x)$ \\\n\nIndependence $\\perp\\!\\!\\!\\perp$: \\\n$P(A, B) = P(A) P(B)$ - if $\\neq$ they are not Independent \\\n$P(x | y) = P(x)$ - given $y$ adds no new information => P(x) \\\n- Independence (I) significantly simplifies our model assumptions \\\n- $2^n$ - to represent the joint distribution of n fair coin flips \\\n- $2n$ - if coins are independent \\\n- However, I is rare in practice, so we will rely more on the Conditional Independence (CI) \\\n\n\\\n**Conditional Independence** \\\n$A$ and $B$ are conditionally independent given C, iff $P(A, B | C) = P(A | C) \\cdot P(B | C)$ \\\n=> \\\n$P(A | B, C) = P(A | C) $  - $B$ doesn't add any new information, like Fire in case of Alarm when we already see the smoke \\\n\n$(T \\perp\\!\\!\\!\\perp U) | R$ - traffic is CI of umbrella if we observed that it's raining \\\n\n\\\n**CI and Chain Rule** \\\n$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | T, R)$ - traffic, rain, and umbrella  \\\n$P(T, R, U) = P(T) \\cdot P(R | T) \\cdot (U | R)$ - assuming CI $(T \\perp\\!\\!\\!\\perp U) | R$ \\\n\n\\\n**Bayes' Nets (BN)** \\\nBayes' Nets = Bayesian Network \\\nBN = Graph + Local Conditional probabilities: \\\n- Set of nodes - one per var \\\n- DAG - Directed Acyclic Graph \\\n- Conditional Distribution for each node \\\n\nAlso, BN implicitely represent Joint prob. distribution as a product of local conditional distributions: \\\n$P(x_1, x_2 ... x_n) = \\prod^n_{i = 1} P(x_i | parents(X_i))$ \\\n\nWhy BN result in proper Joint Probability Distribution (JPD)?\n- JPD definition is made using Chain Rule \\\n- When we assume CI, we simplify => BN representing JDP \\\n- Not every BN can represent every JDP as topology enforces certain CI \\\n\n***Advantages of BN*** \\\n$o$ $2^n$ - size of full Joint Distribution table of $n$ Boolean vars \\\n$o$ Both give us power to calc JPD \\\n$+$ $N \\cdot 2^{k+1}$ - size of BN with $N$ nodes with upto $k$ parents \\\n$+$ BN gives us a huge space saving \\\n$+$ Easier to elicit conditional prob. tables \\\n$+$ Faster way to answer quiries \\\n\n$o$ BN with no arcs shows absolute independence of nodes \\\n\n***Causality in BN*** \\\n- Causality helps to logically understand and explain BNs, but makes no math difference \\\n- Causality helps to simplify graphs \\\n- Topology and arrows may encode causality, but they really **encode CI** \\\n\n***CI in BN*** \\\n$(X) -> (Y) -> (Z) -> (W)$ - Causal Chain \\\n\n$(Z \\perp\\!\\!\\!\\perp X) | Y$ \\\n$P(Z | X, Y) = P(Z | Y)$ \\\n\n$(W \\perp\\!\\!\\!\\perp X, Y) | Z$ \\\n$P(W | X, Y, Z) = P(W | Z)$ \\\n- Conditional prob. distribution of a var in BN is based on prob. distribution of its parents \\\n\n\\\n**Decendents D-Separation** \\\n\n\\\n***Causal Chain*** \\\n$(X) -> (Y) -> (Z)$ \\\n$P(X, Y, Z) = P(X) \\cdot P(Y | X) \\cdot P(Z | Y)$ \\\n\n$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\n$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\n$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\n\n$o$ Example: Once we've seen the rain, observing Low Pressure gives us no new information \\\n$o$ Evidence along the chain \"blocks\" the influence \\\n\n\\\n***Common Cause*** \\\n$....(Y)...$ \\\n$.../ ...\\setminus ...$ \\\n$(X)...(Z)$ \\\n$P(X, Y, Z) = P(Y) \\cdot P(X | Y) \\cdot P(Z | Y)$ \\\n$Y:$ Project Due \\\n$X:$ Ed busy \\\n$Z:$ OH are full \\\n\n$(1)$ Guaranteed $X$ is fully independent of $Z$? $\\implies$ No \\\n$o$ If we know $X$, then we also know $Z$ \\\n\n$(2)$ Guaranteed $X$ is CI of $Z$ given $Y$? $\\implies$ Yes \\\n$(X \\perp\\!\\!\\!\\perp Z) | Y$ or $(Z \\perp\\!\\!\\!\\perp X) | Y$\\\n$o$ If we haven't observed $Y$, observing $X$ tells us smth useful \\\n$o$ But once we have observed $Y$, seeing $X$ adds no additional info \\\n$o$ Observing the cause \"blocks\" the influence between two different effects \\\n\n\\\n***Common Effect*** \\\n$(X)...(Y)$ \\\n$...\\setminus ../...$ \\\n$....(Z)....$ \\\n\n$P(X, Y, Z) = P(X) \\cdot P(Y) \\cdot P(Z | X) \\cdot P(Z | Y)$ \\\n$X:$ Rain \\\n$Y:$ Ballgame \\\n$Z:$ Traffic \\\n\n$(1)$ Are $X$ and $Y$ fully independent? $\\implies$ Yes \\\n$X \\perp\\!\\!\\!\\perp Y$ \\\n$o$ The Rain and Ballgame cause traffic, but they are not correlated \\\n\n$(2)$ Are $X$ and $Y$ independent given $Z$? $\\implies$ No \\\n$o$ Seeing Traffic puts Rain & Ballgame in competition as explanation \\\n$o$ Observing effect \"activates\" the influence between possible causes \\\n\n\\\n**D-Separation Inference** \\\nAnalyze the Graph and Decompose it into repetition of these 3 canonical cases. \\\n\n\\\n**Active Triples** .... | ... **Inactive Triples** \\\n$ () - () - ()$ ..... | .... $() - (\\oplus) - ()$ \\\n\n$....()...$............... | .......$...(\\oplus)...$... \\\n$../..\\setminus..$............. | .......$../...\\setminus..$.. \\\n$()....()$............... | ........$()....()$...... \\\n\n$()....()$............... | ........$()....()$...... \\\n$.\\setminus../$................. | .......$.\\setminus../$..... \\\n$..(\\oplus)...$.............. | .......$...()...$... \\\n\n$()....()$............... | \\\n$.\\setminus../$................. | \\\n$....()...$............... | \\\n$....\\downarrow...$.............. | \\\n$...(\\oplus)...$............. | \\\n\n$(\\oplus)$ - given, observed node \\\n- A path is active, if each triple is active \\\n- Active path = independence is not guaranteed \\\n- Single inactive segment makes the whole path inactive \\\n- No active path = independence \\\n- Only consider path that is formed from assignment nodes, all other (upper) nodes can be ignored \\\n- Every variable in a BN is independent given its parents. And not independent of all of its descendants given its children. \\\n\n\\\n**Probabalistic Inference** \\\n\n$(5)$ Variable Elimination \\\nAdvance technique of interleaves joining & marginalizing \\\n- Usually, much faster than Inference by Enumeration but still NP-Hard \\\n\n\n## Module 11 Definitions\n* **Independence** - two events are independent, if occurence of one doesn't affect prob. of occurence of the other\n* **Bayes' nets** - graphical models that allow us to express CI assumptions explicitely\n\n\n# Module 12: Naive Bayes and Perceptrons\n## AIMA Chapter 5.1 Game Theory\n Your summary here \n\n## AIMA Chapter 5.2 Optimal Decisions in Games\n Your summary here \n\n## AIMA Chapter 5.3 Heuristic Alpha–Beta Tree Search\n Your summary here \n\n## AIMA Chapter 5.5 Stochastic Games\n Your summary here \n\n## AIMA Chapter 16.1 Combining Beliefs and Desires under Uncertainty\n Your summary here \n\n## AIMA Chapter 16.2 The Basis of Utility Theory\n Your summary here \n\n## SLP Chapter 4.1 Naive Bayes Classifiers\n Your summary here \n\n## SLP Chapter 4.2 Training the Naive Bayes Classifier\n Your summary here \n\n## SLP Chapter 4.3 Worked Example\n Your summary here \n\n## SLP Chapter 4.4 Optimizing for Sentiment Analysis\n Your summary here \n\n## SLP Chapter 4.5 Naive Bayes for other text classification tasks\n Your summary here \n\n## SLP Chapter 4.6 Naive Bayes as a Language Model\n Your summary here \n\n## SLP Chapter 4.7 Evaluation: Precision, Recall, F-measure\n Your summary here \n\n## SLP Chapter 4.8 Test sets and Cross-validation\n Your summary here \n\n## SLP Chapter 4.9 Statistical Significance Testing\n Your summary here \n\n## SLP Chapter 4.10 Avoiding Harms in Classification\n Your summary here \n\n## Module 12 Lecture Notes\n\nMachine Learning (ML) - how to acquire model from data/experience \\\n~ Learning params (prob.) \\\n~ Learning structure (BN Graphs) \\\n~ Learning hidden concepts (clustering) \\\nvs before we learned how to use a model to make decisions \\\n\n\\\n**Model-based Classification** \\\n$(1)$ Build a model (eg. BN), where label & features are random vars \\\n$(2)$ Instantiate any observed feature \\\n$(3)$ Query for distribution of the label, conditioned on the features \\\n$(4)$ Use Inference to give us info about query var \\ \n\n\\\n**Naive Bayes (NB)** \\\nModel: $P(Y, F_1 ... F_n) = P(Y) \\prod_i P(F_i | Y)$ \\\n\n$+$ We only specify how each feature depends on the class $Y$ \\\n$+$ Num of params we have to estimate is pretty small (linear in num of features) \\\n$-$ Model is very simplistic \\\n$+$ But empirically it works well anyway \\\n\nHow fast: \\\n$|Y| \\cdot |F|^n$ - general JPD, exponential with respect to num. of features \\\n$n|F| \\cdot |Y|$ - NB model, linear with respect to num. of features \\\n\n***Inference for NB*** \\\nGoal: compute posterior distribution over label $Y$ \\\n$(1)$ Get Joint prob. of a label and evidence for each label \\\n$(2)$ Sum to get prob. of evidence: $P(f_1 ... f_n)$ \\\n$(3)$ Normilize: \\\n$\\frac{P(Y, f_1 ... f_n)}{P(f_1 ... f_n)} = P(Y | f_1 ... f_n)$ \\\n\n***Bag-of-Words*** - model that is insensitive to word order or reordering \\\nAssumes: \\\n~ each position is identically distributed \\\n~ each position shares the same $P(w | Y)$ \\\n\n\\\n**Basic Concepts of ML** \\\n\n***Overfitting*** \\\n$\\implies$ not generalizing well to unseen data \\\n- To generalize better, smooth/regulate the estimates \\\nHyperparam ~ amount / type of smoothing to do \\\n\n***Param Estimation Techniques*** \\\n$(1)$ Elicitation - askinng DRI/expert \\\n$(2)$ Empirically - using training data \\\n- but we still have to address unseen events \\\n\n$(3)$ Laplace Smoothing \\\n$-$ performs poorly on $P(X | Y)$ when $|X|$ or $|Y|$ are very large \\\n\n$(4)$ Linear Interpolation - weighting cond. prob. vs prior prob. \\\n$P_{LIN}(x | y) = \\alpha \\overline{P}(x | y) + (1 - \\alpha)\\overline{P}(x)$ \\\n\n$(5)$ Adding Feature \\\n\n$(6)$ Baseline - picking super simple baseline \\\n- helps to determine how hard the task is \\\n- helps to know what \"good\" accuracy is \\\n- in research, previous work usually serves as a strong baseline \\\n\n***Confidence and Calibration*** \\\ncondifence(x) = $max_y P(y | x)$ \\\n- important caveat, there is no guarantee that confidence is correct \\\n\nWeak calibration: High Confidence = High Accuracy \\\nStrong calibration: confidence predicts Accuracy rate \\\n\n\\\n**Perceptron** \\\nPerceptron - doesn't deal with prob., instead it consists of a set of weights, input values, and a threshold (bias). \\\n~ Decision-making device \\\n\n- Feature values are fixed, but we can choose diff. w vectors \\\n- Depending on the w vector we pick, we get diff. classifier \\\n$2D$ - line \\\n$3D$ - plane \\\n$4D+$ - separating hyperplane \\\n- Bias - allows us to move decision boundary around as it helps to compare with 0 \\\n- Error-driven learning - allows us to learn & adjust the weights \\\n\n***Binary Perceptron*** \\\nStart with $\\overline{w} = 0$ \\\nFor each example: \\\n$o$ Classify with curr weights (identify $y$ as 0 or 1) \\\n$o$ If correct, no change \\\n$o$ Else, adjust $\\overline{w}$ \\\n\n***Multiclass Perceptron*** \\\n$o$ Take a $w$ vector for each label $w_y$ \\\n$o$ Score activation of a label: $w_y \\cdot f(x)$ \\\n$o$ The highest score prediction wins: $y = argmax_y w_y \\cdot f(x) $ \\\n$o$ If correct, no changes \\\n$o$ Else: \\\n$oo$ Raise the score of the right answer \\\n$oo$ Lower the score of the wrong one \\\n\n***Properties of Perceptron*** \\\n- Separability \\\nTrue, if some params get the training set perfectly correct \\\n- Convergence \\\nIf the trainninng set is separable, perceptron will eventually converge \\\n- Mistake Bound \\\nThe max num of mistakes, related to margin or degree of separability \\\n$mistakes < \\frac{k}{\\sigma^2}$, where \\\n$k$ - some constant \\\n$\\sigma$ - margin \\\nif space between points is large, we will have a small number of mistakes \\\n\n***Problems with Perceptron:*** \\\n$-$ Noise \\\nIf the data is not separatable, weights might trash (go back and forth) \\\n$\\implies$ Averaging $w$ vector over time can help. \\\n$-$ Mediocre Generalization \\\nFinds a barely separable solution. \\\n$-$ Overtraining \\\nTest/held-out accuracy usually rises, then falls. That can lead to overfitting. \\\nEasy to fix though: Stop iterating as soon as tests on held-out start declining. \\\n\n\\\n**Fixing the Perceptron** \\\n\n***MIRA*** \\\nMIRA - Margin-Infused Relaxed Algorithm \\\nIdea: \\\n- Adjust the $w$ updates to mitigate Perceptron problems \\\n- Choose an update size that fixes curr. mistake, but minimizes changes to $w$ \\\n\n$\\tau = \\frac{(w'y - w'y^*) \\cdot f + 1}{2f \\cdot f}$, where \\\n$+1$ - helps to generalize\\\n$y$ - guessed\\\n$y^*$ - truth\\\n$w_y = w'y - \\tau f(x)$ \\\n$wy^* = w'y^* + \\tau f(x)$ \\\n$\\tau$ - helps to amplify $f(x)$, minimally affecting $w$. With $\\tau, w'$ will be closer to initial $w_y$  \\\n\nDownsides: \\\n$-$ But even with $\tau$ sometimes it could update a little too much \\\n$-$ Examples maybe labeled incorrectly \\\n$-$ We might not have enough features \\\n\n\\\n***Max step size*** \\\nMax step size - cap the max possible value of $\tau$ with some constant $C$ \\\n$\\tau^* = min (\\frac{(w'y - w'y^*) \\cdot f + 1}{2f \\cdot f}, C)$ such that $\\frac{()}{()}$ or $C$ \\\n- Helps a lot when we have non-separatable data \\\n- Usually converges faster than Perceptron \\\n- Is usually better, especially on the noisy data \\\n\n\\\n***Support Vector Machine*** \\\nSupport Vector Machine - helps to choose the best Linear Separator when there are a few of them \\\n- Draws 2 additional support vectors & maximizes the margin between them \\\n- Only support vectors that matter, ignoring others \\\n- Finds the separator with max margins \\\n~ Similar to MIRA, where we also optimize over all examples at once \\\n\n~ Non-linear-separatable data in $2D$? Take it to $3D+$ & find an angle! \\\n\n\\\n**Comparing Classification Approaches** \\\n\n***NB:*** \\\n- Builds a model training data \\\n- Gives prediction prob. \\\n- Strong assumptions about feature independence \\\n- One pass through the data (counting) \\\n\n***Percepton / MIRA:*** \\\n- Makes less assumptions about the data \\\n- Mistake-driven learning \\\n- Multiple passes through the data (prediction) \\\n- Often more accurate \\\n\n\n## Module 12 Definitions\n* **Naive bayes** - makes strong (naive) Independence assumptions between features - assumes all features are independent from each other and only depend on a label\n* **Overfitting** - modeling the training data too closely\n* **Confidence** - represents how sure the classifier is of its classification\n* **Calibration** - comparison between the standard and measurement given by the instrument\n* **Perceptron** - consists of a set of weights, input values, and a threshold (bias)\n* **Bias** - allows us to move decision boundary around as it helps to compare with 0\n* **** - \n\n\n# Module 13: Neural Networks\n## SLP Chapter 5.1 Classification: the sigmoid\n Your summary here \n\n## SLP Chapter 5.2 Learning in Logistic Regression\n Your summary here \n\n## SLP Chapter 5.3 The cross-entropy loss function\n Your summary here \n\n## SLP Chapter 5.4 Gradient Descent\n Your summary here \n\n## SLP Chapter 5.5 Regularization\n Your summary here \n\n## SLP Chapter 5.6 Multinomial logistic regression\n Your summary here \n\n## SLP Chapter 7.1 Units\n Your summary here \n\n## SLP Chapter 7.2 The XOR problem\n Your summary here \n\n## SLP Chapter 7.3 Feedforward Neural Networks\n Your summary here \n\n## SLP Chapter 7.4 Feedforward networks for NLP: Classification\n Your summary here \n\n## SLP Chapter 7.5 Feedforward Neural Language Modeling\n Your summary here \n\n## SLP Chapter 7.6 Training Neural Nets\n Your summary here \n\n## SLP Chapter 7.7 Training the neural language model\n Your summary here \n\n## Module 13 Lecture Notes\n\n**Classifier Components** \\\nML Classifiers require a training corpus of $m$ observations input/output pairs $(x^i, y^i)$ \\\n\n\n\n## Module 13 Definitions\n* **Logistic regression (lr)** - (another type of classifier) estimaties the params of logistic model\n* **Logistic model** - models the prob. of the event, using log-odds of the event\n\n\n# Module 14: Natural Language Processing\n## SLP Chapter 9.1 Language Models Revisited\n Your summary here \n\n## SLP Chapter 9.2 Recurrent Neural Networks\n Your summary here \n\n## SLP Chapter 9.3 RNNs as Language Models\n Your summary here \n\n## SLP Chapter 6.1 Lexical Semantics\n Your summary here \n\n## SLP Chapter 6.2 Vector Semantics\n Your summary here \n\n## SLP Chapter 6.3 Words and Vectors\n Your summary here \n\n## SLP Chapter 6.4 Cosine for measuring similarity\n Your summary here \n\n## SLP Chapter 6.8 Word2vec\n Your summary here \n\n## SLP Chapter 6.10 Semantic properties of embeddings\n Your summary here \n\n## SLP Chapter 6.11 Bias and Embeddings\n Your summary here \n\n## SLP Chapter 6.12 Evaluating Vector Models\n Your summary here \n\n## Module 13 Lecture Notes\n\n**Classifier Components** \\\nML Classifiers require a training corpus of $m$ observations input/output pairs $(x^i, y^i)$ \\\n\n\n\n## Module 14 Definitions\n* **Concept** - definition\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}