{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarinaWolters/Coding-Tracker/blob/master/HandsOn10_CNN_Pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "* Popular libraries that implement Neural Network? Why PyTorch?\n",
        "* Fully connected Neural Network for classification.\n",
        "* What is Convolutional Neural Network (CNN)? Tools to explore inside CNN\n",
        "* The essential steps to train a CNN.\n",
        "* Train and evaluate with PyTorch\n",
        "\n",
        "Refer to [Training a Classifier with PyTorch](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). Note there are some functions that has been updated, like new version of PyTorch uses inference_mode instead of no_grad.\n",
        "\n",
        "Thanks to Daniel Bourke for inspiring examples and tricks. "
      ],
      "metadata": {
        "id": "mq6gnNhmpAy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch\n",
        "1. [PyTorch](https://pytorch.org/docs/stable/index.html) is an open source machine learning and deep learning framework.\n",
        "2. PyTorch is more flexible and popular but slightly slower compare to other platform (improved recently)\n",
        "  * Very user friendly. \n",
        "  * Very flexible, allowing for more low-level customization and experimentation with new ideas compared with Tensorflow and MXNet\n",
        "  * Researchers love it and the most advanced models are using it.[Trend](https://paperswithcode.com/trends)\n",
        "  * Leading technology companies using PyTorch: Tesla, OpenAI\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-ZAceIFBxuQk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basics with PyTorch\n"
      ],
      "metadata": {
        "id": "tTmf4sLi0CDA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensor Basic Examples"
      ],
      "metadata": {
        "id": "ABgyKMVW6pM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "1EpjiCsU0Pfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Scalar**: 0 dimension tensor"
      ],
      "metadata": {
        "id": "EJqPVuMY0msM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scalar\n",
        "scalar = torch.tensor(1)\n",
        "print(scalar)\n",
        "# dimension of tensor\n",
        "print(f\"dimension of scalar:{scalar.ndim}\")\n",
        "# From scalar 0-dim tensor to python numbers\n",
        "scalar.item()"
      ],
      "metadata": {
        "id": "uDHAQU_l0LgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector**: 1 dimension tensor"
      ],
      "metadata": {
        "id": "BpnzmVzg0t8f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vector\n",
        "vector = torch.tensor([1,2])\n",
        "print(vector)\n",
        "# dimensin of tensor\n",
        "print(f\"dimension of vector: {vector.ndim}\")\n",
        "vector.shape"
      ],
      "metadata": {
        "id": "7n6VmFHh0yAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Matrix**: 2 dimension tensor"
      ],
      "metadata": {
        "id": "7A3aorQG1LM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = torch.tensor([[1,2],[3,4]])\n",
        "print(matrix)\n",
        "print(f\"dimension of matrix: {matrix.ndim}\")\n",
        "print(f\"shape of matrix: {matrix.shape}\")"
      ],
      "metadata": {
        "id": "t3CYrQ0F1O7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**More dimensions** tensors, explore by yourself!\n",
        "\n",
        "There are different [types of tensor](https://pytorch.org/docs/stable/tensors.html)."
      ],
      "metadata": {
        "id": "sryOMEd82KWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tensors on GPUs and CPUs\n",
        "Computation with Tensors on GPUs runs faster than CPU.\n",
        "\n",
        "**Enabling GPU in Colab** Go to Edit | Notebook Settings and make sure you have enabled _Hardware Accelerator - GPU_"
      ],
      "metadata": {
        "id": "jgNs-lsS3kS4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The tensor used on GPU cannot be directly used on CPU, you have to transfer it from GPU to CPU, vice versa**. You will see torch.cuda as the GPU device, since Nvidia GPUs use a computing tookit called CUDA."
      ],
      "metadata": {
        "id": "KBQ1hd8s3jmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# if you don't have a nvidia gpu, you will see this\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "WIJD1ETU4kpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "18L6_K2m4RP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for GPU\n",
        "import torch\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "id": "IJffbNew4tne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A better practice is to set up the device, if you re-run it and the gpu is not available, your code still works well\n",
        "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "H0q9EUaG5AQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Moving tensor from cpu to gpu and gpu to cpu**"
      ],
      "metadata": {
        "id": "ipuCWFhD6J0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# by default, when creating a tensor, it's on cpu\n",
        "data = torch.tensor([1,2])\n",
        "print(data.device)\n",
        "# Put tensor on gpu\n",
        "data_on_gpu = data.to(device)\n",
        "data_on_gpu"
      ],
      "metadata": {
        "id": "ka6pSpm55mHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the data is on gpu, cannot transform it to numpy data\n",
        "data_back_on_cpu = data_on_gpu.cpu().numpy()\n",
        "data_back_on_cpu"
      ],
      "metadata": {
        "id": "3gZ5vafd6OqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v6ezxY2e2GGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fully Connected Neural Network for Classification"
      ],
      "metadata": {
        "id": "hjrBsjUTz5es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Essential components of a fully connected neural network:\n",
        "* Input layer (shape /  in_features)\n",
        "* Hidden layers\n",
        "* Neurons in each hidden layers\n",
        "  * Generally from 10 to 512, \n",
        "* Activation function in each hidden layer\n",
        "  * Relu\n",
        "  * Sigmoid\n",
        "  * Tanh\n",
        "* Output layer (shape / out_features)\n",
        "* Output activation function\n",
        "  * sigmoid for binary classification\n",
        "  * softmax for multiclass classification\n",
        "* Loss function\n",
        "  * binary cross entropy binary class\n",
        "  * cross entropy for multiclass \n",
        "* Optimizer\n",
        "  * see torch.optim, mostly SGD and Adam\n"
      ],
      "metadata": {
        "id": "8gVnCep57pwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_circles\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Check versions. It's a good practice to print versions in your neural network code.\n",
        "# this is a fast developing field, now working code does not imply working in the future\n",
        "print(f\"PyTorch version: {torch.__version__}\")"
      ],
      "metadata": {
        "id": "-ua4V6Ho93Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ko-Rg6ro0C4"
      },
      "outputs": [],
      "source": [
        "n_samples = 1000\n",
        "\n",
        "data, label = make_circles(n_samples, noise = 0.04, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[:3], label[:3]"
      ],
      "metadata": {
        "id": "jIarj5my_G9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=data[:,0], y=data[:,1], c=label)"
      ],
      "metadata": {
        "id": "tYFYJrmy-WIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification of points"
      ],
      "metadata": {
        "id": "lJZZ1Cp7_XFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data with tensor format and split into train and test"
      ],
      "metadata": {
        "id": "TG68vkSc_tAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from numpy array to tensor\n",
        "data = torch.from_numpy(data).type(torch.float) \n",
        "label = torch.from_numpy(label).type(torch.float) "
      ],
      "metadata": {
        "id": "Btlk_hnZ_WyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0], label[0]"
      ],
      "metadata": {
        "id": "uiXGK5FmAZkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split to train and test data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "Crjo7FcxA_TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the Neural Network\n",
        "1. Set up device\n",
        "2. Define the model class\n",
        "3. Loss function\n",
        "4. Optimizer\n",
        "5. Training loop"
      ],
      "metadata": {
        "id": "aaoQSpzTBRRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Set up device"
      ],
      "metadata": {
        "id": "hs6EkCmqKDTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 1. Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "m1zT8kuNBn-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. To define model:\n",
        " * Create a the model class as the subclass of nn.Module. Almost all PyTorch models are subclasses of nn.Module.\n",
        " * Define a forward() methord. How to do forward computation? Take input data how to achieve to lables.\n",
        " * Create an instance of the model and send to the device"
      ],
      "metadata": {
        "id": "ypICYtwmB62s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pay attention to the shape of input and output between each layer**\n",
        "* The output of previous layer should be the same as the shape of the input of next layer\n",
        "* Explore the purpose of activation function\n",
        "* The difference of more layers and more hidden units"
      ],
      "metadata": {
        "id": "ieFy1wnoEpD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is a neural network layer?**\n",
        "\n",
        "$ Y=\\sum weights * x + bias $"
      ],
      "metadata": {
        "id": "R1q3lbx1v11j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters inside a layer\n",
        "layer = nn.Linear(in_features=2, out_features=10) \n",
        "layer.state_dict()"
      ],
      "metadata": {
        "id": "gMK5ee4avjPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define the model class\n",
        "class ClassificationNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(in_features=2, out_features=10) \n",
        "    self.fc2 = nn.Linear(10, 10)\n",
        "    self.fc3 = nn.Linear(10, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    #x = self.relu(x)\n",
        "    x = self.fc2(x)\n",
        "   # x = self.relu(x)\n",
        "    x = self.fc3(x)\n",
        "    # Above is the same as \n",
        "    # net = nn.Sequential(self.fc1, self.relu, self.fc2, self.relu, self.fc3)\n",
        "    return x\n",
        "\n",
        "netFC = ClassificationNet().to(device)\n",
        "netFC\n"
      ],
      "metadata": {
        "id": "zy3TiUd2Cj9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we created a fully connected layer. You can take a look at the network we just created in the [playground](https://playground.tensorflow.org/)"
      ],
      "metadata": {
        "id": "CbIoZZYHGd1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3 & 4. Loss function and optimizer\n",
        "\n",
        "PyTorch has two binary corss entropy: `torch.nn.BCELoss()`, `torch.nn.BCEWithLogitsLoss()`. `BCEWithLogitsLoss()` is the `BCELoss()` with `Sigmoid` built in."
      ],
      "metadata": {
        "id": "_g8-WNszJ81d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Loss function\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "# 4. Optimizer\n",
        "optimizer = torch.optim.Adam(params=netFC.parameters(), lr=0.1) # Adam optimizer with learning rate = 0.1"
      ],
      "metadata": {
        "id": "emUVHcvIGzgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train the Neural Network\n",
        "In one loop, we need to:\n",
        "1. Zero gradients\n",
        "2. Forward pass\n",
        "3. Calcuate the loss\n",
        "4. Backward propogation\n",
        "5. Optimizer steps"
      ],
      "metadata": {
        "id": "WGo_LTc-MBZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are each above steps doing?"
      ],
      "metadata": {
        "id": "Wmmfn18NNIjZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are the important parameters we need to adjust?\n",
        "* Learning rate\n",
        "* Activation function\n",
        "* Number of layers\n",
        "* Number of hidden units"
      ],
      "metadata": {
        "id": "KKKauFxn2Wlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
        "    acc = (correct / len(y_pred)) * 100 \n",
        "    return acc"
      ],
      "metadata": {
        "id": "dgOZTdO2qVu0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Set the number of loops\n",
        "epochs = 100\n",
        "\n",
        "# Put data to device, sklearn is working on cpu, now we need to put data to cuda\n",
        "X_train, y_train = X_train.to(device), y_train.to(device)\n",
        "X_test, y_test = X_test.to(device), y_test.to(device)\n",
        "\n",
        "# Build training and evaluation loop\n",
        "for epoch in range(epochs):\n",
        "    ### Training\n",
        "    netFC.train() # set to training mode, nn.Module default on training, so can ignore\n",
        "\n",
        "    # 1.zero grad\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # 2. forward\n",
        "    y_logits = netFC(X_train).squeeze() # squeeze to remove extra `1` dimensions\n",
        "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labls(0,1)\n",
        "  \n",
        "    # 3. loss & accuracy\n",
        "    loss = criterion(y_logits, y_train) \n",
        "    acc = accuracy_fn(y_true=y_train, \n",
        "                      y_pred=y_pred) \n",
        "\n",
        "   \n",
        "\n",
        "    # 4. Loss backwards\n",
        "    loss.backward()\n",
        "\n",
        "    # 5. Optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    ### Testing\n",
        "    netFC.eval()\n",
        "    with torch.inference_mode():\n",
        "        # 1. forward\n",
        "        test_logits = netFC(X_test).squeeze() \n",
        "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
        "        # 2.loss & accuracy\n",
        "        test_loss = criterion(test_logits,\n",
        "                            y_test)\n",
        "        test_acc = accuracy_fn(y_true=y_test,\n",
        "                               y_pred=test_pred)\n",
        "\n",
        "    # Print out what's happening every 10 epochs\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")"
      ],
      "metadata": {
        "id": "h9pvrL6UXSa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_cpu = X_test.cpu().detach().numpy()\n",
        "test_pred_cpu = test_pred.cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "bI__VxLksanU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot prediction results\n",
        "plt.scatter(x=X_test_cpu[:,0], y=X_test_cpu[:,1], c=test_pred_cpu)"
      ],
      "metadata": {
        "id": "nv6-Tm6Qr7eU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The importance of activation function\n",
        "Adds **non-linearity** to the model. Try if we remove relu in the above `ClassificationNet` model."
      ],
      "metadata": {
        "id": "dlqegDFR7IJm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare Data for Convolutional Neural Network "
      ],
      "metadata": {
        "id": "Uu3fopw6_yPu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we are using MNIST in our lecture videos, you are probably familiar with the MNIST dataset. It is a large collection of handwritten digits that has been used for computer vision research for many years. It is like the \"hello world\" dataset for computer vision and may not provide a sufficiently challenging test for CNN models due to its relatively simple nature. So there are some guys created FashionMNIST. "
      ],
      "metadata": {
        "id": "x7N4pRg2AjD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Fashion-MNIST dataset comprises of 60,000 examples in the training set and 10,000 examples in the test set, each of which is a grayscale image of 28x28 pixels associated with one of 10 class labels. The dataset was created by Zalando, an online fashion retailer in Germany, by collecting images from their own website and other fashion retailers, and labeling them manually.\n",
        "\n",
        "Fashion-MNIST has been designed to serve as a direct substitute for the original MNIST dataset for evaluating machine learning algorithms, as it shares the same image size and training/testing splits. Due to its popularity, it is commonly used in research pertaining to computer vision and natural language processing, in addition to its role as a benchmark dataset for machine learning models."
      ],
      "metadata": {
        "id": "GGNKaOvJB_fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to use [FashionMNIST](https://github.com/zalandoresearch/fashion-mnist). \n",
        "`torchvision.datasets` contains a lot of datasets you can play around. FashionMNIST is one of those datasets."
      ],
      "metadata": {
        "id": "hUFO8rPB_5Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Check versions. It's a good practice to print versions in your neural network code. \n",
        "# this is a fast developing field, now working code does not imply working in the future\n",
        "print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")"
      ],
      "metadata": {
        "id": "Wu8O51uhCd5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform the dataset while loading. You might want to:\n",
        "* Resize the images. \n",
        "* Turn the data into tensor. Images comes with Python Image Library (PIL) format and we need to turn them into PyTorch Tensors."
      ],
      "metadata": {
        "id": "8Ot3TWWgD4-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Set up device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "id": "lEL6bnnCqbmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([ToTensor()])"
      ],
      "metadata": {
        "id": "jfjEzKpFDvdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup training data\n",
        "train_data = datasets.FashionMNIST(\n",
        "    root=\"data\", # directory to download data\n",
        "    train=True, # want training?\n",
        "    download=True, \n",
        "    transform=transform, \n",
        "    target_transform=None # don't transform labels for this dataset\n",
        ")\n",
        "\n",
        "# Setup testing data\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False, # not training but testing\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")"
      ],
      "metadata": {
        "id": "jzHD1RIeC086"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first data sample\n",
        "print(f\"image: {train_data[0][0].shape}\")\n",
        "print(f\"label: {train_data[0][1]}\")\n",
        "image, label = train_data[0]\n",
        "plt.imshow(image.squeeze(), cmap='gray') \n",
        "plt.title(label);"
      ],
      "metadata": {
        "id": "pPG_BT_XE4di"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.classes)\n",
        "class_names = train_data.classes\n",
        "class_names[9]"
      ],
      "metadata": {
        "id": "8YyvqLk2Glbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of samples\n",
        "print(len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets))\n"
      ],
      "metadata": {
        "id": "NWUzkZt8FIGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare dataloader from dataset\n",
        " The DataLoader function is used to load data into a model for both training and inference purposes. It functions by transforming a sizable Dataset into a Python iterable containing smaller chunks, which are referred to as batches or mini-batches. The batch size for these mini-batches can be set using the appropriate parameter."
      ],
      "metadata": {
        "id": "K7x4sYUQIpQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many to compute in a batch\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Load datasets as iterables of batches\n",
        "train_dataloader = DataLoader(train_data, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True # in case some dataset are ordered\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False \n",
        ")\n"
      ],
      "metadata": {
        "id": "nxs8HflkI_zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network\n",
        "\n",
        "Typical Structure: Input layer -> [Convolutional layer -> activation layer -> pooling layer] -> Flatten Layer -> [Fully Connected Layer]+ -> Output layer\n",
        "\n",
        "**Useful Resources:**\n",
        "\n",
        "Building blocks in Neural Network: [torch.nn](https://pytorch.org/docs/stable/nn.html)\n",
        "\n",
        "What's inside each layer:\n",
        "[CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
        "\n",
        "Parameters in Convolutional layer: [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l85sUK_PuI1v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most beautiful part of the convolutional blocks is that they automatically extract important features without requiring explicit instructions. These features are then forwarded to the fully connected layer for classification. Thus, there is no need to specify beforehand which features the model should consider, as the convolutional layer performs this task automatically."
      ],
      "metadata": {
        "id": "wBXDHHiw-FWw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How to start build a model?**\n",
        "\n",
        "You can start with some existing pre-trained models. [torchvision.models](https://pytorch.org/vision/0.8/models.html).\n",
        "\n",
        "Here we are using the TinyVGG structure from CNN Explainer."
      ],
      "metadata": {
        "id": "B4wZ0UZLKxFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some parameters before build the CNN:\n",
        "* input channels of each layer\n",
        "* output channels of each layer\n",
        "* kernel size\n",
        "* stride\n",
        "* padding"
      ],
      "metadata": {
        "id": "WnkGb1-lh3Yf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a convolutional neural network \n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
        "        super().__init__()\n",
        "        self.block_1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=input_shape, \n",
        "                      out_channels=hidden_units, \n",
        "                      kernel_size=3, \n",
        "                      stride=1, \n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_units, \n",
        "                      out_channels=hidden_units,\n",
        "                      kernel_size=3,\n",
        "                      stride=1,\n",
        "                      padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2,\n",
        "                         stride=2)\n",
        "        )\n",
        "        self.block_2 = nn.Sequential(\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=hidden_units*7*7, \n",
        "                      out_features=output_shape)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.block_1(x)\n",
        "        #print(x.shape)\n",
        "        x = self.block_2(x)\n",
        "        #print(x.shape)\n",
        "        x = self.classifier(x)\n",
        "        #print(x.shape)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Cbge6Cnj4zrH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "netCNN = ConvNet(input_shape=1, \n",
        "    hidden_units=10, \n",
        "    output_shape=len(class_names)).to(device)\n",
        "netCNN"
      ],
      "metadata": {
        "id": "98svsru85GW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Trick to know the shape of input of certain layer**\n",
        "\n",
        "Use a dummy input, and print the torch shape after each block/layer"
      ],
      "metadata": {
        "id": "eREjqasvrRIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dummy input with the same shape of our data\n",
        "rand_image_tensor = torch.randn(size=(1,28,28))\n",
        "print(rand_image_tensor.shape)\n",
        "netCNN(rand_image_tensor.unsqueeze(0).to(device))"
      ],
      "metadata": {
        "id": "A-zW6T8JqgMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set Loss and Optimizer to prepare training"
      ],
      "metadata": {
        "id": "hYbovX26uNio"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\n",
        "optimizer = torch.optim.SGD(params=netCNN.parameters(), lr=0.1) # Adam also perform good on classification problems"
      ],
      "metadata": {
        "id": "nREsycwGKcV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the Neural Network\n",
        "In one loop, we need to:\n",
        "\n",
        "1. Zero gradients\n",
        "2. Forward pass\n",
        "3. Calcuate the loss\n",
        "4. Backward propogation\n",
        "5. Optimizer steps"
      ],
      "metadata": {
        "id": "s5xRaA8buUOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "epochs = 5 # start with small number to make sure it's on the right direction, change to larger later\n",
        "\n",
        "# Create training and testing loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch}\\n-------\")\n",
        "    ### Training\n",
        "    train_loss = 0\n",
        "    # Add a loop to loop through training batches\n",
        "    for batch, (X, y) in enumerate(train_dataloader):\n",
        "        netCNN.train() # set the mode to training mode\n",
        "        # 1. zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. forward\n",
        "        y_pred = netCNN(X)\n",
        "\n",
        "        # 3. loss \n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss # accumulatively add up the loss per epoch \n",
        "        \n",
        "        # 4. backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print\n",
        "        if batch % 500 == 0:\n",
        "            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n",
        "\n",
        "    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n",
        "    train_loss /= len(train_dataloader)\n",
        "    \n",
        "    ### Testing\n",
        "    # Setup variables for accumulatively adding up loss and accuracy \n",
        "    test_loss, test_acc = 0, 0 \n",
        "    netCNN.eval()\n",
        "    with torch.inference_mode():\n",
        "        for X, y in test_dataloader:\n",
        "            # 1. Forward pass\n",
        "            test_pred = netCNN(X)\n",
        "           \n",
        "            # 2. Calculate loss (accumatively)\n",
        "            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n",
        "\n",
        "            # 3. Calculate accuracy (preds need to be same as y_true)\n",
        "            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n",
        "        \n",
        "        # Calculations on test metrics need to happen inside torch.inference_mode()\n",
        "        # Divide total test loss by length of test dataloader (per batch)\n",
        "        test_loss /= len(test_dataloader)\n",
        "\n",
        "        # Divide total accuracy by length of test dataloader (per batch)\n",
        "        test_acc /= len(test_dataloader)\n",
        "\n",
        "    ## Print out what's happening\n",
        "    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n"
      ],
      "metadata": {
        "id": "xD1ntk3TtfQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's sample 9 images and see how the prediction works."
      ],
      "metadata": {
        "id": "66YXaFZq9AcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "#random.seed(42)\n",
        "test_samples = []\n",
        "test_labels = []\n",
        "for sample, label in random.sample(list(test_data), k=9):\n",
        "    test_samples.append(sample)\n",
        "    test_labels.append(label)\n",
        "pred_probs = []\n",
        "netCNN.eval()\n",
        "with torch.inference_mode():\n",
        "    for sample in test_samples:\n",
        "        sample = torch.unsqueeze(sample, dim=0).to(device)\n",
        "        pred_logit = netCNN(sample)\n",
        "        pred_prob = torch.softmax(pred_logit.squeeze(), dim=0)\n",
        "        pred_probs.append(pred_prob.cpu())\n",
        "\n",
        "pred_classes = torch.stack(pred_probs).argmax(dim=1)"
      ],
      "metadata": {
        "id": "1aic5j9jxufq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions\n",
        "plt.figure(figsize=(9, 9))\n",
        "nrows = 3\n",
        "ncols = 3\n",
        "for i, sample in enumerate(test_samples):\n",
        "  plt.subplot(nrows, ncols, i+1)\n",
        "  plt.imshow(sample.squeeze(), cmap=\"gray\")\n",
        "  pred_label = class_names[pred_classes[i]]\n",
        "  truth_label = class_names[test_labels[i]] \n",
        "\n",
        "  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n",
        "  if pred_label == truth_label:\n",
        "      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n",
        "  else:\n",
        "      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n",
        "  plt.axis(False);"
      ],
      "metadata": {
        "id": "BBOi_6TGOnw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "    train_loss, train_acc = 0, 0\n",
        "    for batch, (X, y) in enumerate(data_loader):\n",
        "        # Send data to GPU\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 2. forward\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 3. loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss\n",
        "        train_acc += accuracy_fn(y_true=y,\n",
        "                                 y_pred=y_pred.argmax(dim=1)) # why argmax here?\n",
        "\n",
        "        # 4. backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "    # Calculate loss and accuracy per epoch and print out what's happening\n",
        "    train_loss /= len(data_loader)\n",
        "    train_acc /= len(data_loader)\n",
        "    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "    test_loss, test_acc = 0, 0\n",
        "    model.eval() # put model in eval mode\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode(): \n",
        "        for X, y in data_loader:\n",
        "            # data to GPU\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            \n",
        "            # 1. forward\n",
        "            test_pred = model(X)\n",
        "            \n",
        "            # 2. loss and accuracy\n",
        "            test_loss += loss_fn(test_pred, y)\n",
        "            test_acc += accuracy_fn(y_true=y,\n",
        "                y_pred=test_pred.argmax(dim=1) \n",
        "            )\n",
        "        \n",
        "        # metrics and print out\n",
        "        test_loss /= len(data_loader)\n",
        "        test_acc /= len(data_loader)\n",
        "        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")"
      ],
      "metadata": {
        "id": "zwAjAfucyOlg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "# Train and test model \n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch: {epoch}\\n---------\")\n",
        "    train_step(data_loader=train_dataloader, \n",
        "        model=netCNN, \n",
        "        loss_fn=loss_fn,\n",
        "        optimizer=optimizer,\n",
        "        accuracy_fn=accuracy_fn,\n",
        "        device=device\n",
        "    )\n"
      ],
      "metadata": {
        "id": "1PeXmK__NM2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_step(data_loader=test_dataloader,\n",
        "    model=netCNN,\n",
        "    loss_fn=loss_fn,\n",
        "    accuracy_fn=accuracy_fn,\n",
        "    device=device\n",
        ")"
      ],
      "metadata": {
        "id": "KvuMBGZ5N3Uv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}